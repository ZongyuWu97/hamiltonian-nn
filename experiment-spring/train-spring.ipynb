{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, argparse\n",
    "import numpy as np\n",
    "\n",
    "import os, sys\n",
    "\n",
    "# sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "THIS_DIR = os.path.abspath(os.path.join('.'))\n",
    "PARENT_DIR = os.path.dirname(THIS_DIR)\n",
    "sys.path.append(PARENT_DIR)\n",
    "\n",
    "from nn_models import MLP\n",
    "from hnn import HNN\n",
    "from utils import L2_loss, rk4\n",
    "from data_position_only import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ZongyuWu/hamiltonian-nn/experiment-spring\n",
      "/Users/ZongyuWu/hamiltonian-nn\n"
     ]
    }
   ],
   "source": [
    "print(THIS_DIR)\n",
    "print(PARENT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print squared loss at specific steps for comparison with HNN\n",
    "print_every = 200\n",
    "def print_results(results, print_every=200):\n",
    "    for step in range(0, len(results[\"train_loss\"]), print_every):\n",
    "        print(\n",
    "            \"step {}, train_loss {:.4e}, test_loss {:.4e}\".format(\n",
    "                step,\n",
    "                results[\"train_loss\"][step],\n",
    "                results[\"test_loss\"][step],\n",
    "            )\n",
    "        )\n",
    "    # print('Final train loss {:.4e} +/- {:.4e}\\nFinal test loss {:.4e} +/- {:.4e}'\n",
    "    #     .format(results[\"train_loss\"][-1], results[\"train_std\"][-1],\n",
    "    #             results[\"test_loss\"][-1], results[\"test_std\"][-1]))\n",
    "\n",
    "def print_best(results):\n",
    "    curr_min = 0\n",
    "\n",
    "    for step in range(0, len(results[\"train_loss\"])):\n",
    "        if results[\"test_loss\"][step] < results[\"test_loss\"][curr_min]:\n",
    "            curr_min = step\n",
    "    print(\n",
    "        \"best test loss at step {}, train_loss {:.4e}, test_loss {:.4e}\".format(\n",
    "            curr_min,\n",
    "            results[\"train_loss\"][curr_min],\n",
    "            results[\"test_loss\"][curr_min],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument(\n",
    "        \"--input_dim\", default=2, type=int, help=\"dimensionality of input tensor\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_dim\", default=200, type=int, help=\"hidden dimension of mlp\"\n",
    "    )\n",
    "    parser.add_argument(\"--learn_rate\", default=1e-3, type=float, help=\"learning rate\")\n",
    "    parser.add_argument(\n",
    "        \"--nonlinearity\", default=\"tanh\", type=str, help=\"neural net nonlinearity\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--total_steps\", default=2000, type=int, help=\"number of gradient steps\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--print_every\",\n",
    "        default=200,\n",
    "        type=int,\n",
    "        help=\"number of gradient steps between prints\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--name\", default=\"spring\", type=str, help=\"only one option right now\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--baseline\",\n",
    "        dest=\"baseline\",\n",
    "        action=\"store_true\",\n",
    "        help=\"run baseline or experiment?\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_rk4\",\n",
    "        dest=\"use_rk4\",\n",
    "        action=\"store_true\",\n",
    "        help=\"integrate derivative with RK4\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--verbose\", dest=\"verbose\", action=\"store_true\", help=\"verbose?\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--kan\", dest=\"kan\", action=\"store_true\", help=\"use kan instead of mlp?\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--field_type\",\n",
    "        default=\"solenoidal\",\n",
    "        type=str,\n",
    "        help=\"type of vector field to learn\",\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int, help=\"random seed\")\n",
    "    parser.add_argument(\n",
    "        \"--save_dir\", default=THIS_DIR, type=str, help=\"where to save the trained model\"\n",
    "    )\n",
    "    parser.set_defaults(feature=True)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # set random seed\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    # init model and optimizer\n",
    "    if args.verbose:\n",
    "        print(\"Training baseline model:\" if args.baseline else \"Training HNN model:\")\n",
    "\n",
    "    output_dim = args.input_dim if args.baseline else 2\n",
    "    nn_model = MLP(args.input_dim, args.hidden_dim, output_dim, args.nonlinearity)\n",
    "    model = HNN(\n",
    "        args.input_dim,\n",
    "        differentiable_model=nn_model,\n",
    "        field_type=args.field_type,\n",
    "        baseline=args.baseline,\n",
    "    )\n",
    "    optim = torch.optim.Adam(model.parameters(), args.learn_rate, weight_decay=1e-4)\n",
    "\n",
    "    # arrange data\n",
    "    data = get_dataset(seed=args.seed)\n",
    "    x = torch.tensor(data[\"x\"], requires_grad=True, dtype=torch.float32)\n",
    "    test_x = torch.tensor(data[\"test_x\"], requires_grad=True, dtype=torch.float32)\n",
    "    dxdt = torch.Tensor(data[\"dx\"])\n",
    "    test_dxdt = torch.Tensor(data[\"test_dx\"])\n",
    "\n",
    "    # vanilla train loop\n",
    "    stats = {\"train_loss\": [], \"test_loss\": []}\n",
    "    for step in range(args.total_steps + 1):\n",
    "\n",
    "        # train step\n",
    "        dxdt_hat = (\n",
    "            model.rk4_time_derivative(x) if args.use_rk4 else model.time_derivative(x)\n",
    "        )\n",
    "        loss = L2_loss(dxdt, dxdt_hat)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # run test data\n",
    "        test_dxdt_hat = (\n",
    "            model.rk4_time_derivative(test_x)\n",
    "            if args.use_rk4\n",
    "            else model.time_derivative(test_x)\n",
    "        )\n",
    "        test_loss = L2_loss(test_dxdt, test_dxdt_hat)\n",
    "\n",
    "        # logging\n",
    "        stats[\"train_loss\"].append(loss.item())\n",
    "        stats[\"test_loss\"].append(test_loss.item())\n",
    "        if args.verbose and step % args.print_every == 0:\n",
    "            print(\n",
    "                \"step {}, train_loss {:.4e}, test_loss {:.4e}\".format(\n",
    "                    step, loss.item(), test_loss.item()\n",
    "                )\n",
    "            )\n",
    "\n",
    "    train_dxdt_hat = model.time_derivative(x)\n",
    "    train_dist = (dxdt - train_dxdt_hat) ** 2\n",
    "    test_dxdt_hat = model.time_derivative(test_x)\n",
    "    test_dist = (test_dxdt - test_dxdt_hat) ** 2\n",
    "    print(\n",
    "        \"Final train loss {:.4e} +/- {:.4e}\\nFinal test loss {:.4e} +/- {:.4e}\".format(\n",
    "            train_dist.mean().item(),\n",
    "            train_dist.std().item() / np.sqrt(train_dist.shape[0]),\n",
    "            test_dist.mean().item(),\n",
    "            test_dist.std().item() / np.sqrt(test_dist.shape[0]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return model, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "data = get_dataset(seed=0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.tensor(data[\"x\"], requires_grad=True, dtype=torch.float32).to(device)\n",
    "test_x = torch.tensor(data[\"test_x\"], requires_grad=True, dtype=torch.float32).to(device)\n",
    "dxdt = torch.Tensor(data[\"dx\"]).to(device)\n",
    "test_dxdt = torch.Tensor(data[\"test_dx\"]).to(device)\n",
    "\n",
    "# dataset['train_input'], dataset['train_label'],dataset['test_input'], dataset['test_label']\n",
    "dataset = {\n",
    "    \"train_input\": x,\n",
    "    \"train_label\": dxdt,\n",
    "    \"test_input\": test_x,\n",
    "    \"test_label\": test_dxdt,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7500, 2]), torch.Size([7500, 2]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train_input\"].shape, dataset[\"train_label\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HNN model:\n",
      "step 0, train_loss 2.0823e+00, test_loss 2.0830e+00\n",
      "step 200, train_loss 3.1658e-03, test_loss 4.6407e-03\n",
      "step 400, train_loss 2.9592e-03, test_loss 4.4936e-03\n",
      "step 600, train_loss 2.9248e-03, test_loss 4.4616e-03\n",
      "step 800, train_loss 2.9073e-03, test_loss 4.4440e-03\n",
      "step 1000, train_loss 2.8952e-03, test_loss 4.4319e-03\n",
      "step 1200, train_loss 2.8859e-03, test_loss 4.4228e-03\n",
      "step 1400, train_loss 2.8780e-03, test_loss 4.4147e-03\n",
      "step 1600, train_loss 2.9854e-03, test_loss 4.4085e-03\n",
      "step 1800, train_loss 2.8700e-03, test_loss 4.4014e-03\n",
      "step 2000, train_loss 2.8858e-03, test_loss 4.4076e-03\n",
      "Final train loss 2.8716e-03 +/- 6.2804e-04\n",
      "Final test loss 4.4076e-03 +/- 1.0590e-03\n"
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "args.verbose = True\n",
    "model, stats = train(args)\n",
    "\n",
    "# save\n",
    "os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None\n",
    "label = \"-position\"\n",
    "path = \"{}/{}{}.tar\".format(args.save_dir, args.name, label)\n",
    "torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train_loss 2.0823e+00, test_loss 2.0830e+00\n",
      "step 1, train_loss 2.0625e+00, test_loss 2.0699e+00\n",
      "step 2, train_loss 2.0498e+00, test_loss 2.0538e+00\n",
      "step 3, train_loss 2.0341e+00, test_loss 2.0366e+00\n",
      "step 4, train_loss 2.0170e+00, test_loss 2.0204e+00\n",
      "step 5, train_loss 2.0009e+00, test_loss 2.0053e+00\n",
      "step 6, train_loss 1.9860e+00, test_loss 1.9900e+00\n",
      "step 7, train_loss 1.9710e+00, test_loss 1.9736e+00\n",
      "step 8, train_loss 1.9549e+00, test_loss 1.9561e+00\n",
      "step 9, train_loss 1.9377e+00, test_loss 1.9379e+00\n",
      "step 10, train_loss 1.9199e+00, test_loss 1.9192e+00\n",
      "step 11, train_loss 1.9016e+00, test_loss 1.8997e+00\n",
      "step 12, train_loss 1.8824e+00, test_loss 1.8786e+00\n",
      "step 13, train_loss 1.8618e+00, test_loss 1.8557e+00\n",
      "step 14, train_loss 1.8392e+00, test_loss 1.8306e+00\n",
      "step 15, train_loss 1.8145e+00, test_loss 1.8035e+00\n",
      "step 16, train_loss 1.7878e+00, test_loss 1.7742e+00\n",
      "step 17, train_loss 1.7588e+00, test_loss 1.7424e+00\n",
      "step 18, train_loss 1.7273e+00, test_loss 1.7076e+00\n",
      "step 19, train_loss 1.6930e+00, test_loss 1.6696e+00\n",
      "step 20, train_loss 1.6554e+00, test_loss 1.6283e+00\n",
      "step 21, train_loss 1.6145e+00, test_loss 1.5836e+00\n",
      "step 22, train_loss 1.5702e+00, test_loss 1.5356e+00\n",
      "step 23, train_loss 1.5225e+00, test_loss 1.4839e+00\n",
      "step 24, train_loss 1.4712e+00, test_loss 1.4285e+00\n",
      "step 25, train_loss 1.4161e+00, test_loss 1.3693e+00\n",
      "step 26, train_loss 1.3571e+00, test_loss 1.3063e+00\n",
      "step 27, train_loss 1.2942e+00, test_loss 1.2397e+00\n",
      "step 28, train_loss 1.2276e+00, test_loss 1.1692e+00\n",
      "step 29, train_loss 1.1571e+00, test_loss 1.0950e+00\n",
      "step 30, train_loss 1.0828e+00, test_loss 1.0172e+00\n",
      "step 31, train_loss 1.0048e+00, test_loss 9.3623e-01\n",
      "step 32, train_loss 9.2350e-01, test_loss 8.5232e-01\n",
      "step 33, train_loss 8.3921e-01, test_loss 7.6589e-01\n",
      "step 34, train_loss 7.5230e-01, test_loss 6.7758e-01\n",
      "step 35, train_loss 6.6344e-01, test_loss 5.8831e-01\n",
      "step 36, train_loss 5.7361e-01, test_loss 4.9921e-01\n",
      "step 37, train_loss 4.8396e-01, test_loss 4.1176e-01\n",
      "step 38, train_loss 3.9605e-01, test_loss 3.2800e-01\n",
      "step 39, train_loss 3.1200e-01, test_loss 2.5041e-01\n",
      "step 40, train_loss 2.3435e-01, test_loss 1.8179e-01\n",
      "step 41, train_loss 1.6600e-01, test_loss 1.2523e-01\n",
      "step 42, train_loss 1.1017e-01, test_loss 8.3462e-02\n",
      "step 43, train_loss 6.9717e-02, test_loss 5.8205e-02\n",
      "step 44, train_loss 4.6427e-02, test_loss 4.9171e-02\n",
      "step 45, train_loss 3.9956e-02, test_loss 5.3402e-02\n",
      "step 46, train_loss 4.7174e-02, test_loss 6.5475e-02\n",
      "step 47, train_loss 6.2371e-02, test_loss 7.9169e-02\n",
      "step 48, train_loss 7.8916e-02, test_loss 8.9047e-02\n",
      "step 49, train_loss 9.0991e-02, test_loss 9.1785e-02\n",
      "step 50, train_loss 9.5049e-02, test_loss 8.6389e-02\n",
      "step 51, train_loss 9.0024e-02, test_loss 7.3961e-02\n",
      "step 52, train_loss 7.7107e-02, test_loss 5.7230e-02\n",
      "step 53, train_loss 5.9274e-02, test_loss 3.9999e-02\n",
      "step 54, train_loss 4.0669e-02, test_loss 2.6284e-02\n",
      "step 55, train_loss 2.5644e-02, test_loss 1.9108e-02\n",
      "step 56, train_loss 1.7456e-02, test_loss 1.9253e-02\n",
      "step 57, train_loss 1.6970e-02, test_loss 2.4860e-02\n",
      "step 58, train_loss 2.2275e-02, test_loss 3.2287e-02\n",
      "step 59, train_loss 2.9608e-02, test_loss 3.7892e-02\n",
      "step 60, train_loss 3.5206e-02, test_loss 3.9530e-02\n",
      "step 61, train_loss 3.6853e-02, test_loss 3.6939e-02\n",
      "step 62, train_loss 3.4264e-02, test_loss 3.1268e-02\n",
      "step 63, train_loss 2.8607e-02, test_loss 2.4311e-02\n",
      "step 64, train_loss 2.1713e-02, test_loss 1.7818e-02\n",
      "step 65, train_loss 1.5364e-02, test_loss 1.3049e-02\n",
      "step 66, train_loss 1.0836e-02, test_loss 1.0600e-02\n",
      "step 67, train_loss 8.7122e-03, test_loss 1.0391e-02\n",
      "step 68, train_loss 8.8786e-03, test_loss 1.1770e-02\n",
      "step 69, train_loss 1.0626e-02, test_loss 1.3714e-02\n",
      "step 70, train_loss 1.2869e-02, test_loss 1.5160e-02\n",
      "step 71, train_loss 1.4494e-02, test_loss 1.5427e-02\n",
      "step 72, train_loss 1.4793e-02, test_loss 1.4472e-02\n",
      "step 73, train_loss 1.3733e-02, test_loss 1.2822e-02\n",
      "step 74, train_loss 1.1871e-02, test_loss 1.1212e-02\n",
      "step 75, train_loss 9.9908e-03, test_loss 1.0200e-02\n",
      "step 76, train_loss 8.6973e-03, test_loss 9.9375e-03\n",
      "step 77, train_loss 8.1868e-03, test_loss 1.0221e-02\n",
      "step 78, train_loss 8.2812e-03, test_loss 1.0693e-02\n",
      "step 79, train_loss 8.6328e-03, test_loss 1.1048e-02\n",
      "step 80, train_loss 8.9349e-03, test_loss 1.1128e-02\n",
      "step 81, train_loss 9.0201e-03, test_loss 1.0905e-02\n",
      "step 82, train_loss 8.8461e-03, test_loss 1.0424e-02\n",
      "step 83, train_loss 8.4444e-03, test_loss 9.7589e-03\n",
      "step 84, train_loss 7.8777e-03, test_loss 9.0017e-03\n",
      "step 85, train_loss 7.2303e-03, test_loss 8.2655e-03\n",
      "step 86, train_loss 6.6082e-03, test_loss 7.6696e-03\n",
      "step 87, train_loss 6.1238e-03, test_loss 7.3017e-03\n",
      "step 88, train_loss 5.8575e-03, test_loss 7.1797e-03\n",
      "step 89, train_loss 5.8206e-03, test_loss 7.2386e-03\n",
      "step 90, train_loss 5.9427e-03, test_loss 7.3614e-03\n",
      "step 91, train_loss 6.1034e-03, test_loss 7.4327e-03\n",
      "step 92, train_loss 6.1866e-03, test_loss 7.3862e-03\n",
      "step 93, train_loss 6.1288e-03, test_loss 7.2216e-03\n",
      "step 94, train_loss 5.9353e-03, test_loss 6.9906e-03\n",
      "step 95, train_loss 5.6639e-03, test_loss 6.7638e-03\n",
      "step 96, train_loss 5.3924e-03, test_loss 6.6000e-03\n",
      "step 97, train_loss 5.1864e-03, test_loss 6.5249e-03\n",
      "step 98, train_loss 5.0763e-03, test_loss 6.5260e-03\n",
      "step 99, train_loss 5.0517e-03, test_loss 6.5601e-03\n",
      "step 100, train_loss 5.0694e-03, test_loss 6.5725e-03\n",
      "step 101, train_loss 5.0739e-03, test_loss 6.5224e-03\n",
      "step 102, train_loss 5.0236e-03, test_loss 6.4017e-03\n",
      "step 103, train_loss 4.9094e-03, test_loss 6.2373e-03\n",
      "step 104, train_loss 4.7569e-03, test_loss 6.0751e-03\n",
      "step 105, train_loss 4.6104e-03, test_loss 5.9541e-03\n",
      "step 106, train_loss 4.5074e-03, test_loss 5.8877e-03\n",
      "step 107, train_loss 4.4594e-03, test_loss 5.8610e-03\n",
      "step 108, train_loss 4.4494e-03, test_loss 5.8452e-03\n",
      "step 109, train_loss 4.4461e-03, test_loss 5.8164e-03\n",
      "step 110, train_loss 4.4242e-03, test_loss 5.7683e-03\n",
      "step 111, train_loss 4.3764e-03, test_loss 5.7104e-03\n",
      "step 112, train_loss 4.3126e-03, test_loss 5.6575e-03\n",
      "step 113, train_loss 4.2495e-03, test_loss 5.6201e-03\n",
      "step 114, train_loss 4.1991e-03, test_loss 5.5995e-03\n",
      "step 115, train_loss 4.1648e-03, test_loss 5.5898e-03\n",
      "step 116, train_loss 4.1419e-03, test_loss 5.5820e-03\n",
      "step 117, train_loss 4.1229e-03, test_loss 5.5681e-03\n",
      "step 118, train_loss 4.1008e-03, test_loss 5.5434e-03\n",
      "step 119, train_loss 4.0714e-03, test_loss 5.5075e-03\n",
      "step 120, train_loss 4.0342e-03, test_loss 5.4639e-03\n",
      "step 121, train_loss 3.9928e-03, test_loss 5.4186e-03\n",
      "step 122, train_loss 3.9527e-03, test_loss 5.3775e-03\n",
      "step 123, train_loss 3.9190e-03, test_loss 5.3433e-03\n",
      "step 124, train_loss 3.8934e-03, test_loss 5.3157e-03\n",
      "step 125, train_loss 3.8739e-03, test_loss 5.2915e-03\n",
      "step 126, train_loss 3.8565e-03, test_loss 5.2683e-03\n",
      "step 127, train_loss 3.8374e-03, test_loss 5.2452e-03\n",
      "step 128, train_loss 3.8155e-03, test_loss 5.2233e-03\n",
      "step 129, train_loss 3.7919e-03, test_loss 5.2041e-03\n",
      "step 130, train_loss 3.7686e-03, test_loss 5.1882e-03\n",
      "step 131, train_loss 3.7471e-03, test_loss 5.1755e-03\n",
      "step 132, train_loss 3.7282e-03, test_loss 5.1647e-03\n",
      "step 133, train_loss 3.7118e-03, test_loss 5.1546e-03\n",
      "step 134, train_loss 3.6972e-03, test_loss 5.1434e-03\n",
      "step 135, train_loss 3.6829e-03, test_loss 5.1295e-03\n",
      "step 136, train_loss 3.6678e-03, test_loss 5.1124e-03\n",
      "step 137, train_loss 3.6512e-03, test_loss 5.0928e-03\n",
      "step 138, train_loss 3.6336e-03, test_loss 5.0724e-03\n",
      "step 139, train_loss 3.6162e-03, test_loss 5.0532e-03\n",
      "step 140, train_loss 3.6005e-03, test_loss 5.0362e-03\n",
      "step 141, train_loss 3.5867e-03, test_loss 5.0212e-03\n",
      "step 142, train_loss 3.5741e-03, test_loss 5.0074e-03\n",
      "step 143, train_loss 3.5617e-03, test_loss 4.9944e-03\n",
      "step 144, train_loss 3.5487e-03, test_loss 4.9823e-03\n",
      "step 145, train_loss 3.5352e-03, test_loss 4.9715e-03\n",
      "step 146, train_loss 3.5220e-03, test_loss 4.9621e-03\n",
      "step 147, train_loss 3.5098e-03, test_loss 4.9538e-03\n",
      "step 148, train_loss 3.4986e-03, test_loss 4.9461e-03\n",
      "step 149, train_loss 3.4883e-03, test_loss 4.9379e-03\n",
      "step 150, train_loss 3.4784e-03, test_loss 4.9285e-03\n",
      "step 151, train_loss 3.4683e-03, test_loss 4.9179e-03\n",
      "step 152, train_loss 3.4578e-03, test_loss 4.9062e-03\n",
      "step 153, train_loss 3.4471e-03, test_loss 4.8943e-03\n",
      "step 154, train_loss 3.4368e-03, test_loss 4.8828e-03\n",
      "step 155, train_loss 3.4270e-03, test_loss 4.8722e-03\n",
      "step 156, train_loss 3.4180e-03, test_loss 4.8625e-03\n",
      "step 157, train_loss 3.4094e-03, test_loss 4.8537e-03\n",
      "step 158, train_loss 3.4008e-03, test_loss 4.8455e-03\n",
      "step 159, train_loss 3.3922e-03, test_loss 4.8378e-03\n",
      "step 160, train_loss 3.3835e-03, test_loss 4.8308e-03\n",
      "step 161, train_loss 3.3750e-03, test_loss 4.8243e-03\n",
      "step 162, train_loss 3.3669e-03, test_loss 4.8182e-03\n",
      "step 163, train_loss 3.3593e-03, test_loss 4.8120e-03\n",
      "step 164, train_loss 3.3519e-03, test_loss 4.8055e-03\n",
      "step 165, train_loss 3.3446e-03, test_loss 4.7986e-03\n",
      "step 166, train_loss 3.3374e-03, test_loss 4.7913e-03\n",
      "step 167, train_loss 3.3302e-03, test_loss 4.7840e-03\n",
      "step 168, train_loss 3.3231e-03, test_loss 4.7768e-03\n",
      "step 169, train_loss 3.3163e-03, test_loss 4.7700e-03\n",
      "step 170, train_loss 3.3097e-03, test_loss 4.7635e-03\n",
      "step 171, train_loss 3.3033e-03, test_loss 4.7575e-03\n",
      "step 172, train_loss 3.2970e-03, test_loss 4.7519e-03\n",
      "step 173, train_loss 3.2909e-03, test_loss 4.7466e-03\n",
      "step 174, train_loss 3.2848e-03, test_loss 4.7416e-03\n",
      "step 175, train_loss 3.2788e-03, test_loss 4.7367e-03\n",
      "step 176, train_loss 3.2730e-03, test_loss 4.7320e-03\n",
      "step 177, train_loss 3.2674e-03, test_loss 4.7273e-03\n",
      "step 178, train_loss 3.2620e-03, test_loss 4.7225e-03\n",
      "step 179, train_loss 3.2566e-03, test_loss 4.7177e-03\n",
      "step 180, train_loss 3.2513e-03, test_loss 4.7128e-03\n",
      "step 181, train_loss 3.2461e-03, test_loss 4.7079e-03\n",
      "step 182, train_loss 3.2410e-03, test_loss 4.7032e-03\n",
      "step 183, train_loss 3.2360e-03, test_loss 4.6987e-03\n",
      "step 184, train_loss 3.2311e-03, test_loss 4.6944e-03\n",
      "step 185, train_loss 3.2264e-03, test_loss 4.6904e-03\n",
      "step 186, train_loss 3.2217e-03, test_loss 4.6865e-03\n",
      "step 187, train_loss 3.2172e-03, test_loss 4.6828e-03\n",
      "step 188, train_loss 3.2127e-03, test_loss 4.6793e-03\n",
      "step 189, train_loss 3.2083e-03, test_loss 4.6758e-03\n",
      "step 190, train_loss 3.2041e-03, test_loss 4.6724e-03\n",
      "step 191, train_loss 3.1999e-03, test_loss 4.6689e-03\n",
      "step 192, train_loss 3.1958e-03, test_loss 4.6655e-03\n",
      "step 193, train_loss 3.1918e-03, test_loss 4.6621e-03\n",
      "step 194, train_loss 3.1878e-03, test_loss 4.6587e-03\n",
      "step 195, train_loss 3.1840e-03, test_loss 4.6554e-03\n",
      "step 196, train_loss 3.1802e-03, test_loss 4.6522e-03\n",
      "step 197, train_loss 3.1764e-03, test_loss 4.6491e-03\n",
      "step 198, train_loss 3.1728e-03, test_loss 4.6462e-03\n",
      "step 199, train_loss 3.1693e-03, test_loss 4.6434e-03\n",
      "step 200, train_loss 3.1658e-03, test_loss 4.6407e-03\n",
      "step 201, train_loss 3.1623e-03, test_loss 4.6380e-03\n",
      "step 202, train_loss 3.1590e-03, test_loss 4.6355e-03\n",
      "step 203, train_loss 3.1557e-03, test_loss 4.6329e-03\n",
      "step 204, train_loss 3.1525e-03, test_loss 4.6304e-03\n",
      "step 205, train_loss 3.1493e-03, test_loss 4.6279e-03\n",
      "step 206, train_loss 3.1462e-03, test_loss 4.6254e-03\n",
      "step 207, train_loss 3.1431e-03, test_loss 4.6230e-03\n",
      "step 208, train_loss 3.1402e-03, test_loss 4.6207e-03\n",
      "step 209, train_loss 3.1372e-03, test_loss 4.6184e-03\n",
      "step 210, train_loss 3.1343e-03, test_loss 4.6161e-03\n",
      "step 211, train_loss 3.1315e-03, test_loss 4.6140e-03\n",
      "step 212, train_loss 3.1287e-03, test_loss 4.6119e-03\n",
      "step 213, train_loss 3.1260e-03, test_loss 4.6098e-03\n",
      "step 214, train_loss 3.1234e-03, test_loss 4.6078e-03\n",
      "step 215, train_loss 3.1207e-03, test_loss 4.6059e-03\n",
      "step 216, train_loss 3.1182e-03, test_loss 4.6039e-03\n",
      "step 217, train_loss 3.1157e-03, test_loss 4.6021e-03\n",
      "step 218, train_loss 3.1132e-03, test_loss 4.6002e-03\n",
      "step 219, train_loss 3.1108e-03, test_loss 4.5984e-03\n",
      "step 220, train_loss 3.1084e-03, test_loss 4.5966e-03\n",
      "step 221, train_loss 3.1060e-03, test_loss 4.5949e-03\n",
      "step 222, train_loss 3.1037e-03, test_loss 4.5932e-03\n",
      "step 223, train_loss 3.1015e-03, test_loss 4.5916e-03\n",
      "step 224, train_loss 3.0992e-03, test_loss 4.5899e-03\n",
      "step 225, train_loss 3.0971e-03, test_loss 4.5884e-03\n",
      "step 226, train_loss 3.0949e-03, test_loss 4.5868e-03\n",
      "step 227, train_loss 3.0928e-03, test_loss 4.5853e-03\n",
      "step 228, train_loss 3.0908e-03, test_loss 4.5838e-03\n",
      "step 229, train_loss 3.0887e-03, test_loss 4.5824e-03\n",
      "step 230, train_loss 3.0867e-03, test_loss 4.5810e-03\n",
      "step 231, train_loss 3.0848e-03, test_loss 4.5796e-03\n",
      "step 232, train_loss 3.0829e-03, test_loss 4.5782e-03\n",
      "step 233, train_loss 3.0810e-03, test_loss 4.5769e-03\n",
      "step 234, train_loss 3.0791e-03, test_loss 4.5755e-03\n",
      "step 235, train_loss 3.0773e-03, test_loss 4.5743e-03\n",
      "step 236, train_loss 3.0755e-03, test_loss 4.5730e-03\n",
      "step 237, train_loss 3.0737e-03, test_loss 4.5718e-03\n",
      "step 238, train_loss 3.0720e-03, test_loss 4.5706e-03\n",
      "step 239, train_loss 3.0703e-03, test_loss 4.5694e-03\n",
      "step 240, train_loss 3.0686e-03, test_loss 4.5682e-03\n",
      "step 241, train_loss 3.0669e-03, test_loss 4.5671e-03\n",
      "step 242, train_loss 3.0653e-03, test_loss 4.5660e-03\n",
      "step 243, train_loss 3.0637e-03, test_loss 4.5649e-03\n",
      "step 244, train_loss 3.0621e-03, test_loss 4.5638e-03\n",
      "step 245, train_loss 3.0606e-03, test_loss 4.5627e-03\n",
      "step 246, train_loss 3.0590e-03, test_loss 4.5617e-03\n",
      "step 247, train_loss 3.0575e-03, test_loss 4.5607e-03\n",
      "step 248, train_loss 3.0561e-03, test_loss 4.5597e-03\n",
      "step 249, train_loss 3.0546e-03, test_loss 4.5587e-03\n",
      "step 250, train_loss 3.0532e-03, test_loss 4.5577e-03\n",
      "step 251, train_loss 3.0518e-03, test_loss 4.5568e-03\n",
      "step 252, train_loss 3.0504e-03, test_loss 4.5558e-03\n",
      "step 253, train_loss 3.0490e-03, test_loss 4.5549e-03\n",
      "step 254, train_loss 3.0477e-03, test_loss 4.5540e-03\n",
      "step 255, train_loss 3.0464e-03, test_loss 4.5531e-03\n",
      "step 256, train_loss 3.0451e-03, test_loss 4.5523e-03\n",
      "step 257, train_loss 3.0438e-03, test_loss 4.5514e-03\n",
      "step 258, train_loss 3.0425e-03, test_loss 4.5506e-03\n",
      "step 259, train_loss 3.0413e-03, test_loss 4.5498e-03\n",
      "step 260, train_loss 3.0401e-03, test_loss 4.5489e-03\n",
      "step 261, train_loss 3.0389e-03, test_loss 4.5481e-03\n",
      "step 262, train_loss 3.0377e-03, test_loss 4.5474e-03\n",
      "step 263, train_loss 3.0365e-03, test_loss 4.5466e-03\n",
      "step 264, train_loss 3.0354e-03, test_loss 4.5458e-03\n",
      "step 265, train_loss 3.0342e-03, test_loss 4.5451e-03\n",
      "step 266, train_loss 3.0331e-03, test_loss 4.5443e-03\n",
      "step 267, train_loss 3.0320e-03, test_loss 4.5436e-03\n",
      "step 268, train_loss 3.0309e-03, test_loss 4.5429e-03\n",
      "step 269, train_loss 3.0298e-03, test_loss 4.5422e-03\n",
      "step 270, train_loss 3.0288e-03, test_loss 4.5415e-03\n",
      "step 271, train_loss 3.0277e-03, test_loss 4.5408e-03\n",
      "step 272, train_loss 3.0267e-03, test_loss 4.5401e-03\n",
      "step 273, train_loss 3.0257e-03, test_loss 4.5395e-03\n",
      "step 274, train_loss 3.0247e-03, test_loss 4.5388e-03\n",
      "step 275, train_loss 3.0237e-03, test_loss 4.5382e-03\n",
      "step 276, train_loss 3.0227e-03, test_loss 4.5375e-03\n",
      "step 277, train_loss 3.0218e-03, test_loss 4.5369e-03\n",
      "step 278, train_loss 3.0208e-03, test_loss 4.5363e-03\n",
      "step 279, train_loss 3.0199e-03, test_loss 4.5357e-03\n",
      "step 280, train_loss 3.0190e-03, test_loss 4.5351e-03\n",
      "step 281, train_loss 3.0181e-03, test_loss 4.5345e-03\n",
      "step 282, train_loss 3.0172e-03, test_loss 4.5339e-03\n",
      "step 283, train_loss 3.0163e-03, test_loss 4.5333e-03\n",
      "step 284, train_loss 3.0154e-03, test_loss 4.5328e-03\n",
      "step 285, train_loss 3.0146e-03, test_loss 4.5322e-03\n",
      "step 286, train_loss 3.0137e-03, test_loss 4.5316e-03\n",
      "step 287, train_loss 3.0129e-03, test_loss 4.5311e-03\n",
      "step 288, train_loss 3.0121e-03, test_loss 4.5306e-03\n",
      "step 289, train_loss 3.0112e-03, test_loss 4.5300e-03\n",
      "step 290, train_loss 3.0104e-03, test_loss 4.5295e-03\n",
      "step 291, train_loss 3.0096e-03, test_loss 4.5290e-03\n",
      "step 292, train_loss 3.0089e-03, test_loss 4.5285e-03\n",
      "step 293, train_loss 3.0081e-03, test_loss 4.5279e-03\n",
      "step 294, train_loss 3.0073e-03, test_loss 4.5274e-03\n",
      "step 295, train_loss 3.0066e-03, test_loss 4.5269e-03\n",
      "step 296, train_loss 3.0058e-03, test_loss 4.5265e-03\n",
      "step 297, train_loss 3.0051e-03, test_loss 4.5260e-03\n",
      "step 298, train_loss 3.0043e-03, test_loss 4.5255e-03\n",
      "step 299, train_loss 3.0036e-03, test_loss 4.5250e-03\n",
      "step 300, train_loss 3.0029e-03, test_loss 4.5245e-03\n",
      "step 301, train_loss 3.0022e-03, test_loss 4.5241e-03\n",
      "step 302, train_loss 3.0015e-03, test_loss 4.5236e-03\n",
      "step 303, train_loss 3.0008e-03, test_loss 4.5232e-03\n",
      "step 304, train_loss 3.0001e-03, test_loss 4.5227e-03\n",
      "step 305, train_loss 2.9995e-03, test_loss 4.5223e-03\n",
      "step 306, train_loss 2.9988e-03, test_loss 4.5218e-03\n",
      "step 307, train_loss 2.9982e-03, test_loss 4.5214e-03\n",
      "step 308, train_loss 2.9975e-03, test_loss 4.5209e-03\n",
      "step 309, train_loss 2.9969e-03, test_loss 4.5205e-03\n",
      "step 310, train_loss 2.9962e-03, test_loss 4.5201e-03\n",
      "step 311, train_loss 2.9956e-03, test_loss 4.5197e-03\n",
      "step 312, train_loss 2.9950e-03, test_loss 4.5193e-03\n",
      "step 313, train_loss 2.9944e-03, test_loss 4.5188e-03\n",
      "step 314, train_loss 2.9938e-03, test_loss 4.5184e-03\n",
      "step 315, train_loss 2.9932e-03, test_loss 4.5180e-03\n",
      "step 316, train_loss 2.9926e-03, test_loss 4.5176e-03\n",
      "step 317, train_loss 2.9920e-03, test_loss 4.5172e-03\n",
      "step 318, train_loss 2.9914e-03, test_loss 4.5168e-03\n",
      "step 319, train_loss 2.9908e-03, test_loss 4.5164e-03\n",
      "step 320, train_loss 2.9903e-03, test_loss 4.5161e-03\n",
      "step 321, train_loss 2.9897e-03, test_loss 4.5157e-03\n",
      "step 322, train_loss 2.9892e-03, test_loss 4.5153e-03\n",
      "step 323, train_loss 2.9886e-03, test_loss 4.5149e-03\n",
      "step 324, train_loss 2.9881e-03, test_loss 4.5145e-03\n",
      "step 325, train_loss 2.9875e-03, test_loss 4.5142e-03\n",
      "step 326, train_loss 2.9870e-03, test_loss 4.5138e-03\n",
      "step 327, train_loss 2.9865e-03, test_loss 4.5134e-03\n",
      "step 328, train_loss 2.9860e-03, test_loss 4.5131e-03\n",
      "step 329, train_loss 2.9854e-03, test_loss 4.5127e-03\n",
      "step 330, train_loss 2.9849e-03, test_loss 4.5124e-03\n",
      "step 331, train_loss 2.9844e-03, test_loss 4.5120e-03\n",
      "step 332, train_loss 2.9839e-03, test_loss 4.5117e-03\n",
      "step 333, train_loss 2.9834e-03, test_loss 4.5113e-03\n",
      "step 334, train_loss 2.9829e-03, test_loss 4.5110e-03\n",
      "step 335, train_loss 2.9825e-03, test_loss 4.5106e-03\n",
      "step 336, train_loss 2.9820e-03, test_loss 4.5103e-03\n",
      "step 337, train_loss 2.9815e-03, test_loss 4.5099e-03\n",
      "step 338, train_loss 2.9810e-03, test_loss 4.5096e-03\n",
      "step 339, train_loss 2.9806e-03, test_loss 4.5093e-03\n",
      "step 340, train_loss 2.9801e-03, test_loss 4.5089e-03\n",
      "step 341, train_loss 2.9797e-03, test_loss 4.5086e-03\n",
      "step 342, train_loss 2.9792e-03, test_loss 4.5083e-03\n",
      "step 343, train_loss 2.9788e-03, test_loss 4.5080e-03\n",
      "step 344, train_loss 2.9783e-03, test_loss 4.5076e-03\n",
      "step 345, train_loss 2.9779e-03, test_loss 4.5073e-03\n",
      "step 346, train_loss 2.9774e-03, test_loss 4.5070e-03\n",
      "step 347, train_loss 2.9770e-03, test_loss 4.5067e-03\n",
      "step 348, train_loss 2.9766e-03, test_loss 4.5064e-03\n",
      "step 349, train_loss 2.9762e-03, test_loss 4.5061e-03\n",
      "step 350, train_loss 2.9757e-03, test_loss 4.5058e-03\n",
      "step 351, train_loss 2.9753e-03, test_loss 4.5055e-03\n",
      "step 352, train_loss 2.9749e-03, test_loss 4.5052e-03\n",
      "step 353, train_loss 2.9745e-03, test_loss 4.5049e-03\n",
      "step 354, train_loss 2.9741e-03, test_loss 4.5046e-03\n",
      "step 355, train_loss 2.9737e-03, test_loss 4.5043e-03\n",
      "step 356, train_loss 2.9733e-03, test_loss 4.5040e-03\n",
      "step 357, train_loss 2.9729e-03, test_loss 4.5037e-03\n",
      "step 358, train_loss 2.9725e-03, test_loss 4.5034e-03\n",
      "step 359, train_loss 2.9721e-03, test_loss 4.5031e-03\n",
      "step 360, train_loss 2.9717e-03, test_loss 4.5028e-03\n",
      "step 361, train_loss 2.9714e-03, test_loss 4.5025e-03\n",
      "step 362, train_loss 2.9710e-03, test_loss 4.5022e-03\n",
      "step 363, train_loss 2.9706e-03, test_loss 4.5019e-03\n",
      "step 364, train_loss 2.9702e-03, test_loss 4.5017e-03\n",
      "step 365, train_loss 2.9699e-03, test_loss 4.5014e-03\n",
      "step 366, train_loss 2.9695e-03, test_loss 4.5011e-03\n",
      "step 367, train_loss 2.9691e-03, test_loss 4.5008e-03\n",
      "step 368, train_loss 2.9688e-03, test_loss 4.5005e-03\n",
      "step 369, train_loss 2.9684e-03, test_loss 4.5003e-03\n",
      "step 370, train_loss 2.9681e-03, test_loss 4.5000e-03\n",
      "step 371, train_loss 2.9677e-03, test_loss 4.4997e-03\n",
      "step 372, train_loss 2.9674e-03, test_loss 4.4995e-03\n",
      "step 373, train_loss 2.9670e-03, test_loss 4.4992e-03\n",
      "step 374, train_loss 2.9667e-03, test_loss 4.4989e-03\n",
      "step 375, train_loss 2.9664e-03, test_loss 4.4987e-03\n",
      "step 376, train_loss 2.9660e-03, test_loss 4.4984e-03\n",
      "step 377, train_loss 2.9657e-03, test_loss 4.4981e-03\n",
      "step 378, train_loss 2.9654e-03, test_loss 4.4979e-03\n",
      "step 379, train_loss 2.9650e-03, test_loss 4.4976e-03\n",
      "step 380, train_loss 2.9647e-03, test_loss 4.4974e-03\n",
      "step 381, train_loss 2.9644e-03, test_loss 4.4971e-03\n",
      "step 382, train_loss 2.9641e-03, test_loss 4.4969e-03\n",
      "step 383, train_loss 2.9637e-03, test_loss 4.4966e-03\n",
      "step 384, train_loss 2.9634e-03, test_loss 4.4964e-03\n",
      "step 385, train_loss 2.9631e-03, test_loss 4.4961e-03\n",
      "step 386, train_loss 2.9628e-03, test_loss 4.4959e-03\n",
      "step 387, train_loss 2.9625e-03, test_loss 4.4956e-03\n",
      "step 388, train_loss 2.9622e-03, test_loss 4.4954e-03\n",
      "step 389, train_loss 2.9619e-03, test_loss 4.4951e-03\n",
      "step 390, train_loss 2.9616e-03, test_loss 4.4949e-03\n",
      "step 391, train_loss 2.9613e-03, test_loss 4.4946e-03\n",
      "step 392, train_loss 2.9610e-03, test_loss 4.4944e-03\n",
      "step 393, train_loss 2.9607e-03, test_loss 4.4942e-03\n",
      "step 394, train_loss 2.9604e-03, test_loss 4.4939e-03\n",
      "step 395, train_loss 2.9601e-03, test_loss 4.4937e-03\n",
      "step 396, train_loss 2.9599e-03, test_loss 4.4935e-03\n",
      "step 397, train_loss 2.9596e-03, test_loss 4.4933e-03\n",
      "step 398, train_loss 2.9594e-03, test_loss 4.4932e-03\n",
      "step 399, train_loss 2.9592e-03, test_loss 4.4933e-03\n",
      "step 400, train_loss 2.9592e-03, test_loss 4.4936e-03\n",
      "step 401, train_loss 2.9596e-03, test_loss 4.4951e-03\n",
      "step 402, train_loss 2.9608e-03, test_loss 4.4981e-03\n",
      "step 403, train_loss 2.9641e-03, test_loss 4.5066e-03\n",
      "step 404, train_loss 2.9723e-03, test_loss 4.5254e-03\n",
      "step 405, train_loss 2.9917e-03, test_loss 4.5726e-03\n",
      "step 406, train_loss 3.0382e-03, test_loss 4.6749e-03\n",
      "step 407, train_loss 3.1425e-03, test_loss 4.8903e-03\n",
      "step 408, train_loss 3.3572e-03, test_loss 5.2377e-03\n",
      "step 409, train_loss 3.7095e-03, test_loss 5.5932e-03\n",
      "step 410, train_loss 4.0637e-03, test_loss 5.5557e-03\n",
      "step 411, train_loss 4.0301e-03, test_loss 4.9992e-03\n",
      "step 412, train_loss 3.4673e-03, test_loss 4.5141e-03\n",
      "step 413, train_loss 2.9808e-03, test_loss 4.6514e-03\n",
      "step 414, train_loss 3.1192e-03, test_loss 5.0452e-03\n",
      "step 415, train_loss 3.5130e-03, test_loss 5.0134e-03\n",
      "step 416, train_loss 3.4831e-03, test_loss 4.6217e-03\n",
      "step 417, train_loss 3.0872e-03, test_loss 4.5089e-03\n",
      "step 418, train_loss 2.9741e-03, test_loss 4.7643e-03\n",
      "step 419, train_loss 3.2323e-03, test_loss 4.8521e-03\n",
      "step 420, train_loss 3.3189e-03, test_loss 4.6102e-03\n",
      "step 421, train_loss 3.0774e-03, test_loss 4.4907e-03\n",
      "step 422, train_loss 2.9569e-03, test_loss 4.6515e-03\n",
      "step 423, train_loss 3.1178e-03, test_loss 4.7240e-03\n",
      "step 424, train_loss 3.1920e-03, test_loss 4.5709e-03\n",
      "step 425, train_loss 3.0367e-03, test_loss 4.4890e-03\n",
      "step 426, train_loss 2.9543e-03, test_loss 4.5953e-03\n",
      "step 427, train_loss 3.0617e-03, test_loss 4.6427e-03\n",
      "step 428, train_loss 3.1083e-03, test_loss 4.5368e-03\n",
      "step 429, train_loss 3.0027e-03, test_loss 4.4879e-03\n",
      "step 430, train_loss 2.9535e-03, test_loss 4.5618e-03\n",
      "step 431, train_loss 3.0274e-03, test_loss 4.5859e-03\n",
      "step 432, train_loss 3.0526e-03, test_loss 4.5152e-03\n",
      "step 433, train_loss 2.9807e-03, test_loss 4.4873e-03\n",
      "step 434, train_loss 2.9527e-03, test_loss 4.5372e-03\n",
      "step 435, train_loss 3.0031e-03, test_loss 4.5512e-03\n",
      "step 436, train_loss 3.0164e-03, test_loss 4.5023e-03\n",
      "step 437, train_loss 2.9676e-03, test_loss 4.4863e-03\n",
      "step 438, train_loss 2.9513e-03, test_loss 4.5203e-03\n",
      "step 439, train_loss 2.9852e-03, test_loss 4.5272e-03\n",
      "step 440, train_loss 2.9927e-03, test_loss 4.4954e-03\n",
      "step 441, train_loss 2.9604e-03, test_loss 4.4846e-03\n",
      "step 442, train_loss 2.9496e-03, test_loss 4.5064e-03\n",
      "step 443, train_loss 2.9717e-03, test_loss 4.5125e-03\n",
      "step 444, train_loss 2.9774e-03, test_loss 4.4913e-03\n",
      "step 445, train_loss 2.9562e-03, test_loss 4.4832e-03\n",
      "step 446, train_loss 2.9478e-03, test_loss 4.4974e-03\n",
      "step 447, train_loss 2.9619e-03, test_loss 4.5022e-03\n",
      "step 448, train_loss 2.9670e-03, test_loss 4.4892e-03\n",
      "step 449, train_loss 2.9537e-03, test_loss 4.4820e-03\n",
      "step 450, train_loss 2.9464e-03, test_loss 4.4901e-03\n",
      "step 451, train_loss 2.9547e-03, test_loss 4.4953e-03\n",
      "step 452, train_loss 2.9598e-03, test_loss 4.4873e-03\n",
      "step 453, train_loss 2.9519e-03, test_loss 4.4811e-03\n",
      "step 454, train_loss 2.9455e-03, test_loss 4.4854e-03\n",
      "step 455, train_loss 2.9497e-03, test_loss 4.4899e-03\n",
      "step 456, train_loss 2.9543e-03, test_loss 4.4863e-03\n",
      "step 457, train_loss 2.9505e-03, test_loss 4.4809e-03\n",
      "step 458, train_loss 2.9451e-03, test_loss 4.4821e-03\n",
      "step 459, train_loss 2.9463e-03, test_loss 4.4859e-03\n",
      "step 460, train_loss 2.9500e-03, test_loss 4.4847e-03\n",
      "step 461, train_loss 2.9489e-03, test_loss 4.4808e-03\n",
      "step 462, train_loss 2.9449e-03, test_loss 4.4801e-03\n",
      "step 463, train_loss 2.9442e-03, test_loss 4.4825e-03\n",
      "step 464, train_loss 2.9466e-03, test_loss 4.4832e-03\n",
      "step 465, train_loss 2.9472e-03, test_loss 4.4807e-03\n",
      "step 466, train_loss 2.9448e-03, test_loss 4.4792e-03\n",
      "step 467, train_loss 2.9431e-03, test_loss 4.4802e-03\n",
      "step 468, train_loss 2.9441e-03, test_loss 4.4813e-03\n",
      "step 469, train_loss 2.9452e-03, test_loss 4.4804e-03\n",
      "step 470, train_loss 2.9443e-03, test_loss 4.4787e-03\n",
      "step 471, train_loss 2.9427e-03, test_loss 4.4785e-03\n",
      "step 472, train_loss 2.9424e-03, test_loss 4.4794e-03\n",
      "step 473, train_loss 2.9433e-03, test_loss 4.4795e-03\n",
      "step 474, train_loss 2.9434e-03, test_loss 4.4785e-03\n",
      "step 475, train_loss 2.9424e-03, test_loss 4.4777e-03\n",
      "step 476, train_loss 2.9416e-03, test_loss 4.4779e-03\n",
      "step 477, train_loss 2.9417e-03, test_loss 4.4784e-03\n",
      "step 478, train_loss 2.9421e-03, test_loss 4.4781e-03\n",
      "step 479, train_loss 2.9418e-03, test_loss 4.4773e-03\n",
      "step 480, train_loss 2.9411e-03, test_loss 4.4769e-03\n",
      "step 481, train_loss 2.9407e-03, test_loss 4.4770e-03\n",
      "step 482, train_loss 2.9408e-03, test_loss 4.4772e-03\n",
      "step 483, train_loss 2.9410e-03, test_loss 4.4769e-03\n",
      "step 484, train_loss 2.9406e-03, test_loss 4.4764e-03\n",
      "step 485, train_loss 2.9401e-03, test_loss 4.4761e-03\n",
      "step 486, train_loss 2.9399e-03, test_loss 4.4762e-03\n",
      "step 487, train_loss 2.9399e-03, test_loss 4.4762e-03\n",
      "step 488, train_loss 2.9399e-03, test_loss 4.4759e-03\n",
      "step 489, train_loss 2.9396e-03, test_loss 4.4755e-03\n",
      "step 490, train_loss 2.9393e-03, test_loss 4.4753e-03\n",
      "step 491, train_loss 2.9390e-03, test_loss 4.4753e-03\n",
      "step 492, train_loss 2.9390e-03, test_loss 4.4752e-03\n",
      "step 493, train_loss 2.9389e-03, test_loss 4.4750e-03\n",
      "step 494, train_loss 2.9387e-03, test_loss 4.4747e-03\n",
      "step 495, train_loss 2.9384e-03, test_loss 4.4745e-03\n",
      "step 496, train_loss 2.9382e-03, test_loss 4.4744e-03\n",
      "step 497, train_loss 2.9381e-03, test_loss 4.4744e-03\n",
      "step 498, train_loss 2.9380e-03, test_loss 4.4742e-03\n",
      "step 499, train_loss 2.9379e-03, test_loss 4.4740e-03\n",
      "step 500, train_loss 2.9376e-03, test_loss 4.4738e-03\n",
      "step 501, train_loss 2.9374e-03, test_loss 4.4736e-03\n",
      "step 502, train_loss 2.9373e-03, test_loss 4.4735e-03\n",
      "step 503, train_loss 2.9372e-03, test_loss 4.4734e-03\n",
      "step 504, train_loss 2.9370e-03, test_loss 4.4732e-03\n",
      "step 505, train_loss 2.9369e-03, test_loss 4.4730e-03\n",
      "step 506, train_loss 2.9367e-03, test_loss 4.4729e-03\n",
      "step 507, train_loss 2.9365e-03, test_loss 4.4727e-03\n",
      "step 508, train_loss 2.9364e-03, test_loss 4.4726e-03\n",
      "step 509, train_loss 2.9362e-03, test_loss 4.4725e-03\n",
      "step 510, train_loss 2.9361e-03, test_loss 4.4723e-03\n",
      "step 511, train_loss 2.9359e-03, test_loss 4.4721e-03\n",
      "step 512, train_loss 2.9358e-03, test_loss 4.4720e-03\n",
      "step 513, train_loss 2.9356e-03, test_loss 4.4719e-03\n",
      "step 514, train_loss 2.9355e-03, test_loss 4.4717e-03\n",
      "step 515, train_loss 2.9353e-03, test_loss 4.4716e-03\n",
      "step 516, train_loss 2.9352e-03, test_loss 4.4714e-03\n",
      "step 517, train_loss 2.9350e-03, test_loss 4.4713e-03\n",
      "step 518, train_loss 2.9349e-03, test_loss 4.4711e-03\n",
      "step 519, train_loss 2.9347e-03, test_loss 4.4710e-03\n",
      "step 520, train_loss 2.9346e-03, test_loss 4.4709e-03\n",
      "step 521, train_loss 2.9344e-03, test_loss 4.4707e-03\n",
      "step 522, train_loss 2.9343e-03, test_loss 4.4706e-03\n",
      "step 523, train_loss 2.9342e-03, test_loss 4.4704e-03\n",
      "step 524, train_loss 2.9340e-03, test_loss 4.4703e-03\n",
      "step 525, train_loss 2.9339e-03, test_loss 4.4702e-03\n",
      "step 526, train_loss 2.9337e-03, test_loss 4.4700e-03\n",
      "step 527, train_loss 2.9336e-03, test_loss 4.4699e-03\n",
      "step 528, train_loss 2.9334e-03, test_loss 4.4698e-03\n",
      "step 529, train_loss 2.9333e-03, test_loss 4.4696e-03\n",
      "step 530, train_loss 2.9332e-03, test_loss 4.4695e-03\n",
      "step 531, train_loss 2.9330e-03, test_loss 4.4694e-03\n",
      "step 532, train_loss 2.9329e-03, test_loss 4.4692e-03\n",
      "step 533, train_loss 2.9327e-03, test_loss 4.4691e-03\n",
      "step 534, train_loss 2.9326e-03, test_loss 4.4690e-03\n",
      "step 535, train_loss 2.9325e-03, test_loss 4.4688e-03\n",
      "step 536, train_loss 2.9323e-03, test_loss 4.4687e-03\n",
      "step 537, train_loss 2.9322e-03, test_loss 4.4686e-03\n",
      "step 538, train_loss 2.9321e-03, test_loss 4.4684e-03\n",
      "step 539, train_loss 2.9319e-03, test_loss 4.4683e-03\n",
      "step 540, train_loss 2.9318e-03, test_loss 4.4682e-03\n",
      "step 541, train_loss 2.9317e-03, test_loss 4.4680e-03\n",
      "step 542, train_loss 2.9315e-03, test_loss 4.4679e-03\n",
      "step 543, train_loss 2.9314e-03, test_loss 4.4678e-03\n",
      "step 544, train_loss 2.9313e-03, test_loss 4.4677e-03\n",
      "step 545, train_loss 2.9311e-03, test_loss 4.4675e-03\n",
      "step 546, train_loss 2.9310e-03, test_loss 4.4674e-03\n",
      "step 547, train_loss 2.9309e-03, test_loss 4.4673e-03\n",
      "step 548, train_loss 2.9307e-03, test_loss 4.4672e-03\n",
      "step 549, train_loss 2.9306e-03, test_loss 4.4670e-03\n",
      "step 550, train_loss 2.9305e-03, test_loss 4.4669e-03\n",
      "step 551, train_loss 2.9304e-03, test_loss 4.4668e-03\n",
      "step 552, train_loss 2.9302e-03, test_loss 4.4667e-03\n",
      "step 553, train_loss 2.9301e-03, test_loss 4.4665e-03\n",
      "step 554, train_loss 2.9300e-03, test_loss 4.4664e-03\n",
      "step 555, train_loss 2.9299e-03, test_loss 4.4663e-03\n",
      "step 556, train_loss 2.9297e-03, test_loss 4.4662e-03\n",
      "step 557, train_loss 2.9296e-03, test_loss 4.4660e-03\n",
      "step 558, train_loss 2.9295e-03, test_loss 4.4659e-03\n",
      "step 559, train_loss 2.9293e-03, test_loss 4.4658e-03\n",
      "step 560, train_loss 2.9292e-03, test_loss 4.4657e-03\n",
      "step 561, train_loss 2.9291e-03, test_loss 4.4656e-03\n",
      "step 562, train_loss 2.9290e-03, test_loss 4.4654e-03\n",
      "step 563, train_loss 2.9289e-03, test_loss 4.4653e-03\n",
      "step 564, train_loss 2.9287e-03, test_loss 4.4652e-03\n",
      "step 565, train_loss 2.9286e-03, test_loss 4.4651e-03\n",
      "step 566, train_loss 2.9285e-03, test_loss 4.4650e-03\n",
      "step 567, train_loss 2.9284e-03, test_loss 4.4649e-03\n",
      "step 568, train_loss 2.9282e-03, test_loss 4.4647e-03\n",
      "step 569, train_loss 2.9281e-03, test_loss 4.4646e-03\n",
      "step 570, train_loss 2.9280e-03, test_loss 4.4645e-03\n",
      "step 571, train_loss 2.9279e-03, test_loss 4.4644e-03\n",
      "step 572, train_loss 2.9278e-03, test_loss 4.4643e-03\n",
      "step 573, train_loss 2.9276e-03, test_loss 4.4642e-03\n",
      "step 574, train_loss 2.9275e-03, test_loss 4.4640e-03\n",
      "step 575, train_loss 2.9274e-03, test_loss 4.4639e-03\n",
      "step 576, train_loss 2.9273e-03, test_loss 4.4638e-03\n",
      "step 577, train_loss 2.9272e-03, test_loss 4.4637e-03\n",
      "step 578, train_loss 2.9271e-03, test_loss 4.4636e-03\n",
      "step 579, train_loss 2.9269e-03, test_loss 4.4635e-03\n",
      "step 580, train_loss 2.9268e-03, test_loss 4.4634e-03\n",
      "step 581, train_loss 2.9267e-03, test_loss 4.4632e-03\n",
      "step 582, train_loss 2.9266e-03, test_loss 4.4631e-03\n",
      "step 583, train_loss 2.9265e-03, test_loss 4.4630e-03\n",
      "step 584, train_loss 2.9264e-03, test_loss 4.4629e-03\n",
      "step 585, train_loss 2.9263e-03, test_loss 4.4628e-03\n",
      "step 586, train_loss 2.9261e-03, test_loss 4.4627e-03\n",
      "step 587, train_loss 2.9260e-03, test_loss 4.4626e-03\n",
      "step 588, train_loss 2.9259e-03, test_loss 4.4625e-03\n",
      "step 589, train_loss 2.9258e-03, test_loss 4.4624e-03\n",
      "step 590, train_loss 2.9257e-03, test_loss 4.4623e-03\n",
      "step 591, train_loss 2.9256e-03, test_loss 4.4621e-03\n",
      "step 592, train_loss 2.9255e-03, test_loss 4.4620e-03\n",
      "step 593, train_loss 2.9254e-03, test_loss 4.4619e-03\n",
      "step 594, train_loss 2.9253e-03, test_loss 4.4618e-03\n",
      "step 595, train_loss 2.9252e-03, test_loss 4.4617e-03\n",
      "step 596, train_loss 2.9251e-03, test_loss 4.4616e-03\n",
      "step 597, train_loss 2.9250e-03, test_loss 4.4616e-03\n",
      "step 598, train_loss 2.9249e-03, test_loss 4.4615e-03\n",
      "step 599, train_loss 2.9249e-03, test_loss 4.4616e-03\n",
      "step 600, train_loss 2.9248e-03, test_loss 4.4616e-03\n",
      "step 601, train_loss 2.9250e-03, test_loss 4.4620e-03\n",
      "step 602, train_loss 2.9252e-03, test_loss 4.4624e-03\n",
      "step 603, train_loss 2.9258e-03, test_loss 4.4638e-03\n",
      "step 604, train_loss 2.9270e-03, test_loss 4.4659e-03\n",
      "step 605, train_loss 2.9294e-03, test_loss 4.4709e-03\n",
      "step 606, train_loss 2.9341e-03, test_loss 4.4796e-03\n",
      "step 607, train_loss 2.9433e-03, test_loss 4.4980e-03\n",
      "step 608, train_loss 2.9611e-03, test_loss 4.5321e-03\n",
      "step 609, train_loss 2.9964e-03, test_loss 4.6015e-03\n",
      "step 610, train_loss 3.0648e-03, test_loss 4.7300e-03\n",
      "step 611, train_loss 3.1961e-03, test_loss 4.9704e-03\n",
      "step 612, train_loss 3.4351e-03, test_loss 5.3593e-03\n",
      "step 613, train_loss 3.8302e-03, test_loss 5.8824e-03\n",
      "step 614, train_loss 4.3515e-03, test_loss 6.2708e-03\n",
      "step 615, train_loss 4.7485e-03, test_loss 6.0930e-03\n",
      "step 616, train_loss 4.5640e-03, test_loss 5.2577e-03\n",
      "step 617, train_loss 3.7293e-03, test_loss 4.5331e-03\n",
      "step 618, train_loss 2.9977e-03, test_loss 4.6030e-03\n",
      "step 619, train_loss 3.0672e-03, test_loss 5.1570e-03\n",
      "step 620, train_loss 3.6264e-03, test_loss 5.3573e-03\n",
      "step 621, train_loss 3.8234e-03, test_loss 4.9126e-03\n",
      "step 622, train_loss 3.3801e-03, test_loss 4.4799e-03\n",
      "step 623, train_loss 2.9434e-03, test_loss 4.6099e-03\n",
      "step 624, train_loss 3.0741e-03, test_loss 4.9516e-03\n",
      "step 625, train_loss 3.4208e-03, test_loss 4.9104e-03\n",
      "step 626, train_loss 3.3762e-03, test_loss 4.5658e-03\n",
      "step 627, train_loss 3.0322e-03, test_loss 4.4731e-03\n",
      "step 628, train_loss 2.9381e-03, test_loss 4.6942e-03\n",
      "step 629, train_loss 3.1585e-03, test_loss 4.7847e-03\n",
      "step 630, train_loss 3.2519e-03, test_loss 4.5974e-03\n",
      "step 631, train_loss 3.0610e-03, test_loss 4.4562e-03\n",
      "step 632, train_loss 2.9201e-03, test_loss 4.5588e-03\n",
      "step 633, train_loss 3.0243e-03, test_loss 4.6708e-03\n",
      "step 634, train_loss 3.1352e-03, test_loss 4.5839e-03\n",
      "step 635, train_loss 3.0501e-03, test_loss 4.4628e-03\n",
      "step 636, train_loss 2.9272e-03, test_loss 4.4965e-03\n",
      "step 637, train_loss 2.9605e-03, test_loss 4.5853e-03\n",
      "step 638, train_loss 3.0511e-03, test_loss 4.5616e-03\n",
      "step 639, train_loss 3.0252e-03, test_loss 4.4717e-03\n",
      "step 640, train_loss 2.9358e-03, test_loss 4.4676e-03\n",
      "step 641, train_loss 2.9316e-03, test_loss 4.5306e-03\n",
      "step 642, train_loss 2.9940e-03, test_loss 4.5344e-03\n",
      "step 643, train_loss 2.9993e-03, test_loss 4.4780e-03\n",
      "step 644, train_loss 2.9416e-03, test_loss 4.4566e-03\n",
      "step 645, train_loss 2.9204e-03, test_loss 4.4926e-03\n",
      "step 646, train_loss 2.9572e-03, test_loss 4.5121e-03\n",
      "step 647, train_loss 2.9753e-03, test_loss 4.4800e-03\n",
      "step 648, train_loss 2.9440e-03, test_loss 4.4552e-03\n",
      "step 649, train_loss 2.9184e-03, test_loss 4.4718e-03\n",
      "step 650, train_loss 2.9346e-03, test_loss 4.4912e-03\n",
      "step 651, train_loss 2.9551e-03, test_loss 4.4799e-03\n",
      "step 652, train_loss 2.9428e-03, test_loss 4.4572e-03\n",
      "step 653, train_loss 2.9206e-03, test_loss 4.4592e-03\n",
      "step 654, train_loss 2.9227e-03, test_loss 4.4758e-03\n",
      "step 655, train_loss 2.9387e-03, test_loss 4.4752e-03\n",
      "step 656, train_loss 2.9388e-03, test_loss 4.4609e-03\n",
      "step 657, train_loss 2.9237e-03, test_loss 4.4551e-03\n",
      "step 658, train_loss 2.9180e-03, test_loss 4.4639e-03\n",
      "step 659, train_loss 2.9270e-03, test_loss 4.4702e-03\n",
      "step 660, train_loss 2.9327e-03, test_loss 4.4626e-03\n",
      "step 661, train_loss 2.9257e-03, test_loss 4.4550e-03\n",
      "step 662, train_loss 2.9178e-03, test_loss 4.4572e-03\n",
      "step 663, train_loss 2.9199e-03, test_loss 4.4629e-03\n",
      "step 664, train_loss 2.9261e-03, test_loss 4.4628e-03\n",
      "step 665, train_loss 2.9254e-03, test_loss 4.4565e-03\n",
      "step 666, train_loss 2.9194e-03, test_loss 4.4544e-03\n",
      "step 667, train_loss 2.9171e-03, test_loss 4.4580e-03\n",
      "step 668, train_loss 2.9205e-03, test_loss 4.4601e-03\n",
      "step 669, train_loss 2.9230e-03, test_loss 4.4581e-03\n",
      "step 670, train_loss 2.9205e-03, test_loss 4.4544e-03\n",
      "step 671, train_loss 2.9171e-03, test_loss 4.4545e-03\n",
      "step 672, train_loss 2.9172e-03, test_loss 4.4571e-03\n",
      "step 673, train_loss 2.9196e-03, test_loss 4.4572e-03\n",
      "step 674, train_loss 2.9201e-03, test_loss 4.4555e-03\n",
      "step 675, train_loss 2.9180e-03, test_loss 4.4537e-03\n",
      "step 676, train_loss 2.9164e-03, test_loss 4.4543e-03\n",
      "step 677, train_loss 2.9170e-03, test_loss 4.4559e-03\n",
      "step 678, train_loss 2.9183e-03, test_loss 4.4554e-03\n",
      "step 679, train_loss 2.9182e-03, test_loss 4.4542e-03\n",
      "step 680, train_loss 2.9167e-03, test_loss 4.4533e-03\n",
      "step 681, train_loss 2.9160e-03, test_loss 4.4538e-03\n",
      "step 682, train_loss 2.9165e-03, test_loss 4.4546e-03\n",
      "step 683, train_loss 2.9172e-03, test_loss 4.4541e-03\n",
      "step 684, train_loss 2.9169e-03, test_loss 4.4534e-03\n",
      "step 685, train_loss 2.9160e-03, test_loss 4.4529e-03\n",
      "step 686, train_loss 2.9156e-03, test_loss 4.4532e-03\n",
      "step 687, train_loss 2.9159e-03, test_loss 4.4537e-03\n",
      "step 688, train_loss 2.9163e-03, test_loss 4.4533e-03\n",
      "step 689, train_loss 2.9161e-03, test_loss 4.4528e-03\n",
      "step 690, train_loss 2.9155e-03, test_loss 4.4525e-03\n",
      "step 691, train_loss 2.9152e-03, test_loss 4.4525e-03\n",
      "step 692, train_loss 2.9153e-03, test_loss 4.4528e-03\n",
      "step 693, train_loss 2.9155e-03, test_loss 4.4526e-03\n",
      "step 694, train_loss 2.9154e-03, test_loss 4.4524e-03\n",
      "step 695, train_loss 2.9151e-03, test_loss 4.4520e-03\n",
      "step 696, train_loss 2.9148e-03, test_loss 4.4520e-03\n",
      "step 697, train_loss 2.9148e-03, test_loss 4.4522e-03\n",
      "step 698, train_loss 2.9149e-03, test_loss 4.4520e-03\n",
      "step 699, train_loss 2.9149e-03, test_loss 4.4519e-03\n",
      "step 700, train_loss 2.9147e-03, test_loss 4.4516e-03\n",
      "step 701, train_loss 2.9145e-03, test_loss 4.4515e-03\n",
      "step 702, train_loss 2.9144e-03, test_loss 4.4516e-03\n",
      "step 703, train_loss 2.9144e-03, test_loss 4.4515e-03\n",
      "step 704, train_loss 2.9144e-03, test_loss 4.4514e-03\n",
      "step 705, train_loss 2.9143e-03, test_loss 4.4512e-03\n",
      "step 706, train_loss 2.9141e-03, test_loss 4.4511e-03\n",
      "step 707, train_loss 2.9140e-03, test_loss 4.4510e-03\n",
      "step 708, train_loss 2.9139e-03, test_loss 4.4509e-03\n",
      "step 709, train_loss 2.9139e-03, test_loss 4.4509e-03\n",
      "step 710, train_loss 2.9138e-03, test_loss 4.4507e-03\n",
      "step 711, train_loss 2.9137e-03, test_loss 4.4506e-03\n",
      "step 712, train_loss 2.9136e-03, test_loss 4.4505e-03\n",
      "step 713, train_loss 2.9135e-03, test_loss 4.4505e-03\n",
      "step 714, train_loss 2.9135e-03, test_loss 4.4505e-03\n",
      "step 715, train_loss 2.9134e-03, test_loss 4.4503e-03\n",
      "step 716, train_loss 2.9133e-03, test_loss 4.4502e-03\n",
      "step 717, train_loss 2.9132e-03, test_loss 4.4501e-03\n",
      "step 718, train_loss 2.9131e-03, test_loss 4.4500e-03\n",
      "step 719, train_loss 2.9131e-03, test_loss 4.4500e-03\n",
      "step 720, train_loss 2.9130e-03, test_loss 4.4499e-03\n",
      "step 721, train_loss 2.9129e-03, test_loss 4.4498e-03\n",
      "step 722, train_loss 2.9129e-03, test_loss 4.4497e-03\n",
      "step 723, train_loss 2.9128e-03, test_loss 4.4496e-03\n",
      "step 724, train_loss 2.9127e-03, test_loss 4.4495e-03\n",
      "step 725, train_loss 2.9126e-03, test_loss 4.4495e-03\n",
      "step 726, train_loss 2.9125e-03, test_loss 4.4494e-03\n",
      "step 727, train_loss 2.9125e-03, test_loss 4.4493e-03\n",
      "step 728, train_loss 2.9124e-03, test_loss 4.4492e-03\n",
      "step 729, train_loss 2.9123e-03, test_loss 4.4491e-03\n",
      "step 730, train_loss 2.9122e-03, test_loss 4.4491e-03\n",
      "step 731, train_loss 2.9122e-03, test_loss 4.4490e-03\n",
      "step 732, train_loss 2.9121e-03, test_loss 4.4489e-03\n",
      "step 733, train_loss 2.9120e-03, test_loss 4.4488e-03\n",
      "step 734, train_loss 2.9119e-03, test_loss 4.4487e-03\n",
      "step 735, train_loss 2.9119e-03, test_loss 4.4487e-03\n",
      "step 736, train_loss 2.9118e-03, test_loss 4.4486e-03\n",
      "step 737, train_loss 2.9117e-03, test_loss 4.4485e-03\n",
      "step 738, train_loss 2.9116e-03, test_loss 4.4484e-03\n",
      "step 739, train_loss 2.9116e-03, test_loss 4.4484e-03\n",
      "step 740, train_loss 2.9115e-03, test_loss 4.4483e-03\n",
      "step 741, train_loss 2.9114e-03, test_loss 4.4482e-03\n",
      "step 742, train_loss 2.9114e-03, test_loss 4.4481e-03\n",
      "step 743, train_loss 2.9113e-03, test_loss 4.4481e-03\n",
      "step 744, train_loss 2.9112e-03, test_loss 4.4480e-03\n",
      "step 745, train_loss 2.9111e-03, test_loss 4.4479e-03\n",
      "step 746, train_loss 2.9111e-03, test_loss 4.4478e-03\n",
      "step 747, train_loss 2.9110e-03, test_loss 4.4478e-03\n",
      "step 748, train_loss 2.9109e-03, test_loss 4.4477e-03\n",
      "step 749, train_loss 2.9108e-03, test_loss 4.4476e-03\n",
      "step 750, train_loss 2.9108e-03, test_loss 4.4475e-03\n",
      "step 751, train_loss 2.9107e-03, test_loss 4.4475e-03\n",
      "step 752, train_loss 2.9106e-03, test_loss 4.4474e-03\n",
      "step 753, train_loss 2.9105e-03, test_loss 4.4473e-03\n",
      "step 754, train_loss 2.9105e-03, test_loss 4.4472e-03\n",
      "step 755, train_loss 2.9104e-03, test_loss 4.4472e-03\n",
      "step 756, train_loss 2.9103e-03, test_loss 4.4471e-03\n",
      "step 757, train_loss 2.9103e-03, test_loss 4.4470e-03\n",
      "step 758, train_loss 2.9102e-03, test_loss 4.4470e-03\n",
      "step 759, train_loss 2.9101e-03, test_loss 4.4469e-03\n",
      "step 760, train_loss 2.9100e-03, test_loss 4.4468e-03\n",
      "step 761, train_loss 2.9100e-03, test_loss 4.4467e-03\n",
      "step 762, train_loss 2.9099e-03, test_loss 4.4467e-03\n",
      "step 763, train_loss 2.9098e-03, test_loss 4.4466e-03\n",
      "step 764, train_loss 2.9098e-03, test_loss 4.4465e-03\n",
      "step 765, train_loss 2.9097e-03, test_loss 4.4465e-03\n",
      "step 766, train_loss 2.9096e-03, test_loss 4.4464e-03\n",
      "step 767, train_loss 2.9095e-03, test_loss 4.4463e-03\n",
      "step 768, train_loss 2.9095e-03, test_loss 4.4462e-03\n",
      "step 769, train_loss 2.9094e-03, test_loss 4.4462e-03\n",
      "step 770, train_loss 2.9093e-03, test_loss 4.4461e-03\n",
      "step 771, train_loss 2.9093e-03, test_loss 4.4460e-03\n",
      "step 772, train_loss 2.9092e-03, test_loss 4.4460e-03\n",
      "step 773, train_loss 2.9091e-03, test_loss 4.4459e-03\n",
      "step 774, train_loss 2.9091e-03, test_loss 4.4458e-03\n",
      "step 775, train_loss 2.9090e-03, test_loss 4.4457e-03\n",
      "step 776, train_loss 2.9089e-03, test_loss 4.4457e-03\n",
      "step 777, train_loss 2.9088e-03, test_loss 4.4456e-03\n",
      "step 778, train_loss 2.9088e-03, test_loss 4.4455e-03\n",
      "step 779, train_loss 2.9087e-03, test_loss 4.4455e-03\n",
      "step 780, train_loss 2.9086e-03, test_loss 4.4454e-03\n",
      "step 781, train_loss 2.9086e-03, test_loss 4.4453e-03\n",
      "step 782, train_loss 2.9085e-03, test_loss 4.4453e-03\n",
      "step 783, train_loss 2.9084e-03, test_loss 4.4452e-03\n",
      "step 784, train_loss 2.9084e-03, test_loss 4.4451e-03\n",
      "step 785, train_loss 2.9083e-03, test_loss 4.4450e-03\n",
      "step 786, train_loss 2.9082e-03, test_loss 4.4450e-03\n",
      "step 787, train_loss 2.9082e-03, test_loss 4.4449e-03\n",
      "step 788, train_loss 2.9081e-03, test_loss 4.4448e-03\n",
      "step 789, train_loss 2.9080e-03, test_loss 4.4448e-03\n",
      "step 790, train_loss 2.9080e-03, test_loss 4.4447e-03\n",
      "step 791, train_loss 2.9079e-03, test_loss 4.4446e-03\n",
      "step 792, train_loss 2.9078e-03, test_loss 4.4446e-03\n",
      "step 793, train_loss 2.9078e-03, test_loss 4.4445e-03\n",
      "step 794, train_loss 2.9077e-03, test_loss 4.4444e-03\n",
      "step 795, train_loss 2.9076e-03, test_loss 4.4444e-03\n",
      "step 796, train_loss 2.9076e-03, test_loss 4.4443e-03\n",
      "step 797, train_loss 2.9075e-03, test_loss 4.4442e-03\n",
      "step 798, train_loss 2.9074e-03, test_loss 4.4442e-03\n",
      "step 799, train_loss 2.9074e-03, test_loss 4.4441e-03\n",
      "step 800, train_loss 2.9073e-03, test_loss 4.4440e-03\n",
      "step 801, train_loss 2.9072e-03, test_loss 4.4440e-03\n",
      "step 802, train_loss 2.9072e-03, test_loss 4.4439e-03\n",
      "step 803, train_loss 2.9071e-03, test_loss 4.4438e-03\n",
      "step 804, train_loss 2.9070e-03, test_loss 4.4438e-03\n",
      "step 805, train_loss 2.9070e-03, test_loss 4.4437e-03\n",
      "step 806, train_loss 2.9069e-03, test_loss 4.4436e-03\n",
      "step 807, train_loss 2.9068e-03, test_loss 4.4436e-03\n",
      "step 808, train_loss 2.9068e-03, test_loss 4.4435e-03\n",
      "step 809, train_loss 2.9067e-03, test_loss 4.4434e-03\n",
      "step 810, train_loss 2.9066e-03, test_loss 4.4434e-03\n",
      "step 811, train_loss 2.9066e-03, test_loss 4.4433e-03\n",
      "step 812, train_loss 2.9065e-03, test_loss 4.4432e-03\n",
      "step 813, train_loss 2.9064e-03, test_loss 4.4432e-03\n",
      "step 814, train_loss 2.9064e-03, test_loss 4.4431e-03\n",
      "step 815, train_loss 2.9063e-03, test_loss 4.4430e-03\n",
      "step 816, train_loss 2.9062e-03, test_loss 4.4430e-03\n",
      "step 817, train_loss 2.9062e-03, test_loss 4.4429e-03\n",
      "step 818, train_loss 2.9061e-03, test_loss 4.4428e-03\n",
      "step 819, train_loss 2.9061e-03, test_loss 4.4428e-03\n",
      "step 820, train_loss 2.9060e-03, test_loss 4.4427e-03\n",
      "step 821, train_loss 2.9060e-03, test_loss 4.4428e-03\n",
      "step 822, train_loss 2.9059e-03, test_loss 4.4426e-03\n",
      "step 823, train_loss 2.9059e-03, test_loss 4.4428e-03\n",
      "step 824, train_loss 2.9059e-03, test_loss 4.4427e-03\n",
      "step 825, train_loss 2.9061e-03, test_loss 4.4433e-03\n",
      "step 826, train_loss 2.9063e-03, test_loss 4.4435e-03\n",
      "step 827, train_loss 2.9069e-03, test_loss 4.4451e-03\n",
      "step 828, train_loss 2.9080e-03, test_loss 4.4466e-03\n",
      "step 829, train_loss 2.9103e-03, test_loss 4.4518e-03\n",
      "step 830, train_loss 2.9145e-03, test_loss 4.4589e-03\n",
      "step 831, train_loss 2.9229e-03, test_loss 4.4769e-03\n",
      "step 832, train_loss 2.9392e-03, test_loss 4.5064e-03\n",
      "step 833, train_loss 2.9715e-03, test_loss 4.5726e-03\n",
      "step 834, train_loss 3.0344e-03, test_loss 4.6899e-03\n",
      "step 835, train_loss 3.1574e-03, test_loss 4.9288e-03\n",
      "step 836, train_loss 3.3907e-03, test_loss 5.3369e-03\n",
      "step 837, train_loss 3.8108e-03, test_loss 6.0149e-03\n",
      "step 838, train_loss 4.4799e-03, test_loss 6.8131e-03\n",
      "step 839, train_loss 5.2991e-03, test_loss 7.3008e-03\n",
      "step 840, train_loss 5.7715e-03, test_loss 6.7164e-03\n",
      "step 841, train_loss 5.2025e-03, test_loss 5.3247e-03\n",
      "step 842, train_loss 3.7905e-03, test_loss 4.4551e-03\n",
      "step 843, train_loss 2.9204e-03, test_loss 4.8829e-03\n",
      "step 844, train_loss 3.3538e-03, test_loss 5.7296e-03\n",
      "step 845, train_loss 4.1937e-03, test_loss 5.6888e-03\n",
      "step 846, train_loss 4.1641e-03, test_loss 4.8574e-03\n",
      "step 847, train_loss 3.3206e-03, test_loss 4.4470e-03\n",
      "step 848, train_loss 2.9102e-03, test_loss 4.8936e-03\n",
      "step 849, train_loss 3.3651e-03, test_loss 5.2779e-03\n",
      "step 850, train_loss 3.7433e-03, test_loss 4.9155e-03\n",
      "step 851, train_loss 3.3867e-03, test_loss 4.4627e-03\n",
      "step 852, train_loss 2.9281e-03, test_loss 4.6087e-03\n",
      "step 853, train_loss 3.0719e-03, test_loss 4.9396e-03\n",
      "step 854, train_loss 3.4101e-03, test_loss 4.8268e-03\n",
      "step 855, train_loss 3.2901e-03, test_loss 4.4865e-03\n",
      "step 856, train_loss 2.9514e-03, test_loss 4.5016e-03\n",
      "step 857, train_loss 2.9680e-03, test_loss 4.7424e-03\n",
      "step 858, train_loss 3.2061e-03, test_loss 4.7106e-03\n",
      "step 859, train_loss 3.1796e-03, test_loss 4.4871e-03\n",
      "step 860, train_loss 2.9519e-03, test_loss 4.4660e-03\n",
      "step 861, train_loss 2.9300e-03, test_loss 4.6207e-03\n",
      "step 862, train_loss 3.0883e-03, test_loss 4.6295e-03\n",
      "step 863, train_loss 3.0927e-03, test_loss 4.4791e-03\n",
      "step 864, train_loss 2.9438e-03, test_loss 4.4488e-03\n",
      "step 865, train_loss 2.9132e-03, test_loss 4.5530e-03\n",
      "step 866, train_loss 3.0159e-03, test_loss 4.5655e-03\n",
      "step 867, train_loss 3.0319e-03, test_loss 4.4721e-03\n",
      "step 868, train_loss 2.9357e-03, test_loss 4.4415e-03\n",
      "step 869, train_loss 2.9053e-03, test_loss 4.5052e-03\n",
      "step 870, train_loss 2.9708e-03, test_loss 4.5266e-03\n",
      "step 871, train_loss 2.9895e-03, test_loss 4.4645e-03\n",
      "step 872, train_loss 2.9289e-03, test_loss 4.4383e-03\n",
      "step 873, train_loss 2.9016e-03, test_loss 4.4793e-03\n",
      "step 874, train_loss 2.9418e-03, test_loss 4.4962e-03\n",
      "step 875, train_loss 2.9607e-03, test_loss 4.4614e-03\n",
      "step 876, train_loss 2.9239e-03, test_loss 4.4370e-03\n",
      "step 877, train_loss 2.9003e-03, test_loss 4.4593e-03\n",
      "step 878, train_loss 2.9232e-03, test_loss 4.4780e-03\n",
      "step 879, train_loss 2.9404e-03, test_loss 4.4561e-03\n",
      "step 880, train_loss 2.9199e-03, test_loss 4.4378e-03\n",
      "step 881, train_loss 2.9004e-03, test_loss 4.4493e-03\n",
      "step 882, train_loss 2.9117e-03, test_loss 4.4628e-03\n",
      "step 883, train_loss 2.9262e-03, test_loss 4.4546e-03\n",
      "step 884, train_loss 2.9165e-03, test_loss 4.4383e-03\n",
      "step 885, train_loss 2.9012e-03, test_loss 4.4421e-03\n",
      "step 886, train_loss 2.9049e-03, test_loss 4.4538e-03\n",
      "step 887, train_loss 2.9159e-03, test_loss 4.4499e-03\n",
      "step 888, train_loss 2.9132e-03, test_loss 4.4403e-03\n",
      "step 889, train_loss 2.9024e-03, test_loss 4.4390e-03\n",
      "step 890, train_loss 2.9014e-03, test_loss 4.4461e-03\n",
      "step 891, train_loss 2.9088e-03, test_loss 4.4482e-03\n",
      "step 892, train_loss 2.9099e-03, test_loss 4.4406e-03\n",
      "step 893, train_loss 2.9033e-03, test_loss 4.4380e-03\n",
      "step 894, train_loss 2.9001e-03, test_loss 4.4419e-03\n",
      "step 895, train_loss 2.9040e-03, test_loss 4.4439e-03\n",
      "step 896, train_loss 2.9068e-03, test_loss 4.4418e-03\n",
      "step 897, train_loss 2.9036e-03, test_loss 4.4376e-03\n",
      "step 898, train_loss 2.9001e-03, test_loss 4.4389e-03\n",
      "step 899, train_loss 2.9012e-03, test_loss 4.4420e-03\n",
      "step 900, train_loss 2.9039e-03, test_loss 4.4407e-03\n",
      "step 901, train_loss 2.9033e-03, test_loss 4.4387e-03\n",
      "step 902, train_loss 2.9005e-03, test_loss 4.4376e-03\n",
      "step 903, train_loss 2.8999e-03, test_loss 4.4391e-03\n",
      "step 904, train_loss 2.9016e-03, test_loss 4.4404e-03\n",
      "step 905, train_loss 2.9023e-03, test_loss 4.4383e-03\n",
      "step 906, train_loss 2.9010e-03, test_loss 4.4375e-03\n",
      "step 907, train_loss 2.8996e-03, test_loss 4.4379e-03\n",
      "step 908, train_loss 2.9001e-03, test_loss 4.4386e-03\n",
      "step 909, train_loss 2.9011e-03, test_loss 4.4389e-03\n",
      "step 910, train_loss 2.9009e-03, test_loss 4.4373e-03\n",
      "step 911, train_loss 2.8998e-03, test_loss 4.4372e-03\n",
      "step 912, train_loss 2.8994e-03, test_loss 4.4377e-03\n",
      "step 913, train_loss 2.9000e-03, test_loss 4.4377e-03\n",
      "step 914, train_loss 2.9004e-03, test_loss 4.4378e-03\n",
      "step 915, train_loss 2.8999e-03, test_loss 4.4367e-03\n",
      "step 916, train_loss 2.8993e-03, test_loss 4.4368e-03\n",
      "step 917, train_loss 2.8993e-03, test_loss 4.4374e-03\n",
      "step 918, train_loss 2.8997e-03, test_loss 4.4370e-03\n",
      "step 919, train_loss 2.8997e-03, test_loss 4.4371e-03\n",
      "step 920, train_loss 2.8994e-03, test_loss 4.4364e-03\n",
      "step 921, train_loss 2.8990e-03, test_loss 4.4365e-03\n",
      "step 922, train_loss 2.8991e-03, test_loss 4.4369e-03\n",
      "step 923, train_loss 2.8993e-03, test_loss 4.4364e-03\n",
      "step 924, train_loss 2.8993e-03, test_loss 4.4365e-03\n",
      "step 925, train_loss 2.8990e-03, test_loss 4.4361e-03\n",
      "step 926, train_loss 2.8988e-03, test_loss 4.4361e-03\n",
      "step 927, train_loss 2.8988e-03, test_loss 4.4364e-03\n",
      "step 928, train_loss 2.8989e-03, test_loss 4.4360e-03\n",
      "step 929, train_loss 2.8989e-03, test_loss 4.4361e-03\n",
      "step 930, train_loss 2.8987e-03, test_loss 4.4358e-03\n",
      "step 931, train_loss 2.8986e-03, test_loss 4.4358e-03\n",
      "step 932, train_loss 2.8986e-03, test_loss 4.4360e-03\n",
      "step 933, train_loss 2.8986e-03, test_loss 4.4356e-03\n",
      "step 934, train_loss 2.8986e-03, test_loss 4.4357e-03\n",
      "step 935, train_loss 2.8984e-03, test_loss 4.4355e-03\n",
      "step 936, train_loss 2.8983e-03, test_loss 4.4354e-03\n",
      "step 937, train_loss 2.8983e-03, test_loss 4.4356e-03\n",
      "step 938, train_loss 2.8983e-03, test_loss 4.4353e-03\n",
      "step 939, train_loss 2.8983e-03, test_loss 4.4354e-03\n",
      "step 940, train_loss 2.8982e-03, test_loss 4.4352e-03\n",
      "step 941, train_loss 2.8981e-03, test_loss 4.4351e-03\n",
      "step 942, train_loss 2.8981e-03, test_loss 4.4352e-03\n",
      "step 943, train_loss 2.8981e-03, test_loss 4.4350e-03\n",
      "step 944, train_loss 2.8980e-03, test_loss 4.4350e-03\n",
      "step 945, train_loss 2.8979e-03, test_loss 4.4349e-03\n",
      "step 946, train_loss 2.8979e-03, test_loss 4.4348e-03\n",
      "step 947, train_loss 2.8978e-03, test_loss 4.4348e-03\n",
      "step 948, train_loss 2.8978e-03, test_loss 4.4347e-03\n",
      "step 949, train_loss 2.8978e-03, test_loss 4.4347e-03\n",
      "step 950, train_loss 2.8977e-03, test_loss 4.4346e-03\n",
      "step 951, train_loss 2.8976e-03, test_loss 4.4345e-03\n",
      "step 952, train_loss 2.8976e-03, test_loss 4.4345e-03\n",
      "step 953, train_loss 2.8975e-03, test_loss 4.4344e-03\n",
      "step 954, train_loss 2.8975e-03, test_loss 4.4344e-03\n",
      "step 955, train_loss 2.8975e-03, test_loss 4.4343e-03\n",
      "step 956, train_loss 2.8974e-03, test_loss 4.4343e-03\n",
      "step 957, train_loss 2.8973e-03, test_loss 4.4342e-03\n",
      "step 958, train_loss 2.8973e-03, test_loss 4.4341e-03\n",
      "step 959, train_loss 2.8972e-03, test_loss 4.4341e-03\n",
      "step 960, train_loss 2.8972e-03, test_loss 4.4340e-03\n",
      "step 961, train_loss 2.8972e-03, test_loss 4.4340e-03\n",
      "step 962, train_loss 2.8971e-03, test_loss 4.4339e-03\n",
      "step 963, train_loss 2.8970e-03, test_loss 4.4338e-03\n",
      "step 964, train_loss 2.8970e-03, test_loss 4.4338e-03\n",
      "step 965, train_loss 2.8970e-03, test_loss 4.4337e-03\n",
      "step 966, train_loss 2.8969e-03, test_loss 4.4337e-03\n",
      "step 967, train_loss 2.8969e-03, test_loss 4.4336e-03\n",
      "step 968, train_loss 2.8968e-03, test_loss 4.4336e-03\n",
      "step 969, train_loss 2.8968e-03, test_loss 4.4335e-03\n",
      "step 970, train_loss 2.8967e-03, test_loss 4.4335e-03\n",
      "step 971, train_loss 2.8967e-03, test_loss 4.4334e-03\n",
      "step 972, train_loss 2.8966e-03, test_loss 4.4334e-03\n",
      "step 973, train_loss 2.8966e-03, test_loss 4.4333e-03\n",
      "step 974, train_loss 2.8965e-03, test_loss 4.4333e-03\n",
      "step 975, train_loss 2.8965e-03, test_loss 4.4332e-03\n",
      "step 976, train_loss 2.8964e-03, test_loss 4.4332e-03\n",
      "step 977, train_loss 2.8964e-03, test_loss 4.4331e-03\n",
      "step 978, train_loss 2.8963e-03, test_loss 4.4331e-03\n",
      "step 979, train_loss 2.8963e-03, test_loss 4.4330e-03\n",
      "step 980, train_loss 2.8962e-03, test_loss 4.4330e-03\n",
      "step 981, train_loss 2.8962e-03, test_loss 4.4329e-03\n",
      "step 982, train_loss 2.8961e-03, test_loss 4.4328e-03\n",
      "step 983, train_loss 2.8961e-03, test_loss 4.4328e-03\n",
      "step 984, train_loss 2.8960e-03, test_loss 4.4327e-03\n",
      "step 985, train_loss 2.8960e-03, test_loss 4.4327e-03\n",
      "step 986, train_loss 2.8959e-03, test_loss 4.4326e-03\n",
      "step 987, train_loss 2.8959e-03, test_loss 4.4326e-03\n",
      "step 988, train_loss 2.8958e-03, test_loss 4.4325e-03\n",
      "step 989, train_loss 2.8958e-03, test_loss 4.4325e-03\n",
      "step 990, train_loss 2.8957e-03, test_loss 4.4324e-03\n",
      "step 991, train_loss 2.8957e-03, test_loss 4.4324e-03\n",
      "step 992, train_loss 2.8956e-03, test_loss 4.4323e-03\n",
      "step 993, train_loss 2.8956e-03, test_loss 4.4323e-03\n",
      "step 994, train_loss 2.8955e-03, test_loss 4.4322e-03\n",
      "step 995, train_loss 2.8955e-03, test_loss 4.4322e-03\n",
      "step 996, train_loss 2.8954e-03, test_loss 4.4321e-03\n",
      "step 997, train_loss 2.8954e-03, test_loss 4.4321e-03\n",
      "step 998, train_loss 2.8953e-03, test_loss 4.4320e-03\n",
      "step 999, train_loss 2.8953e-03, test_loss 4.4320e-03\n",
      "step 1000, train_loss 2.8952e-03, test_loss 4.4319e-03\n",
      "step 1001, train_loss 2.8952e-03, test_loss 4.4319e-03\n",
      "step 1002, train_loss 2.8951e-03, test_loss 4.4318e-03\n",
      "step 1003, train_loss 2.8951e-03, test_loss 4.4318e-03\n",
      "step 1004, train_loss 2.8950e-03, test_loss 4.4317e-03\n",
      "step 1005, train_loss 2.8950e-03, test_loss 4.4317e-03\n",
      "step 1006, train_loss 2.8949e-03, test_loss 4.4316e-03\n",
      "step 1007, train_loss 2.8949e-03, test_loss 4.4316e-03\n",
      "step 1008, train_loss 2.8948e-03, test_loss 4.4315e-03\n",
      "step 1009, train_loss 2.8948e-03, test_loss 4.4315e-03\n",
      "step 1010, train_loss 2.8947e-03, test_loss 4.4314e-03\n",
      "step 1011, train_loss 2.8947e-03, test_loss 4.4314e-03\n",
      "step 1012, train_loss 2.8946e-03, test_loss 4.4313e-03\n",
      "step 1013, train_loss 2.8946e-03, test_loss 4.4313e-03\n",
      "step 1014, train_loss 2.8945e-03, test_loss 4.4312e-03\n",
      "step 1015, train_loss 2.8945e-03, test_loss 4.4312e-03\n",
      "step 1016, train_loss 2.8944e-03, test_loss 4.4311e-03\n",
      "step 1017, train_loss 2.8944e-03, test_loss 4.4311e-03\n",
      "step 1018, train_loss 2.8943e-03, test_loss 4.4310e-03\n",
      "step 1019, train_loss 2.8943e-03, test_loss 4.4310e-03\n",
      "step 1020, train_loss 2.8942e-03, test_loss 4.4309e-03\n",
      "step 1021, train_loss 2.8942e-03, test_loss 4.4309e-03\n",
      "step 1022, train_loss 2.8941e-03, test_loss 4.4308e-03\n",
      "step 1023, train_loss 2.8941e-03, test_loss 4.4308e-03\n",
      "step 1024, train_loss 2.8940e-03, test_loss 4.4307e-03\n",
      "step 1025, train_loss 2.8940e-03, test_loss 4.4307e-03\n",
      "step 1026, train_loss 2.8940e-03, test_loss 4.4306e-03\n",
      "step 1027, train_loss 2.8939e-03, test_loss 4.4306e-03\n",
      "step 1028, train_loss 2.8939e-03, test_loss 4.4305e-03\n",
      "step 1029, train_loss 2.8938e-03, test_loss 4.4305e-03\n",
      "step 1030, train_loss 2.8938e-03, test_loss 4.4304e-03\n",
      "step 1031, train_loss 2.8937e-03, test_loss 4.4304e-03\n",
      "step 1032, train_loss 2.8937e-03, test_loss 4.4303e-03\n",
      "step 1033, train_loss 2.8936e-03, test_loss 4.4303e-03\n",
      "step 1034, train_loss 2.8936e-03, test_loss 4.4302e-03\n",
      "step 1035, train_loss 2.8935e-03, test_loss 4.4302e-03\n",
      "step 1036, train_loss 2.8935e-03, test_loss 4.4301e-03\n",
      "step 1037, train_loss 2.8934e-03, test_loss 4.4301e-03\n",
      "step 1038, train_loss 2.8934e-03, test_loss 4.4300e-03\n",
      "step 1039, train_loss 2.8933e-03, test_loss 4.4300e-03\n",
      "step 1040, train_loss 2.8933e-03, test_loss 4.4299e-03\n",
      "step 1041, train_loss 2.8932e-03, test_loss 4.4299e-03\n",
      "step 1042, train_loss 2.8932e-03, test_loss 4.4298e-03\n",
      "step 1043, train_loss 2.8931e-03, test_loss 4.4298e-03\n",
      "step 1044, train_loss 2.8931e-03, test_loss 4.4297e-03\n",
      "step 1045, train_loss 2.8930e-03, test_loss 4.4297e-03\n",
      "step 1046, train_loss 2.8930e-03, test_loss 4.4296e-03\n",
      "step 1047, train_loss 2.8929e-03, test_loss 4.4296e-03\n",
      "step 1048, train_loss 2.8929e-03, test_loss 4.4295e-03\n",
      "step 1049, train_loss 2.8928e-03, test_loss 4.4295e-03\n",
      "step 1050, train_loss 2.8928e-03, test_loss 4.4294e-03\n",
      "step 1051, train_loss 2.8928e-03, test_loss 4.4294e-03\n",
      "step 1052, train_loss 2.8927e-03, test_loss 4.4293e-03\n",
      "step 1053, train_loss 2.8927e-03, test_loss 4.4293e-03\n",
      "step 1054, train_loss 2.8926e-03, test_loss 4.4292e-03\n",
      "step 1055, train_loss 2.8926e-03, test_loss 4.4292e-03\n",
      "step 1056, train_loss 2.8925e-03, test_loss 4.4291e-03\n",
      "step 1057, train_loss 2.8925e-03, test_loss 4.4291e-03\n",
      "step 1058, train_loss 2.8924e-03, test_loss 4.4290e-03\n",
      "step 1059, train_loss 2.8924e-03, test_loss 4.4290e-03\n",
      "step 1060, train_loss 2.8923e-03, test_loss 4.4289e-03\n",
      "step 1061, train_loss 2.8923e-03, test_loss 4.4289e-03\n",
      "step 1062, train_loss 2.8922e-03, test_loss 4.4288e-03\n",
      "step 1063, train_loss 2.8922e-03, test_loss 4.4288e-03\n",
      "step 1064, train_loss 2.8921e-03, test_loss 4.4287e-03\n",
      "step 1065, train_loss 2.8921e-03, test_loss 4.4287e-03\n",
      "step 1066, train_loss 2.8920e-03, test_loss 4.4286e-03\n",
      "step 1067, train_loss 2.8920e-03, test_loss 4.4286e-03\n",
      "step 1068, train_loss 2.8920e-03, test_loss 4.4285e-03\n",
      "step 1069, train_loss 2.8919e-03, test_loss 4.4286e-03\n",
      "step 1070, train_loss 2.8919e-03, test_loss 4.4284e-03\n",
      "step 1071, train_loss 2.8918e-03, test_loss 4.4285e-03\n",
      "step 1072, train_loss 2.8918e-03, test_loss 4.4283e-03\n",
      "step 1073, train_loss 2.8918e-03, test_loss 4.4285e-03\n",
      "step 1074, train_loss 2.8917e-03, test_loss 4.4282e-03\n",
      "step 1075, train_loss 2.8918e-03, test_loss 4.4287e-03\n",
      "step 1076, train_loss 2.8918e-03, test_loss 4.4283e-03\n",
      "step 1077, train_loss 2.8919e-03, test_loss 4.4292e-03\n",
      "step 1078, train_loss 2.8921e-03, test_loss 4.4288e-03\n",
      "step 1079, train_loss 2.8926e-03, test_loss 4.4309e-03\n",
      "step 1080, train_loss 2.8935e-03, test_loss 4.4309e-03\n",
      "step 1081, train_loss 2.8953e-03, test_loss 4.4364e-03\n",
      "step 1082, train_loss 2.8985e-03, test_loss 4.4394e-03\n",
      "step 1083, train_loss 2.9046e-03, test_loss 4.4550e-03\n",
      "step 1084, train_loss 2.9161e-03, test_loss 4.4710e-03\n",
      "step 1085, train_loss 2.9380e-03, test_loss 4.5199e-03\n",
      "step 1086, train_loss 2.9791e-03, test_loss 4.5854e-03\n",
      "step 1087, train_loss 3.0558e-03, test_loss 4.7372e-03\n",
      "step 1088, train_loss 3.1936e-03, test_loss 4.9483e-03\n",
      "step 1089, train_loss 3.4253e-03, test_loss 5.3187e-03\n",
      "step 1090, train_loss 3.7725e-03, test_loss 5.7063e-03\n",
      "step 1091, train_loss 4.1923e-03, test_loss 6.1038e-03\n",
      "step 1092, train_loss 4.5593e-03, test_loss 6.2257e-03\n",
      "step 1093, train_loss 4.7134e-03, test_loss 6.1726e-03\n",
      "step 1094, train_loss 4.6370e-03, test_loss 5.9294e-03\n",
      "step 1095, train_loss 4.4042e-03, test_loss 5.5389e-03\n",
      "step 1096, train_loss 4.0132e-03, test_loss 4.9947e-03\n",
      "step 1097, train_loss 3.4545e-03, test_loss 4.5584e-03\n",
      "step 1098, train_loss 3.0285e-03, test_loss 4.6968e-03\n",
      "step 1099, train_loss 3.1583e-03, test_loss 5.1894e-03\n",
      "step 1100, train_loss 3.6540e-03, test_loss 5.3350e-03\n",
      "step 1101, train_loss 3.8048e-03, test_loss 4.9050e-03\n",
      "step 1102, train_loss 3.3701e-03, test_loss 4.4960e-03\n",
      "step 1103, train_loss 2.9583e-03, test_loss 4.5631e-03\n",
      "step 1104, train_loss 3.0329e-03, test_loss 4.8011e-03\n",
      "step 1105, train_loss 3.2639e-03, test_loss 4.8023e-03\n",
      "step 1106, train_loss 3.2676e-03, test_loss 4.6918e-03\n",
      "step 1107, train_loss 3.1615e-03, test_loss 4.6478e-03\n",
      "step 1108, train_loss 3.1046e-03, test_loss 4.5533e-03\n",
      "step 1109, train_loss 3.0227e-03, test_loss 4.4952e-03\n",
      "step 1110, train_loss 2.9583e-03, test_loss 4.5867e-03\n",
      "step 1111, train_loss 3.0466e-03, test_loss 4.6751e-03\n",
      "step 1112, train_loss 3.1479e-03, test_loss 4.5961e-03\n",
      "step 1113, train_loss 3.0547e-03, test_loss 4.4390e-03\n",
      "step 1114, train_loss 2.9046e-03, test_loss 4.4569e-03\n",
      "step 1115, train_loss 2.9234e-03, test_loss 4.5621e-03\n",
      "step 1116, train_loss 3.0199e-03, test_loss 4.5486e-03\n",
      "step 1117, train_loss 3.0168e-03, test_loss 4.4976e-03\n",
      "step 1118, train_loss 2.9588e-03, test_loss 4.4780e-03\n",
      "step 1119, train_loss 2.9397e-03, test_loss 4.4648e-03\n",
      "step 1120, train_loss 2.9314e-03, test_loss 4.4596e-03\n",
      "step 1121, train_loss 2.9205e-03, test_loss 4.4802e-03\n",
      "step 1122, train_loss 2.9445e-03, test_loss 4.5027e-03\n",
      "step 1123, train_loss 2.9669e-03, test_loss 4.4698e-03\n",
      "step 1124, train_loss 2.9317e-03, test_loss 4.4263e-03\n",
      "step 1125, train_loss 2.8889e-03, test_loss 4.4398e-03\n",
      "step 1126, train_loss 2.9030e-03, test_loss 4.4728e-03\n",
      "step 1127, train_loss 2.9343e-03, test_loss 4.4658e-03\n",
      "step 1128, train_loss 2.9284e-03, test_loss 4.4434e-03\n",
      "step 1129, train_loss 2.9072e-03, test_loss 4.4417e-03\n",
      "step 1130, train_loss 2.9023e-03, test_loss 4.4376e-03\n",
      "step 1131, train_loss 2.9017e-03, test_loss 4.4366e-03\n",
      "step 1132, train_loss 2.8989e-03, test_loss 4.4452e-03\n",
      "step 1133, train_loss 2.9059e-03, test_loss 4.4482e-03\n",
      "step 1134, train_loss 2.9127e-03, test_loss 4.4419e-03\n",
      "step 1135, train_loss 2.9020e-03, test_loss 4.4255e-03\n",
      "step 1136, train_loss 2.8880e-03, test_loss 4.4280e-03\n",
      "step 1137, train_loss 2.8912e-03, test_loss 4.4410e-03\n",
      "step 1138, train_loss 2.9014e-03, test_loss 4.4371e-03\n",
      "step 1139, train_loss 2.9011e-03, test_loss 4.4332e-03\n",
      "step 1140, train_loss 2.8948e-03, test_loss 4.4308e-03\n",
      "step 1141, train_loss 2.8926e-03, test_loss 4.4289e-03\n",
      "step 1142, train_loss 2.8919e-03, test_loss 4.4292e-03\n",
      "step 1143, train_loss 2.8902e-03, test_loss 4.4296e-03\n",
      "step 1144, train_loss 2.8919e-03, test_loss 4.4333e-03\n",
      "step 1145, train_loss 2.8954e-03, test_loss 4.4322e-03\n",
      "step 1146, train_loss 2.8939e-03, test_loss 4.4266e-03\n",
      "step 1147, train_loss 2.8888e-03, test_loss 4.4250e-03\n",
      "step 1148, train_loss 2.8874e-03, test_loss 4.4284e-03\n",
      "step 1149, train_loss 2.8901e-03, test_loss 4.4292e-03\n",
      "step 1150, train_loss 2.8914e-03, test_loss 4.4282e-03\n",
      "step 1151, train_loss 2.8904e-03, test_loss 4.4283e-03\n",
      "step 1152, train_loss 2.8897e-03, test_loss 4.4265e-03\n",
      "step 1153, train_loss 2.8893e-03, test_loss 4.4265e-03\n",
      "step 1154, train_loss 2.8881e-03, test_loss 4.4254e-03\n",
      "step 1155, train_loss 2.8875e-03, test_loss 4.4260e-03\n",
      "step 1156, train_loss 2.8887e-03, test_loss 4.4284e-03\n",
      "step 1157, train_loss 2.8898e-03, test_loss 4.4262e-03\n",
      "step 1158, train_loss 2.8890e-03, test_loss 4.4257e-03\n",
      "step 1159, train_loss 2.8877e-03, test_loss 4.4254e-03\n",
      "step 1160, train_loss 2.8874e-03, test_loss 4.4251e-03\n",
      "step 1161, train_loss 2.8878e-03, test_loss 4.4261e-03\n",
      "step 1162, train_loss 2.8878e-03, test_loss 4.4253e-03\n",
      "step 1163, train_loss 2.8878e-03, test_loss 4.4257e-03\n",
      "step 1164, train_loss 2.8880e-03, test_loss 4.4257e-03\n",
      "step 1165, train_loss 2.8879e-03, test_loss 4.4248e-03\n",
      "step 1166, train_loss 2.8873e-03, test_loss 4.4245e-03\n",
      "step 1167, train_loss 2.8869e-03, test_loss 4.4248e-03\n",
      "step 1168, train_loss 2.8871e-03, test_loss 4.4250e-03\n",
      "step 1169, train_loss 2.8875e-03, test_loss 4.4250e-03\n",
      "step 1170, train_loss 2.8875e-03, test_loss 4.4249e-03\n",
      "step 1171, train_loss 2.8872e-03, test_loss 4.4243e-03\n",
      "step 1172, train_loss 2.8871e-03, test_loss 4.4247e-03\n",
      "step 1173, train_loss 2.8870e-03, test_loss 4.4241e-03\n",
      "step 1174, train_loss 2.8869e-03, test_loss 4.4241e-03\n",
      "step 1175, train_loss 2.8868e-03, test_loss 4.4245e-03\n",
      "step 1176, train_loss 2.8869e-03, test_loss 4.4241e-03\n",
      "step 1177, train_loss 2.8870e-03, test_loss 4.4245e-03\n",
      "step 1178, train_loss 2.8869e-03, test_loss 4.4239e-03\n",
      "step 1179, train_loss 2.8868e-03, test_loss 4.4239e-03\n",
      "step 1180, train_loss 2.8867e-03, test_loss 4.4240e-03\n",
      "step 1181, train_loss 2.8866e-03, test_loss 4.4236e-03\n",
      "step 1182, train_loss 2.8866e-03, test_loss 4.4238e-03\n",
      "step 1183, train_loss 2.8865e-03, test_loss 4.4237e-03\n",
      "step 1184, train_loss 2.8866e-03, test_loss 4.4237e-03\n",
      "step 1185, train_loss 2.8866e-03, test_loss 4.4237e-03\n",
      "step 1186, train_loss 2.8865e-03, test_loss 4.4236e-03\n",
      "step 1187, train_loss 2.8864e-03, test_loss 4.4234e-03\n",
      "step 1188, train_loss 2.8864e-03, test_loss 4.4235e-03\n",
      "step 1189, train_loss 2.8863e-03, test_loss 4.4233e-03\n",
      "step 1190, train_loss 2.8863e-03, test_loss 4.4233e-03\n",
      "step 1191, train_loss 2.8863e-03, test_loss 4.4233e-03\n",
      "step 1192, train_loss 2.8862e-03, test_loss 4.4231e-03\n",
      "step 1193, train_loss 2.8862e-03, test_loss 4.4233e-03\n",
      "step 1194, train_loss 2.8862e-03, test_loss 4.4230e-03\n",
      "step 1195, train_loss 2.8861e-03, test_loss 4.4231e-03\n",
      "step 1196, train_loss 2.8861e-03, test_loss 4.4230e-03\n",
      "step 1197, train_loss 2.8860e-03, test_loss 4.4229e-03\n",
      "step 1198, train_loss 2.8860e-03, test_loss 4.4229e-03\n",
      "step 1199, train_loss 2.8860e-03, test_loss 4.4228e-03\n",
      "step 1200, train_loss 2.8859e-03, test_loss 4.4228e-03\n",
      "step 1201, train_loss 2.8859e-03, test_loss 4.4227e-03\n",
      "step 1202, train_loss 2.8859e-03, test_loss 4.4227e-03\n",
      "step 1203, train_loss 2.8858e-03, test_loss 4.4226e-03\n",
      "step 1204, train_loss 2.8858e-03, test_loss 4.4226e-03\n",
      "step 1205, train_loss 2.8857e-03, test_loss 4.4225e-03\n",
      "step 1206, train_loss 2.8857e-03, test_loss 4.4225e-03\n",
      "step 1207, train_loss 2.8857e-03, test_loss 4.4224e-03\n",
      "step 1208, train_loss 2.8856e-03, test_loss 4.4224e-03\n",
      "step 1209, train_loss 2.8856e-03, test_loss 4.4224e-03\n",
      "step 1210, train_loss 2.8856e-03, test_loss 4.4223e-03\n",
      "step 1211, train_loss 2.8855e-03, test_loss 4.4223e-03\n",
      "step 1212, train_loss 2.8855e-03, test_loss 4.4222e-03\n",
      "step 1213, train_loss 2.8854e-03, test_loss 4.4222e-03\n",
      "step 1214, train_loss 2.8854e-03, test_loss 4.4221e-03\n",
      "step 1215, train_loss 2.8854e-03, test_loss 4.4221e-03\n",
      "step 1216, train_loss 2.8853e-03, test_loss 4.4220e-03\n",
      "step 1217, train_loss 2.8853e-03, test_loss 4.4220e-03\n",
      "step 1218, train_loss 2.8852e-03, test_loss 4.4219e-03\n",
      "step 1219, train_loss 2.8852e-03, test_loss 4.4219e-03\n",
      "step 1220, train_loss 2.8852e-03, test_loss 4.4218e-03\n",
      "step 1221, train_loss 2.8851e-03, test_loss 4.4218e-03\n",
      "step 1222, train_loss 2.8851e-03, test_loss 4.4218e-03\n",
      "step 1223, train_loss 2.8851e-03, test_loss 4.4217e-03\n",
      "step 1224, train_loss 2.8850e-03, test_loss 4.4217e-03\n",
      "step 1225, train_loss 2.8850e-03, test_loss 4.4216e-03\n",
      "step 1226, train_loss 2.8849e-03, test_loss 4.4216e-03\n",
      "step 1227, train_loss 2.8849e-03, test_loss 4.4215e-03\n",
      "step 1228, train_loss 2.8849e-03, test_loss 4.4215e-03\n",
      "step 1229, train_loss 2.8848e-03, test_loss 4.4215e-03\n",
      "step 1230, train_loss 2.8848e-03, test_loss 4.4214e-03\n",
      "step 1231, train_loss 2.8847e-03, test_loss 4.4214e-03\n",
      "step 1232, train_loss 2.8847e-03, test_loss 4.4213e-03\n",
      "step 1233, train_loss 2.8847e-03, test_loss 4.4213e-03\n",
      "step 1234, train_loss 2.8846e-03, test_loss 4.4213e-03\n",
      "step 1235, train_loss 2.8846e-03, test_loss 4.4212e-03\n",
      "step 1236, train_loss 2.8845e-03, test_loss 4.4212e-03\n",
      "step 1237, train_loss 2.8845e-03, test_loss 4.4211e-03\n",
      "step 1238, train_loss 2.8845e-03, test_loss 4.4211e-03\n",
      "step 1239, train_loss 2.8844e-03, test_loss 4.4210e-03\n",
      "step 1240, train_loss 2.8844e-03, test_loss 4.4210e-03\n",
      "step 1241, train_loss 2.8844e-03, test_loss 4.4210e-03\n",
      "step 1242, train_loss 2.8843e-03, test_loss 4.4209e-03\n",
      "step 1243, train_loss 2.8843e-03, test_loss 4.4209e-03\n",
      "step 1244, train_loss 2.8842e-03, test_loss 4.4208e-03\n",
      "step 1245, train_loss 2.8842e-03, test_loss 4.4208e-03\n",
      "step 1246, train_loss 2.8842e-03, test_loss 4.4208e-03\n",
      "step 1247, train_loss 2.8841e-03, test_loss 4.4207e-03\n",
      "step 1248, train_loss 2.8841e-03, test_loss 4.4207e-03\n",
      "step 1249, train_loss 2.8840e-03, test_loss 4.4206e-03\n",
      "step 1250, train_loss 2.8840e-03, test_loss 4.4206e-03\n",
      "step 1251, train_loss 2.8840e-03, test_loss 4.4205e-03\n",
      "step 1252, train_loss 2.8839e-03, test_loss 4.4205e-03\n",
      "step 1253, train_loss 2.8839e-03, test_loss 4.4205e-03\n",
      "step 1254, train_loss 2.8838e-03, test_loss 4.4204e-03\n",
      "step 1255, train_loss 2.8838e-03, test_loss 4.4204e-03\n",
      "step 1256, train_loss 2.8838e-03, test_loss 4.4203e-03\n",
      "step 1257, train_loss 2.8837e-03, test_loss 4.4203e-03\n",
      "step 1258, train_loss 2.8837e-03, test_loss 4.4202e-03\n",
      "step 1259, train_loss 2.8836e-03, test_loss 4.4202e-03\n",
      "step 1260, train_loss 2.8836e-03, test_loss 4.4202e-03\n",
      "step 1261, train_loss 2.8836e-03, test_loss 4.4201e-03\n",
      "step 1262, train_loss 2.8835e-03, test_loss 4.4201e-03\n",
      "step 1263, train_loss 2.8835e-03, test_loss 4.4201e-03\n",
      "step 1264, train_loss 2.8834e-03, test_loss 4.4200e-03\n",
      "step 1265, train_loss 2.8834e-03, test_loss 4.4200e-03\n",
      "step 1266, train_loss 2.8834e-03, test_loss 4.4199e-03\n",
      "step 1267, train_loss 2.8833e-03, test_loss 4.4199e-03\n",
      "step 1268, train_loss 2.8833e-03, test_loss 4.4198e-03\n",
      "step 1269, train_loss 2.8833e-03, test_loss 4.4198e-03\n",
      "step 1270, train_loss 2.8832e-03, test_loss 4.4197e-03\n",
      "step 1271, train_loss 2.8832e-03, test_loss 4.4198e-03\n",
      "step 1272, train_loss 2.8832e-03, test_loss 4.4197e-03\n",
      "step 1273, train_loss 2.8831e-03, test_loss 4.4198e-03\n",
      "step 1274, train_loss 2.8831e-03, test_loss 4.4196e-03\n",
      "step 1275, train_loss 2.8832e-03, test_loss 4.4200e-03\n",
      "step 1276, train_loss 2.8832e-03, test_loss 4.4198e-03\n",
      "step 1277, train_loss 2.8834e-03, test_loss 4.4206e-03\n",
      "step 1278, train_loss 2.8838e-03, test_loss 4.4207e-03\n",
      "step 1279, train_loss 2.8845e-03, test_loss 4.4229e-03\n",
      "step 1280, train_loss 2.8858e-03, test_loss 4.4242e-03\n",
      "step 1281, train_loss 2.8884e-03, test_loss 4.4307e-03\n",
      "step 1282, train_loss 2.8932e-03, test_loss 4.4378e-03\n",
      "step 1283, train_loss 2.9027e-03, test_loss 4.4589e-03\n",
      "step 1284, train_loss 2.9207e-03, test_loss 4.4893e-03\n",
      "step 1285, train_loss 2.9557e-03, test_loss 4.5627e-03\n",
      "step 1286, train_loss 3.0233e-03, test_loss 4.6841e-03\n",
      "step 1287, train_loss 3.1539e-03, test_loss 4.9426e-03\n",
      "step 1288, train_loss 3.4018e-03, test_loss 5.3781e-03\n",
      "step 1289, train_loss 3.8561e-03, test_loss 6.1636e-03\n",
      "step 1290, train_loss 4.6235e-03, test_loss 7.2222e-03\n",
      "step 1291, train_loss 5.7166e-03, test_loss 8.3018e-03\n",
      "step 1292, train_loss 6.7673e-03, test_loss 8.3389e-03\n",
      "step 1293, train_loss 6.8431e-03, test_loss 6.8707e-03\n",
      "step 1294, train_loss 5.3356e-03, test_loss 4.9283e-03\n",
      "step 1295, train_loss 3.4036e-03, test_loss 4.4990e-03\n",
      "step 1296, train_loss 2.9689e-03, test_loss 5.6083e-03\n",
      "step 1297, train_loss 4.0672e-03, test_loss 6.4135e-03\n",
      "step 1298, train_loss 4.8991e-03, test_loss 5.7855e-03\n",
      "step 1299, train_loss 4.2459e-03, test_loss 4.6218e-03\n",
      "step 1300, train_loss 3.0900e-03, test_loss 4.5748e-03\n",
      "step 1301, train_loss 3.0467e-03, test_loss 5.3721e-03\n",
      "step 1302, train_loss 3.8340e-03, test_loss 5.4881e-03\n",
      "step 1303, train_loss 3.9692e-03, test_loss 4.7859e-03\n",
      "step 1304, train_loss 3.2500e-03, test_loss 4.4360e-03\n",
      "step 1305, train_loss 2.8990e-03, test_loss 4.8587e-03\n",
      "step 1306, train_loss 3.3321e-03, test_loss 5.1467e-03\n",
      "step 1307, train_loss 3.6070e-03, test_loss 4.7549e-03\n",
      "step 1308, train_loss 3.2249e-03, test_loss 4.4245e-03\n",
      "step 1309, train_loss 2.8911e-03, test_loss 4.6443e-03\n",
      "step 1310, train_loss 3.1062e-03, test_loss 4.8672e-03\n",
      "step 1311, train_loss 3.3418e-03, test_loss 4.6751e-03\n",
      "step 1312, train_loss 3.1393e-03, test_loss 4.4289e-03\n",
      "step 1313, train_loss 2.8939e-03, test_loss 4.5357e-03\n",
      "step 1314, train_loss 3.0052e-03, test_loss 4.7141e-03\n",
      "step 1315, train_loss 3.1750e-03, test_loss 4.5961e-03\n",
      "step 1316, train_loss 3.0638e-03, test_loss 4.4287e-03\n",
      "step 1317, train_loss 2.8936e-03, test_loss 4.4887e-03\n",
      "step 1318, train_loss 2.9505e-03, test_loss 4.5978e-03\n",
      "step 1319, train_loss 3.0676e-03, test_loss 4.5430e-03\n",
      "step 1320, train_loss 3.0065e-03, test_loss 4.4272e-03\n",
      "step 1321, train_loss 2.8917e-03, test_loss 4.4530e-03\n",
      "step 1322, train_loss 2.9203e-03, test_loss 4.5380e-03\n",
      "step 1323, train_loss 2.9994e-03, test_loss 4.5011e-03\n",
      "step 1324, train_loss 2.9669e-03, test_loss 4.4259e-03\n",
      "step 1325, train_loss 2.8897e-03, test_loss 4.4398e-03\n",
      "step 1326, train_loss 2.9014e-03, test_loss 4.4883e-03\n",
      "step 1327, train_loss 2.9549e-03, test_loss 4.4772e-03\n",
      "step 1328, train_loss 2.9399e-03, test_loss 4.4246e-03\n",
      "step 1329, train_loss 2.8884e-03, test_loss 4.4250e-03\n",
      "step 1330, train_loss 2.8903e-03, test_loss 4.4644e-03\n",
      "step 1331, train_loss 2.9259e-03, test_loss 4.4570e-03\n",
      "step 1332, train_loss 2.9214e-03, test_loss 4.4245e-03\n",
      "step 1333, train_loss 2.8873e-03, test_loss 4.4221e-03\n",
      "step 1334, train_loss 2.8836e-03, test_loss 4.4424e-03\n",
      "step 1335, train_loss 2.9069e-03, test_loss 4.4465e-03\n",
      "step 1336, train_loss 2.9085e-03, test_loss 4.4236e-03\n",
      "step 1337, train_loss 2.8867e-03, test_loss 4.4164e-03\n",
      "step 1338, train_loss 2.8803e-03, test_loss 4.4330e-03\n",
      "step 1339, train_loss 2.8946e-03, test_loss 4.4356e-03\n",
      "step 1340, train_loss 2.8993e-03, test_loss 4.4239e-03\n",
      "step 1341, train_loss 2.8862e-03, test_loss 4.4171e-03\n",
      "step 1342, train_loss 2.8788e-03, test_loss 4.4235e-03\n",
      "step 1343, train_loss 2.8868e-03, test_loss 4.4308e-03\n",
      "step 1344, train_loss 2.8924e-03, test_loss 4.4227e-03\n",
      "step 1345, train_loss 2.8855e-03, test_loss 4.4158e-03\n",
      "step 1346, train_loss 2.8787e-03, test_loss 4.4204e-03\n",
      "step 1347, train_loss 2.8822e-03, test_loss 4.4242e-03\n",
      "step 1348, train_loss 2.8873e-03, test_loss 4.4226e-03\n",
      "step 1349, train_loss 2.8846e-03, test_loss 4.4171e-03\n",
      "step 1350, train_loss 2.8792e-03, test_loss 4.4169e-03\n",
      "step 1351, train_loss 2.8797e-03, test_loss 4.4220e-03\n",
      "step 1352, train_loss 2.8836e-03, test_loss 4.4208e-03\n",
      "step 1353, train_loss 2.8834e-03, test_loss 4.4174e-03\n",
      "step 1354, train_loss 2.8798e-03, test_loss 4.4166e-03\n",
      "step 1355, train_loss 2.8786e-03, test_loss 4.4181e-03\n",
      "step 1356, train_loss 2.8810e-03, test_loss 4.4202e-03\n",
      "step 1357, train_loss 2.8821e-03, test_loss 4.4178e-03\n",
      "step 1358, train_loss 2.8802e-03, test_loss 4.4161e-03\n",
      "step 1359, train_loss 2.8785e-03, test_loss 4.4176e-03\n",
      "step 1360, train_loss 2.8794e-03, test_loss 4.4181e-03\n",
      "step 1361, train_loss 2.8808e-03, test_loss 4.4180e-03\n",
      "step 1362, train_loss 2.8802e-03, test_loss 4.4164e-03\n",
      "step 1363, train_loss 2.8788e-03, test_loss 4.4160e-03\n",
      "step 1364, train_loss 2.8787e-03, test_loss 4.4175e-03\n",
      "step 1365, train_loss 2.8797e-03, test_loss 4.4173e-03\n",
      "step 1366, train_loss 2.8799e-03, test_loss 4.4167e-03\n",
      "step 1367, train_loss 2.8790e-03, test_loss 4.4162e-03\n",
      "step 1368, train_loss 2.8785e-03, test_loss 4.4162e-03\n",
      "step 1369, train_loss 2.8789e-03, test_loss 4.4171e-03\n",
      "step 1370, train_loss 2.8794e-03, test_loss 4.4165e-03\n",
      "step 1371, train_loss 2.8792e-03, test_loss 4.4159e-03\n",
      "step 1372, train_loss 2.8786e-03, test_loss 4.4160e-03\n",
      "step 1373, train_loss 2.8785e-03, test_loss 4.4160e-03\n",
      "step 1374, train_loss 2.8789e-03, test_loss 4.4165e-03\n",
      "step 1375, train_loss 2.8790e-03, test_loss 4.4160e-03\n",
      "step 1376, train_loss 2.8787e-03, test_loss 4.4157e-03\n",
      "step 1377, train_loss 2.8784e-03, test_loss 4.4159e-03\n",
      "step 1378, train_loss 2.8785e-03, test_loss 4.4158e-03\n",
      "step 1379, train_loss 2.8787e-03, test_loss 4.4160e-03\n",
      "step 1380, train_loss 2.8787e-03, test_loss 4.4156e-03\n",
      "step 1381, train_loss 2.8785e-03, test_loss 4.4154e-03\n",
      "step 1382, train_loss 2.8783e-03, test_loss 4.4157e-03\n",
      "step 1383, train_loss 2.8785e-03, test_loss 4.4156e-03\n",
      "step 1384, train_loss 2.8786e-03, test_loss 4.4156e-03\n",
      "step 1385, train_loss 2.8785e-03, test_loss 4.4154e-03\n",
      "step 1386, train_loss 2.8783e-03, test_loss 4.4152e-03\n",
      "step 1387, train_loss 2.8783e-03, test_loss 4.4154e-03\n",
      "step 1388, train_loss 2.8783e-03, test_loss 4.4153e-03\n",
      "step 1389, train_loss 2.8784e-03, test_loss 4.4153e-03\n",
      "step 1390, train_loss 2.8783e-03, test_loss 4.4151e-03\n",
      "step 1391, train_loss 2.8782e-03, test_loss 4.4150e-03\n",
      "step 1392, train_loss 2.8782e-03, test_loss 4.4152e-03\n",
      "step 1393, train_loss 2.8782e-03, test_loss 4.4150e-03\n",
      "step 1394, train_loss 2.8782e-03, test_loss 4.4150e-03\n",
      "step 1395, train_loss 2.8782e-03, test_loss 4.4149e-03\n",
      "step 1396, train_loss 2.8781e-03, test_loss 4.4148e-03\n",
      "step 1397, train_loss 2.8781e-03, test_loss 4.4149e-03\n",
      "step 1398, train_loss 2.8781e-03, test_loss 4.4148e-03\n",
      "step 1399, train_loss 2.8781e-03, test_loss 4.4148e-03\n",
      "step 1400, train_loss 2.8780e-03, test_loss 4.4147e-03\n",
      "step 1401, train_loss 2.8780e-03, test_loss 4.4146e-03\n",
      "step 1402, train_loss 2.8779e-03, test_loss 4.4147e-03\n",
      "step 1403, train_loss 2.8779e-03, test_loss 4.4146e-03\n",
      "step 1404, train_loss 2.8779e-03, test_loss 4.4146e-03\n",
      "step 1405, train_loss 2.8779e-03, test_loss 4.4145e-03\n",
      "step 1406, train_loss 2.8778e-03, test_loss 4.4144e-03\n",
      "step 1407, train_loss 2.8778e-03, test_loss 4.4144e-03\n",
      "step 1408, train_loss 2.8778e-03, test_loss 4.4144e-03\n",
      "step 1409, train_loss 2.8778e-03, test_loss 4.4144e-03\n",
      "step 1410, train_loss 2.8777e-03, test_loss 4.4143e-03\n",
      "step 1411, train_loss 2.8777e-03, test_loss 4.4143e-03\n",
      "step 1412, train_loss 2.8777e-03, test_loss 4.4142e-03\n",
      "step 1413, train_loss 2.8777e-03, test_loss 4.4142e-03\n",
      "step 1414, train_loss 2.8776e-03, test_loss 4.4142e-03\n",
      "step 1415, train_loss 2.8776e-03, test_loss 4.4141e-03\n",
      "step 1416, train_loss 2.8776e-03, test_loss 4.4141e-03\n",
      "step 1417, train_loss 2.8775e-03, test_loss 4.4140e-03\n",
      "step 1418, train_loss 2.8775e-03, test_loss 4.4140e-03\n",
      "step 1419, train_loss 2.8775e-03, test_loss 4.4140e-03\n",
      "step 1420, train_loss 2.8775e-03, test_loss 4.4139e-03\n",
      "step 1421, train_loss 2.8774e-03, test_loss 4.4139e-03\n",
      "step 1422, train_loss 2.8774e-03, test_loss 4.4138e-03\n",
      "step 1423, train_loss 2.8774e-03, test_loss 4.4138e-03\n",
      "step 1424, train_loss 2.8773e-03, test_loss 4.4138e-03\n",
      "step 1425, train_loss 2.8773e-03, test_loss 4.4137e-03\n",
      "step 1426, train_loss 2.8773e-03, test_loss 4.4137e-03\n",
      "step 1427, train_loss 2.8772e-03, test_loss 4.4137e-03\n",
      "step 1428, train_loss 2.8772e-03, test_loss 4.4136e-03\n",
      "step 1429, train_loss 2.8772e-03, test_loss 4.4136e-03\n",
      "step 1430, train_loss 2.8772e-03, test_loss 4.4135e-03\n",
      "step 1431, train_loss 2.8771e-03, test_loss 4.4135e-03\n",
      "step 1432, train_loss 2.8771e-03, test_loss 4.4135e-03\n",
      "step 1433, train_loss 2.8771e-03, test_loss 4.4134e-03\n",
      "step 1434, train_loss 2.8770e-03, test_loss 4.4134e-03\n",
      "step 1435, train_loss 2.8770e-03, test_loss 4.4134e-03\n",
      "step 1436, train_loss 2.8770e-03, test_loss 4.4133e-03\n",
      "step 1437, train_loss 2.8769e-03, test_loss 4.4133e-03\n",
      "step 1438, train_loss 2.8769e-03, test_loss 4.4133e-03\n",
      "step 1439, train_loss 2.8769e-03, test_loss 4.4132e-03\n",
      "step 1440, train_loss 2.8768e-03, test_loss 4.4132e-03\n",
      "step 1441, train_loss 2.8768e-03, test_loss 4.4132e-03\n",
      "step 1442, train_loss 2.8768e-03, test_loss 4.4131e-03\n",
      "step 1443, train_loss 2.8768e-03, test_loss 4.4131e-03\n",
      "step 1444, train_loss 2.8767e-03, test_loss 4.4130e-03\n",
      "step 1445, train_loss 2.8767e-03, test_loss 4.4130e-03\n",
      "step 1446, train_loss 2.8767e-03, test_loss 4.4130e-03\n",
      "step 1447, train_loss 2.8766e-03, test_loss 4.4129e-03\n",
      "step 1448, train_loss 2.8766e-03, test_loss 4.4129e-03\n",
      "step 1449, train_loss 2.8766e-03, test_loss 4.4129e-03\n",
      "step 1450, train_loss 2.8765e-03, test_loss 4.4128e-03\n",
      "step 1451, train_loss 2.8765e-03, test_loss 4.4128e-03\n",
      "step 1452, train_loss 2.8765e-03, test_loss 4.4128e-03\n",
      "step 1453, train_loss 2.8764e-03, test_loss 4.4127e-03\n",
      "step 1454, train_loss 2.8764e-03, test_loss 4.4127e-03\n",
      "step 1455, train_loss 2.8764e-03, test_loss 4.4127e-03\n",
      "step 1456, train_loss 2.8763e-03, test_loss 4.4126e-03\n",
      "step 1457, train_loss 2.8763e-03, test_loss 4.4126e-03\n",
      "step 1458, train_loss 2.8763e-03, test_loss 4.4126e-03\n",
      "step 1459, train_loss 2.8762e-03, test_loss 4.4125e-03\n",
      "step 1460, train_loss 2.8762e-03, test_loss 4.4125e-03\n",
      "step 1461, train_loss 2.8762e-03, test_loss 4.4125e-03\n",
      "step 1462, train_loss 2.8762e-03, test_loss 4.4124e-03\n",
      "step 1463, train_loss 2.8761e-03, test_loss 4.4124e-03\n",
      "step 1464, train_loss 2.8761e-03, test_loss 4.4124e-03\n",
      "step 1465, train_loss 2.8761e-03, test_loss 4.4123e-03\n",
      "step 1466, train_loss 2.8760e-03, test_loss 4.4123e-03\n",
      "step 1467, train_loss 2.8760e-03, test_loss 4.4123e-03\n",
      "step 1468, train_loss 2.8760e-03, test_loss 4.4122e-03\n",
      "step 1469, train_loss 2.8759e-03, test_loss 4.4122e-03\n",
      "step 1470, train_loss 2.8759e-03, test_loss 4.4121e-03\n",
      "step 1471, train_loss 2.8759e-03, test_loss 4.4121e-03\n",
      "step 1472, train_loss 2.8758e-03, test_loss 4.4121e-03\n",
      "step 1473, train_loss 2.8758e-03, test_loss 4.4120e-03\n",
      "step 1474, train_loss 2.8758e-03, test_loss 4.4120e-03\n",
      "step 1475, train_loss 2.8757e-03, test_loss 4.4120e-03\n",
      "step 1476, train_loss 2.8757e-03, test_loss 4.4119e-03\n",
      "step 1477, train_loss 2.8757e-03, test_loss 4.4119e-03\n",
      "step 1478, train_loss 2.8756e-03, test_loss 4.4119e-03\n",
      "step 1479, train_loss 2.8756e-03, test_loss 4.4118e-03\n",
      "step 1480, train_loss 2.8756e-03, test_loss 4.4118e-03\n",
      "step 1481, train_loss 2.8755e-03, test_loss 4.4118e-03\n",
      "step 1482, train_loss 2.8755e-03, test_loss 4.4117e-03\n",
      "step 1483, train_loss 2.8755e-03, test_loss 4.4117e-03\n",
      "step 1484, train_loss 2.8755e-03, test_loss 4.4117e-03\n",
      "step 1485, train_loss 2.8754e-03, test_loss 4.4116e-03\n",
      "step 1486, train_loss 2.8754e-03, test_loss 4.4116e-03\n",
      "step 1487, train_loss 2.8754e-03, test_loss 4.4116e-03\n",
      "step 1488, train_loss 2.8753e-03, test_loss 4.4115e-03\n",
      "step 1489, train_loss 2.8753e-03, test_loss 4.4115e-03\n",
      "step 1490, train_loss 2.8753e-03, test_loss 4.4115e-03\n",
      "step 1491, train_loss 2.8752e-03, test_loss 4.4114e-03\n",
      "step 1492, train_loss 2.8752e-03, test_loss 4.4114e-03\n",
      "step 1493, train_loss 2.8752e-03, test_loss 4.4114e-03\n",
      "step 1494, train_loss 2.8751e-03, test_loss 4.4113e-03\n",
      "step 1495, train_loss 2.8751e-03, test_loss 4.4113e-03\n",
      "step 1496, train_loss 2.8751e-03, test_loss 4.4113e-03\n",
      "step 1497, train_loss 2.8750e-03, test_loss 4.4112e-03\n",
      "step 1498, train_loss 2.8750e-03, test_loss 4.4112e-03\n",
      "step 1499, train_loss 2.8750e-03, test_loss 4.4112e-03\n",
      "step 1500, train_loss 2.8749e-03, test_loss 4.4111e-03\n",
      "step 1501, train_loss 2.8749e-03, test_loss 4.4111e-03\n",
      "step 1502, train_loss 2.8749e-03, test_loss 4.4111e-03\n",
      "step 1503, train_loss 2.8748e-03, test_loss 4.4110e-03\n",
      "step 1504, train_loss 2.8748e-03, test_loss 4.4110e-03\n",
      "step 1505, train_loss 2.8748e-03, test_loss 4.4110e-03\n",
      "step 1506, train_loss 2.8748e-03, test_loss 4.4109e-03\n",
      "step 1507, train_loss 2.8747e-03, test_loss 4.4109e-03\n",
      "step 1508, train_loss 2.8747e-03, test_loss 4.4109e-03\n",
      "step 1509, train_loss 2.8747e-03, test_loss 4.4108e-03\n",
      "step 1510, train_loss 2.8746e-03, test_loss 4.4108e-03\n",
      "step 1511, train_loss 2.8746e-03, test_loss 4.4108e-03\n",
      "step 1512, train_loss 2.8746e-03, test_loss 4.4107e-03\n",
      "step 1513, train_loss 2.8745e-03, test_loss 4.4107e-03\n",
      "step 1514, train_loss 2.8745e-03, test_loss 4.4107e-03\n",
      "step 1515, train_loss 2.8745e-03, test_loss 4.4106e-03\n",
      "step 1516, train_loss 2.8744e-03, test_loss 4.4106e-03\n",
      "step 1517, train_loss 2.8744e-03, test_loss 4.4106e-03\n",
      "step 1518, train_loss 2.8744e-03, test_loss 4.4105e-03\n",
      "step 1519, train_loss 2.8743e-03, test_loss 4.4105e-03\n",
      "step 1520, train_loss 2.8743e-03, test_loss 4.4105e-03\n",
      "step 1521, train_loss 2.8743e-03, test_loss 4.4104e-03\n",
      "step 1522, train_loss 2.8743e-03, test_loss 4.4104e-03\n",
      "step 1523, train_loss 2.8742e-03, test_loss 4.4104e-03\n",
      "step 1524, train_loss 2.8742e-03, test_loss 4.4103e-03\n",
      "step 1525, train_loss 2.8742e-03, test_loss 4.4103e-03\n",
      "step 1526, train_loss 2.8741e-03, test_loss 4.4103e-03\n",
      "step 1527, train_loss 2.8741e-03, test_loss 4.4102e-03\n",
      "step 1528, train_loss 2.8741e-03, test_loss 4.4102e-03\n",
      "step 1529, train_loss 2.8740e-03, test_loss 4.4102e-03\n",
      "step 1530, train_loss 2.8740e-03, test_loss 4.4101e-03\n",
      "step 1531, train_loss 2.8740e-03, test_loss 4.4101e-03\n",
      "step 1532, train_loss 2.8739e-03, test_loss 4.4101e-03\n",
      "step 1533, train_loss 2.8739e-03, test_loss 4.4101e-03\n",
      "step 1534, train_loss 2.8739e-03, test_loss 4.4100e-03\n",
      "step 1535, train_loss 2.8739e-03, test_loss 4.4100e-03\n",
      "step 1536, train_loss 2.8738e-03, test_loss 4.4100e-03\n",
      "step 1537, train_loss 2.8738e-03, test_loss 4.4099e-03\n",
      "step 1538, train_loss 2.8738e-03, test_loss 4.4099e-03\n",
      "step 1539, train_loss 2.8737e-03, test_loss 4.4099e-03\n",
      "step 1540, train_loss 2.8737e-03, test_loss 4.4098e-03\n",
      "step 1541, train_loss 2.8737e-03, test_loss 4.4098e-03\n",
      "step 1542, train_loss 2.8736e-03, test_loss 4.4098e-03\n",
      "step 1543, train_loss 2.8736e-03, test_loss 4.4097e-03\n",
      "step 1544, train_loss 2.8736e-03, test_loss 4.4097e-03\n",
      "step 1545, train_loss 2.8735e-03, test_loss 4.4097e-03\n",
      "step 1546, train_loss 2.8735e-03, test_loss 4.4096e-03\n",
      "step 1547, train_loss 2.8735e-03, test_loss 4.4096e-03\n",
      "step 1548, train_loss 2.8735e-03, test_loss 4.4096e-03\n",
      "step 1549, train_loss 2.8734e-03, test_loss 4.4096e-03\n",
      "step 1550, train_loss 2.8734e-03, test_loss 4.4095e-03\n",
      "step 1551, train_loss 2.8734e-03, test_loss 4.4095e-03\n",
      "step 1552, train_loss 2.8733e-03, test_loss 4.4094e-03\n",
      "step 1553, train_loss 2.8733e-03, test_loss 4.4095e-03\n",
      "step 1554, train_loss 2.8733e-03, test_loss 4.4093e-03\n",
      "step 1555, train_loss 2.8733e-03, test_loss 4.4095e-03\n",
      "step 1556, train_loss 2.8732e-03, test_loss 4.4092e-03\n",
      "step 1557, train_loss 2.8732e-03, test_loss 4.4095e-03\n",
      "step 1558, train_loss 2.8732e-03, test_loss 4.4092e-03\n",
      "step 1559, train_loss 2.8732e-03, test_loss 4.4097e-03\n",
      "step 1560, train_loss 2.8733e-03, test_loss 4.4092e-03\n",
      "step 1561, train_loss 2.8735e-03, test_loss 4.4104e-03\n",
      "step 1562, train_loss 2.8738e-03, test_loss 4.4099e-03\n",
      "step 1563, train_loss 2.8744e-03, test_loss 4.4127e-03\n",
      "step 1564, train_loss 2.8756e-03, test_loss 4.4128e-03\n",
      "step 1565, train_loss 2.8780e-03, test_loss 4.4206e-03\n",
      "step 1566, train_loss 2.8827e-03, test_loss 4.4252e-03\n",
      "step 1567, train_loss 2.8917e-03, test_loss 4.4489e-03\n",
      "step 1568, train_loss 2.9094e-03, test_loss 4.4749e-03\n",
      "step 1569, train_loss 2.9440e-03, test_loss 4.5536e-03\n",
      "step 1570, train_loss 3.0112e-03, test_loss 4.6646e-03\n",
      "step 1571, train_loss 3.1390e-03, test_loss 4.9186e-03\n",
      "step 1572, train_loss 3.3714e-03, test_loss 5.2684e-03\n",
      "step 1573, train_loss 3.7529e-03, test_loss 5.8273e-03\n",
      "step 1574, train_loss 4.2749e-03, test_loss 6.2522e-03\n",
      "step 1575, train_loss 4.7482e-03, test_loss 6.4277e-03\n",
      "step 1576, train_loss 4.8754e-03, test_loss 6.1050e-03\n",
      "step 1577, train_loss 4.5962e-03, test_loss 5.9185e-03\n",
      "step 1578, train_loss 4.3796e-03, test_loss 6.0781e-03\n",
      "step 1579, train_loss 4.5514e-03, test_loss 6.1769e-03\n",
      "step 1580, train_loss 4.6555e-03, test_loss 5.5610e-03\n",
      "step 1581, train_loss 4.0226e-03, test_loss 4.6124e-03\n",
      "step 1582, train_loss 3.0800e-03, test_loss 4.4854e-03\n",
      "step 1583, train_loss 2.9514e-03, test_loss 5.0807e-03\n",
      "step 1584, train_loss 3.5387e-03, test_loss 5.2982e-03\n",
      "step 1585, train_loss 3.7691e-03, test_loss 4.9638e-03\n",
      "step 1586, train_loss 3.4310e-03, test_loss 4.7967e-03\n",
      "step 1587, train_loss 3.2531e-03, test_loss 4.8531e-03\n",
      "step 1588, train_loss 3.3316e-03, test_loss 4.7097e-03\n",
      "step 1589, train_loss 3.1663e-03, test_loss 4.4754e-03\n",
      "step 1590, train_loss 2.9398e-03, test_loss 4.6447e-03\n",
      "step 1591, train_loss 3.1172e-03, test_loss 4.8994e-03\n",
      "step 1592, train_loss 3.3516e-03, test_loss 4.7052e-03\n",
      "step 1593, train_loss 3.1789e-03, test_loss 4.4755e-03\n",
      "step 1594, train_loss 2.9366e-03, test_loss 4.5254e-03\n",
      "step 1595, train_loss 2.9849e-03, test_loss 4.5850e-03\n",
      "step 1596, train_loss 3.0573e-03, test_loss 4.5306e-03\n",
      "step 1597, train_loss 2.9892e-03, test_loss 4.5357e-03\n",
      "step 1598, train_loss 3.0009e-03, test_loss 4.6046e-03\n",
      "step 1599, train_loss 3.0704e-03, test_loss 4.5254e-03\n",
      "step 1600, train_loss 2.9854e-03, test_loss 4.4085e-03\n",
      "step 1601, train_loss 2.8711e-03, test_loss 4.4580e-03\n",
      "step 1602, train_loss 2.9225e-03, test_loss 4.5359e-03\n",
      "step 1603, train_loss 2.9966e-03, test_loss 4.4925e-03\n",
      "step 1604, train_loss 2.9559e-03, test_loss 4.4522e-03\n",
      "step 1605, train_loss 2.9185e-03, test_loss 4.4771e-03\n",
      "step 1606, train_loss 2.9356e-03, test_loss 4.4473e-03\n",
      "step 1607, train_loss 2.9136e-03, test_loss 4.4151e-03\n",
      "step 1608, train_loss 2.8775e-03, test_loss 4.4490e-03\n",
      "step 1609, train_loss 2.9079e-03, test_loss 4.4742e-03\n",
      "step 1610, train_loss 2.9416e-03, test_loss 4.4515e-03\n",
      "step 1611, train_loss 2.9105e-03, test_loss 4.4161e-03\n",
      "step 1612, train_loss 2.8792e-03, test_loss 4.4258e-03\n",
      "step 1613, train_loss 2.8905e-03, test_loss 4.4352e-03\n",
      "step 1614, train_loss 2.8949e-03, test_loss 4.4187e-03\n",
      "step 1615, train_loss 2.8829e-03, test_loss 4.4293e-03\n",
      "step 1616, train_loss 2.8913e-03, test_loss 4.4415e-03\n",
      "step 1617, train_loss 2.9027e-03, test_loss 4.4234e-03\n",
      "step 1618, train_loss 2.8861e-03, test_loss 4.4071e-03\n",
      "step 1619, train_loss 2.8695e-03, test_loss 4.4166e-03\n",
      "step 1620, train_loss 2.8777e-03, test_loss 4.4224e-03\n",
      "step 1621, train_loss 2.8859e-03, test_loss 4.4178e-03\n",
      "step 1622, train_loss 2.8803e-03, test_loss 4.4180e-03\n",
      "step 1623, train_loss 2.8789e-03, test_loss 4.4184e-03\n",
      "step 1624, train_loss 2.8829e-03, test_loss 4.4165e-03\n",
      "step 1625, train_loss 2.8771e-03, test_loss 4.4067e-03\n",
      "step 1626, train_loss 2.8692e-03, test_loss 4.4095e-03\n",
      "step 1627, train_loss 2.8728e-03, test_loss 4.4184e-03\n",
      "step 1628, train_loss 2.8789e-03, test_loss 4.4128e-03\n",
      "step 1629, train_loss 2.8767e-03, test_loss 4.4115e-03\n",
      "step 1630, train_loss 2.8734e-03, test_loss 4.4125e-03\n",
      "step 1631, train_loss 2.8745e-03, test_loss 4.4105e-03\n",
      "step 1632, train_loss 2.8737e-03, test_loss 4.4083e-03\n",
      "step 1633, train_loss 2.8701e-03, test_loss 4.4084e-03\n",
      "step 1634, train_loss 2.8707e-03, test_loss 4.4114e-03\n",
      "step 1635, train_loss 2.8741e-03, test_loss 4.4118e-03\n",
      "step 1636, train_loss 2.8741e-03, test_loss 4.4094e-03\n",
      "step 1637, train_loss 2.8717e-03, test_loss 4.4080e-03\n",
      "step 1638, train_loss 2.8714e-03, test_loss 4.4101e-03\n",
      "step 1639, train_loss 2.8718e-03, test_loss 4.4075e-03\n",
      "step 1640, train_loss 2.8706e-03, test_loss 4.4072e-03\n",
      "step 1641, train_loss 2.8700e-03, test_loss 4.4095e-03\n",
      "step 1642, train_loss 2.8714e-03, test_loss 4.4087e-03\n",
      "step 1643, train_loss 2.8722e-03, test_loss 4.4092e-03\n",
      "step 1644, train_loss 2.8713e-03, test_loss 4.4076e-03\n",
      "step 1645, train_loss 2.8705e-03, test_loss 4.4077e-03\n",
      "step 1646, train_loss 2.8708e-03, test_loss 4.4082e-03\n",
      "step 1647, train_loss 2.8706e-03, test_loss 4.4069e-03\n",
      "step 1648, train_loss 2.8700e-03, test_loss 4.4074e-03\n",
      "step 1649, train_loss 2.8702e-03, test_loss 4.4082e-03\n",
      "step 1650, train_loss 2.8709e-03, test_loss 4.4079e-03\n",
      "step 1651, train_loss 2.8709e-03, test_loss 4.4073e-03\n",
      "step 1652, train_loss 2.8704e-03, test_loss 4.4076e-03\n",
      "step 1653, train_loss 2.8703e-03, test_loss 4.4069e-03\n",
      "step 1654, train_loss 2.8704e-03, test_loss 4.4074e-03\n",
      "step 1655, train_loss 2.8702e-03, test_loss 4.4068e-03\n",
      "step 1656, train_loss 2.8699e-03, test_loss 4.4067e-03\n",
      "step 1657, train_loss 2.8701e-03, test_loss 4.4077e-03\n",
      "step 1658, train_loss 2.8704e-03, test_loss 4.4068e-03\n",
      "step 1659, train_loss 2.8703e-03, test_loss 4.4071e-03\n",
      "step 1660, train_loss 2.8701e-03, test_loss 4.4069e-03\n",
      "step 1661, train_loss 2.8702e-03, test_loss 4.4068e-03\n",
      "step 1662, train_loss 2.8702e-03, test_loss 4.4068e-03\n",
      "step 1663, train_loss 2.8700e-03, test_loss 4.4066e-03\n",
      "step 1664, train_loss 2.8699e-03, test_loss 4.4065e-03\n",
      "step 1665, train_loss 2.8700e-03, test_loss 4.4068e-03\n",
      "step 1666, train_loss 2.8701e-03, test_loss 4.4066e-03\n",
      "step 1667, train_loss 2.8700e-03, test_loss 4.4064e-03\n",
      "step 1668, train_loss 2.8700e-03, test_loss 4.4068e-03\n",
      "step 1669, train_loss 2.8700e-03, test_loss 4.4063e-03\n",
      "step 1670, train_loss 2.8700e-03, test_loss 4.4066e-03\n",
      "step 1671, train_loss 2.8699e-03, test_loss 4.4063e-03\n",
      "step 1672, train_loss 2.8698e-03, test_loss 4.4062e-03\n",
      "step 1673, train_loss 2.8698e-03, test_loss 4.4064e-03\n",
      "step 1674, train_loss 2.8698e-03, test_loss 4.4061e-03\n",
      "step 1675, train_loss 2.8698e-03, test_loss 4.4062e-03\n",
      "step 1676, train_loss 2.8698e-03, test_loss 4.4062e-03\n",
      "step 1677, train_loss 2.8698e-03, test_loss 4.4061e-03\n",
      "step 1678, train_loss 2.8698e-03, test_loss 4.4061e-03\n",
      "step 1679, train_loss 2.8698e-03, test_loss 4.4061e-03\n",
      "step 1680, train_loss 2.8697e-03, test_loss 4.4059e-03\n",
      "step 1681, train_loss 2.8697e-03, test_loss 4.4061e-03\n",
      "step 1682, train_loss 2.8697e-03, test_loss 4.4059e-03\n",
      "step 1683, train_loss 2.8697e-03, test_loss 4.4059e-03\n",
      "step 1684, train_loss 2.8696e-03, test_loss 4.4059e-03\n",
      "step 1685, train_loss 2.8696e-03, test_loss 4.4058e-03\n",
      "step 1686, train_loss 2.8696e-03, test_loss 4.4059e-03\n",
      "step 1687, train_loss 2.8696e-03, test_loss 4.4057e-03\n",
      "step 1688, train_loss 2.8695e-03, test_loss 4.4058e-03\n",
      "step 1689, train_loss 2.8695e-03, test_loss 4.4057e-03\n",
      "step 1690, train_loss 2.8695e-03, test_loss 4.4057e-03\n",
      "step 1691, train_loss 2.8695e-03, test_loss 4.4056e-03\n",
      "step 1692, train_loss 2.8695e-03, test_loss 4.4057e-03\n",
      "step 1693, train_loss 2.8694e-03, test_loss 4.4055e-03\n",
      "step 1694, train_loss 2.8694e-03, test_loss 4.4056e-03\n",
      "step 1695, train_loss 2.8694e-03, test_loss 4.4055e-03\n",
      "step 1696, train_loss 2.8694e-03, test_loss 4.4055e-03\n",
      "step 1697, train_loss 2.8693e-03, test_loss 4.4055e-03\n",
      "step 1698, train_loss 2.8693e-03, test_loss 4.4054e-03\n",
      "step 1699, train_loss 2.8693e-03, test_loss 4.4054e-03\n",
      "step 1700, train_loss 2.8693e-03, test_loss 4.4054e-03\n",
      "step 1701, train_loss 2.8692e-03, test_loss 4.4053e-03\n",
      "step 1702, train_loss 2.8692e-03, test_loss 4.4053e-03\n",
      "step 1703, train_loss 2.8692e-03, test_loss 4.4053e-03\n",
      "step 1704, train_loss 2.8692e-03, test_loss 4.4052e-03\n",
      "step 1705, train_loss 2.8692e-03, test_loss 4.4052e-03\n",
      "step 1706, train_loss 2.8691e-03, test_loss 4.4052e-03\n",
      "step 1707, train_loss 2.8691e-03, test_loss 4.4052e-03\n",
      "step 1708, train_loss 2.8691e-03, test_loss 4.4051e-03\n",
      "step 1709, train_loss 2.8691e-03, test_loss 4.4051e-03\n",
      "step 1710, train_loss 2.8690e-03, test_loss 4.4051e-03\n",
      "step 1711, train_loss 2.8690e-03, test_loss 4.4051e-03\n",
      "step 1712, train_loss 2.8690e-03, test_loss 4.4050e-03\n",
      "step 1713, train_loss 2.8690e-03, test_loss 4.4050e-03\n",
      "step 1714, train_loss 2.8689e-03, test_loss 4.4049e-03\n",
      "step 1715, train_loss 2.8689e-03, test_loss 4.4050e-03\n",
      "step 1716, train_loss 2.8689e-03, test_loss 4.4049e-03\n",
      "step 1717, train_loss 2.8689e-03, test_loss 4.4049e-03\n",
      "step 1718, train_loss 2.8688e-03, test_loss 4.4048e-03\n",
      "step 1719, train_loss 2.8688e-03, test_loss 4.4049e-03\n",
      "step 1720, train_loss 2.8688e-03, test_loss 4.4048e-03\n",
      "step 1721, train_loss 2.8688e-03, test_loss 4.4049e-03\n",
      "step 1722, train_loss 2.8688e-03, test_loss 4.4047e-03\n",
      "step 1723, train_loss 2.8688e-03, test_loss 4.4049e-03\n",
      "step 1724, train_loss 2.8688e-03, test_loss 4.4047e-03\n",
      "step 1725, train_loss 2.8688e-03, test_loss 4.4050e-03\n",
      "step 1726, train_loss 2.8688e-03, test_loss 4.4047e-03\n",
      "step 1727, train_loss 2.8688e-03, test_loss 4.4051e-03\n",
      "step 1728, train_loss 2.8689e-03, test_loss 4.4049e-03\n",
      "step 1729, train_loss 2.8691e-03, test_loss 4.4056e-03\n",
      "step 1730, train_loss 2.8693e-03, test_loss 4.4053e-03\n",
      "step 1731, train_loss 2.8696e-03, test_loss 4.4066e-03\n",
      "step 1732, train_loss 2.8702e-03, test_loss 4.4066e-03\n",
      "step 1733, train_loss 2.8711e-03, test_loss 4.4092e-03\n",
      "step 1734, train_loss 2.8725e-03, test_loss 4.4100e-03\n",
      "step 1735, train_loss 2.8749e-03, test_loss 4.4157e-03\n",
      "step 1736, train_loss 2.8786e-03, test_loss 4.4194e-03\n",
      "step 1737, train_loss 2.8848e-03, test_loss 4.4325e-03\n",
      "step 1738, train_loss 2.8949e-03, test_loss 4.4452e-03\n",
      "step 1739, train_loss 2.9116e-03, test_loss 4.4779e-03\n",
      "step 1740, train_loss 2.9394e-03, test_loss 4.5176e-03\n",
      "step 1741, train_loss 2.9858e-03, test_loss 4.6027e-03\n",
      "step 1742, train_loss 3.0629e-03, test_loss 4.7189e-03\n",
      "step 1743, train_loss 3.1905e-03, test_loss 4.9383e-03\n",
      "step 1744, train_loss 3.3971e-03, test_loss 5.2416e-03\n",
      "step 1745, train_loss 3.7196e-03, test_loss 5.7292e-03\n",
      "step 1746, train_loss 4.1873e-03, test_loss 6.2915e-03\n",
      "step 1747, train_loss 4.7798e-03, test_loss 6.8869e-03\n",
      "step 1748, train_loss 5.3462e-03, test_loss 7.0689e-03\n",
      "step 1749, train_loss 5.5645e-03, test_loss 6.6502e-03\n",
      "step 1750, train_loss 5.1102e-03, test_loss 5.5958e-03\n",
      "step 1751, train_loss 4.0795e-03, test_loss 4.6643e-03\n",
      "step 1752, train_loss 3.1256e-03, test_loss 4.4228e-03\n",
      "step 1753, train_loss 2.8857e-03, test_loss 4.8727e-03\n",
      "step 1754, train_loss 3.3457e-03, test_loss 5.4608e-03\n",
      "step 1755, train_loss 3.9177e-03, test_loss 5.5244e-03\n",
      "step 1756, train_loss 4.0051e-03, test_loss 5.0686e-03\n",
      "step 1757, train_loss 3.5283e-03, test_loss 4.5172e-03\n",
      "step 1758, train_loss 2.9883e-03, test_loss 4.4275e-03\n",
      "step 1759, train_loss 2.8963e-03, test_loss 4.7550e-03\n",
      "step 1760, train_loss 3.2157e-03, test_loss 5.0102e-03\n",
      "step 1761, train_loss 3.4859e-03, test_loss 4.9235e-03\n",
      "step 1762, train_loss 3.3817e-03, test_loss 4.5774e-03\n",
      "step 1763, train_loss 3.0467e-03, test_loss 4.4000e-03\n",
      "step 1764, train_loss 2.8644e-03, test_loss 4.5201e-03\n",
      "step 1765, train_loss 2.9823e-03, test_loss 4.7085e-03\n",
      "step 1766, train_loss 3.1821e-03, test_loss 4.7323e-03\n",
      "step 1767, train_loss 3.1934e-03, test_loss 4.5448e-03\n",
      "step 1768, train_loss 3.0154e-03, test_loss 4.4078e-03\n",
      "step 1769, train_loss 2.8716e-03, test_loss 4.4446e-03\n",
      "step 1770, train_loss 2.9068e-03, test_loss 4.5607e-03\n",
      "step 1771, train_loss 3.0299e-03, test_loss 4.6056e-03\n",
      "step 1772, train_loss 3.0657e-03, test_loss 4.5066e-03\n",
      "step 1773, train_loss 2.9756e-03, test_loss 4.4132e-03\n",
      "step 1774, train_loss 2.8772e-03, test_loss 4.4132e-03\n",
      "step 1775, train_loss 2.8774e-03, test_loss 4.4791e-03\n",
      "step 1776, train_loss 2.9478e-03, test_loss 4.5227e-03\n",
      "step 1777, train_loss 2.9839e-03, test_loss 4.4749e-03\n",
      "step 1778, train_loss 2.9424e-03, test_loss 4.4170e-03\n",
      "step 1779, train_loss 2.8795e-03, test_loss 4.4029e-03\n",
      "step 1780, train_loss 2.8663e-03, test_loss 4.4366e-03\n",
      "step 1781, train_loss 2.9032e-03, test_loss 4.4713e-03\n",
      "step 1782, train_loss 2.9332e-03, test_loss 4.4511e-03\n",
      "step 1783, train_loss 2.9187e-03, test_loss 4.4175e-03\n",
      "step 1784, train_loss 2.8807e-03, test_loss 4.3994e-03\n",
      "step 1785, train_loss 2.8636e-03, test_loss 4.4144e-03\n",
      "step 1786, train_loss 2.8798e-03, test_loss 4.4401e-03\n",
      "step 1787, train_loss 2.9017e-03, test_loss 4.4354e-03\n",
      "step 1788, train_loss 2.9013e-03, test_loss 4.4184e-03\n",
      "step 1789, train_loss 2.8809e-03, test_loss 4.4005e-03\n",
      "step 1790, train_loss 2.8649e-03, test_loss 4.4035e-03\n",
      "step 1791, train_loss 2.8684e-03, test_loss 4.4194e-03\n",
      "step 1792, train_loss 2.8822e-03, test_loss 4.4220e-03\n",
      "step 1793, train_loss 2.8878e-03, test_loss 4.4171e-03\n",
      "step 1794, train_loss 2.8795e-03, test_loss 4.4035e-03\n",
      "step 1795, train_loss 2.8677e-03, test_loss 4.4009e-03\n",
      "step 1796, train_loss 2.8646e-03, test_loss 4.4083e-03\n",
      "step 1797, train_loss 2.8710e-03, test_loss 4.4124e-03\n",
      "step 1798, train_loss 2.8775e-03, test_loss 4.4139e-03\n",
      "step 1799, train_loss 2.8766e-03, test_loss 4.4052e-03\n",
      "step 1800, train_loss 2.8700e-03, test_loss 4.4014e-03\n",
      "step 1801, train_loss 2.8650e-03, test_loss 4.4026e-03\n",
      "step 1802, train_loss 2.8659e-03, test_loss 4.4059e-03\n",
      "step 1803, train_loss 2.8702e-03, test_loss 4.4101e-03\n",
      "step 1804, train_loss 2.8726e-03, test_loss 4.4062e-03\n",
      "step 1805, train_loss 2.8706e-03, test_loss 4.4037e-03\n",
      "step 1806, train_loss 2.8668e-03, test_loss 4.4012e-03\n",
      "step 1807, train_loss 2.8650e-03, test_loss 4.4021e-03\n",
      "step 1808, train_loss 2.8663e-03, test_loss 4.4055e-03\n",
      "step 1809, train_loss 2.8686e-03, test_loss 4.4050e-03\n",
      "step 1810, train_loss 2.8694e-03, test_loss 4.4050e-03\n",
      "step 1811, train_loss 2.8680e-03, test_loss 4.4021e-03\n",
      "step 1812, train_loss 2.8660e-03, test_loss 4.4017e-03\n",
      "step 1813, train_loss 2.8652e-03, test_loss 4.4028e-03\n",
      "step 1814, train_loss 2.8661e-03, test_loss 4.4031e-03\n",
      "step 1815, train_loss 2.8673e-03, test_loss 4.4045e-03\n",
      "step 1816, train_loss 2.8677e-03, test_loss 4.4027e-03\n",
      "step 1817, train_loss 2.8669e-03, test_loss 4.4024e-03\n",
      "step 1818, train_loss 2.8658e-03, test_loss 4.4017e-03\n",
      "step 1819, train_loss 2.8654e-03, test_loss 4.4019e-03\n",
      "step 1820, train_loss 2.8658e-03, test_loss 4.4032e-03\n",
      "step 1821, train_loss 2.8665e-03, test_loss 4.4027e-03\n",
      "step 1822, train_loss 2.8668e-03, test_loss 4.4030e-03\n",
      "step 1823, train_loss 2.8664e-03, test_loss 4.4018e-03\n",
      "step 1824, train_loss 2.8659e-03, test_loss 4.4017e-03\n",
      "step 1825, train_loss 2.8655e-03, test_loss 4.4019e-03\n",
      "step 1826, train_loss 2.8657e-03, test_loss 4.4019e-03\n",
      "step 1827, train_loss 2.8660e-03, test_loss 4.4027e-03\n",
      "step 1828, train_loss 2.8662e-03, test_loss 4.4021e-03\n",
      "step 1829, train_loss 2.8662e-03, test_loss 4.4023e-03\n",
      "step 1830, train_loss 2.8659e-03, test_loss 4.4017e-03\n",
      "step 1831, train_loss 2.8657e-03, test_loss 4.4016e-03\n",
      "step 1832, train_loss 2.8656e-03, test_loss 4.4019e-03\n",
      "step 1833, train_loss 2.8657e-03, test_loss 4.4017e-03\n",
      "step 1834, train_loss 2.8659e-03, test_loss 4.4022e-03\n",
      "step 1835, train_loss 2.8660e-03, test_loss 4.4017e-03\n",
      "step 1836, train_loss 2.8659e-03, test_loss 4.4019e-03\n",
      "step 1837, train_loss 2.8657e-03, test_loss 4.4016e-03\n",
      "step 1838, train_loss 2.8656e-03, test_loss 4.4016e-03\n",
      "step 1839, train_loss 2.8656e-03, test_loss 4.4017e-03\n",
      "step 1840, train_loss 2.8657e-03, test_loss 4.4015e-03\n",
      "step 1841, train_loss 2.8657e-03, test_loss 4.4019e-03\n",
      "step 1842, train_loss 2.8658e-03, test_loss 4.4015e-03\n",
      "step 1843, train_loss 2.8657e-03, test_loss 4.4017e-03\n",
      "step 1844, train_loss 2.8656e-03, test_loss 4.4015e-03\n",
      "step 1845, train_loss 2.8656e-03, test_loss 4.4015e-03\n",
      "step 1846, train_loss 2.8655e-03, test_loss 4.4015e-03\n",
      "step 1847, train_loss 2.8656e-03, test_loss 4.4014e-03\n",
      "step 1848, train_loss 2.8656e-03, test_loss 4.4016e-03\n",
      "step 1849, train_loss 2.8656e-03, test_loss 4.4013e-03\n",
      "step 1850, train_loss 2.8656e-03, test_loss 4.4015e-03\n",
      "step 1851, train_loss 2.8655e-03, test_loss 4.4013e-03\n",
      "step 1852, train_loss 2.8655e-03, test_loss 4.4013e-03\n",
      "step 1853, train_loss 2.8655e-03, test_loss 4.4013e-03\n",
      "step 1854, train_loss 2.8655e-03, test_loss 4.4012e-03\n",
      "step 1855, train_loss 2.8655e-03, test_loss 4.4013e-03\n",
      "step 1856, train_loss 2.8655e-03, test_loss 4.4012e-03\n",
      "step 1857, train_loss 2.8655e-03, test_loss 4.4013e-03\n",
      "step 1858, train_loss 2.8654e-03, test_loss 4.4011e-03\n",
      "step 1859, train_loss 2.8654e-03, test_loss 4.4012e-03\n",
      "step 1860, train_loss 2.8654e-03, test_loss 4.4011e-03\n",
      "step 1861, train_loss 2.8653e-03, test_loss 4.4011e-03\n",
      "step 1862, train_loss 2.8653e-03, test_loss 4.4011e-03\n",
      "step 1863, train_loss 2.8653e-03, test_loss 4.4010e-03\n",
      "step 1864, train_loss 2.8653e-03, test_loss 4.4011e-03\n",
      "step 1865, train_loss 2.8653e-03, test_loss 4.4010e-03\n",
      "step 1866, train_loss 2.8653e-03, test_loss 4.4010e-03\n",
      "step 1867, train_loss 2.8653e-03, test_loss 4.4009e-03\n",
      "step 1868, train_loss 2.8652e-03, test_loss 4.4010e-03\n",
      "step 1869, train_loss 2.8652e-03, test_loss 4.4009e-03\n",
      "step 1870, train_loss 2.8652e-03, test_loss 4.4009e-03\n",
      "step 1871, train_loss 2.8652e-03, test_loss 4.4009e-03\n",
      "step 1872, train_loss 2.8651e-03, test_loss 4.4008e-03\n",
      "step 1873, train_loss 2.8651e-03, test_loss 4.4009e-03\n",
      "step 1874, train_loss 2.8651e-03, test_loss 4.4008e-03\n",
      "step 1875, train_loss 2.8651e-03, test_loss 4.4008e-03\n",
      "step 1876, train_loss 2.8651e-03, test_loss 4.4007e-03\n",
      "step 1877, train_loss 2.8651e-03, test_loss 4.4008e-03\n",
      "step 1878, train_loss 2.8650e-03, test_loss 4.4007e-03\n",
      "step 1879, train_loss 2.8650e-03, test_loss 4.4007e-03\n",
      "step 1880, train_loss 2.8650e-03, test_loss 4.4007e-03\n",
      "step 1881, train_loss 2.8650e-03, test_loss 4.4006e-03\n",
      "step 1882, train_loss 2.8650e-03, test_loss 4.4006e-03\n",
      "step 1883, train_loss 2.8649e-03, test_loss 4.4006e-03\n",
      "step 1884, train_loss 2.8649e-03, test_loss 4.4006e-03\n",
      "step 1885, train_loss 2.8649e-03, test_loss 4.4005e-03\n",
      "step 1886, train_loss 2.8649e-03, test_loss 4.4006e-03\n",
      "step 1887, train_loss 2.8648e-03, test_loss 4.4005e-03\n",
      "step 1888, train_loss 2.8648e-03, test_loss 4.4005e-03\n",
      "step 1889, train_loss 2.8648e-03, test_loss 4.4005e-03\n",
      "step 1890, train_loss 2.8648e-03, test_loss 4.4004e-03\n",
      "step 1891, train_loss 2.8648e-03, test_loss 4.4004e-03\n",
      "step 1892, train_loss 2.8647e-03, test_loss 4.4004e-03\n",
      "step 1893, train_loss 2.8647e-03, test_loss 4.4004e-03\n",
      "step 1894, train_loss 2.8647e-03, test_loss 4.4003e-03\n",
      "step 1895, train_loss 2.8647e-03, test_loss 4.4003e-03\n",
      "step 1896, train_loss 2.8647e-03, test_loss 4.4003e-03\n",
      "step 1897, train_loss 2.8646e-03, test_loss 4.4003e-03\n",
      "step 1898, train_loss 2.8646e-03, test_loss 4.4002e-03\n",
      "step 1899, train_loss 2.8646e-03, test_loss 4.4003e-03\n",
      "step 1900, train_loss 2.8646e-03, test_loss 4.4002e-03\n",
      "step 1901, train_loss 2.8646e-03, test_loss 4.4002e-03\n",
      "step 1902, train_loss 2.8645e-03, test_loss 4.4002e-03\n",
      "step 1903, train_loss 2.8645e-03, test_loss 4.4002e-03\n",
      "step 1904, train_loss 2.8645e-03, test_loss 4.4001e-03\n",
      "step 1905, train_loss 2.8645e-03, test_loss 4.4001e-03\n",
      "step 1906, train_loss 2.8644e-03, test_loss 4.4001e-03\n",
      "step 1907, train_loss 2.8644e-03, test_loss 4.4001e-03\n",
      "step 1908, train_loss 2.8644e-03, test_loss 4.4000e-03\n",
      "step 1909, train_loss 2.8644e-03, test_loss 4.4000e-03\n",
      "step 1910, train_loss 2.8644e-03, test_loss 4.4000e-03\n",
      "step 1911, train_loss 2.8643e-03, test_loss 4.4000e-03\n",
      "step 1912, train_loss 2.8643e-03, test_loss 4.3999e-03\n",
      "step 1913, train_loss 2.8643e-03, test_loss 4.3999e-03\n",
      "step 1914, train_loss 2.8643e-03, test_loss 4.3999e-03\n",
      "step 1915, train_loss 2.8643e-03, test_loss 4.3999e-03\n",
      "step 1916, train_loss 2.8642e-03, test_loss 4.3999e-03\n",
      "step 1917, train_loss 2.8642e-03, test_loss 4.3998e-03\n",
      "step 1918, train_loss 2.8642e-03, test_loss 4.3998e-03\n",
      "step 1919, train_loss 2.8642e-03, test_loss 4.3998e-03\n",
      "step 1920, train_loss 2.8641e-03, test_loss 4.3998e-03\n",
      "step 1921, train_loss 2.8641e-03, test_loss 4.3997e-03\n",
      "step 1922, train_loss 2.8641e-03, test_loss 4.3997e-03\n",
      "step 1923, train_loss 2.8641e-03, test_loss 4.3997e-03\n",
      "step 1924, train_loss 2.8641e-03, test_loss 4.3997e-03\n",
      "step 1925, train_loss 2.8640e-03, test_loss 4.3996e-03\n",
      "step 1926, train_loss 2.8640e-03, test_loss 4.3997e-03\n",
      "step 1927, train_loss 2.8640e-03, test_loss 4.3996e-03\n",
      "step 1928, train_loss 2.8640e-03, test_loss 4.3996e-03\n",
      "step 1929, train_loss 2.8640e-03, test_loss 4.3995e-03\n",
      "step 1930, train_loss 2.8639e-03, test_loss 4.3996e-03\n",
      "step 1931, train_loss 2.8639e-03, test_loss 4.3995e-03\n",
      "step 1932, train_loss 2.8639e-03, test_loss 4.3996e-03\n",
      "step 1933, train_loss 2.8639e-03, test_loss 4.3994e-03\n",
      "step 1934, train_loss 2.8639e-03, test_loss 4.3995e-03\n",
      "step 1935, train_loss 2.8639e-03, test_loss 4.3994e-03\n",
      "step 1936, train_loss 2.8638e-03, test_loss 4.3996e-03\n",
      "step 1937, train_loss 2.8638e-03, test_loss 4.3994e-03\n",
      "step 1938, train_loss 2.8638e-03, test_loss 4.3996e-03\n",
      "step 1939, train_loss 2.8639e-03, test_loss 4.3994e-03\n",
      "step 1940, train_loss 2.8639e-03, test_loss 4.3998e-03\n",
      "step 1941, train_loss 2.8640e-03, test_loss 4.3995e-03\n",
      "step 1942, train_loss 2.8642e-03, test_loss 4.4004e-03\n",
      "step 1943, train_loss 2.8644e-03, test_loss 4.4000e-03\n",
      "step 1944, train_loss 2.8649e-03, test_loss 4.4018e-03\n",
      "step 1945, train_loss 2.8656e-03, test_loss 4.4018e-03\n",
      "step 1946, train_loss 2.8670e-03, test_loss 4.4058e-03\n",
      "step 1947, train_loss 2.8692e-03, test_loss 4.4074e-03\n",
      "step 1948, train_loss 2.8732e-03, test_loss 4.4172e-03\n",
      "step 1949, train_loss 2.8799e-03, test_loss 4.4250e-03\n",
      "step 1950, train_loss 2.8918e-03, test_loss 4.4509e-03\n",
      "step 1951, train_loss 2.9126e-03, test_loss 4.4806e-03\n",
      "step 1952, train_loss 2.9493e-03, test_loss 4.5542e-03\n",
      "step 1953, train_loss 3.0141e-03, test_loss 4.6561e-03\n",
      "step 1954, train_loss 3.1286e-03, test_loss 4.8704e-03\n",
      "step 1955, train_loss 3.3278e-03, test_loss 5.1863e-03\n",
      "step 1956, train_loss 3.6663e-03, test_loss 5.7556e-03\n",
      "step 1957, train_loss 4.2107e-03, test_loss 6.5046e-03\n",
      "step 1958, train_loss 4.9983e-03, test_loss 7.4623e-03\n",
      "step 1959, train_loss 5.9178e-03, test_loss 8.0387e-03\n",
      "step 1960, train_loss 6.5461e-03, test_loss 7.7845e-03\n",
      "step 1961, train_loss 6.2423e-03, test_loss 6.3622e-03\n",
      "step 1962, train_loss 4.8559e-03, test_loss 4.8758e-03\n",
      "step 1963, train_loss 3.3375e-03, test_loss 4.4403e-03\n",
      "step 1964, train_loss 2.9019e-03, test_loss 5.1441e-03\n",
      "step 1965, train_loss 3.6236e-03, test_loss 5.9946e-03\n",
      "step 1966, train_loss 4.4476e-03, test_loss 5.8868e-03\n",
      "step 1967, train_loss 4.3724e-03, test_loss 5.0514e-03\n",
      "step 1968, train_loss 3.5118e-03, test_loss 4.4433e-03\n",
      "step 1969, train_loss 2.9101e-03, test_loss 4.6738e-03\n",
      "step 1970, train_loss 3.1515e-03, test_loss 5.2242e-03\n",
      "step 1971, train_loss 3.6810e-03, test_loss 5.2148e-03\n",
      "step 1972, train_loss 3.6943e-03, test_loss 4.7467e-03\n",
      "step 1973, train_loss 3.2068e-03, test_loss 4.4385e-03\n",
      "step 1974, train_loss 2.9003e-03, test_loss 4.6150e-03\n",
      "step 1975, train_loss 3.0887e-03, test_loss 4.9046e-03\n",
      "step 1976, train_loss 3.3621e-03, test_loss 4.8078e-03\n",
      "step 1977, train_loss 3.2839e-03, test_loss 4.5301e-03\n",
      "step 1978, train_loss 2.9954e-03, test_loss 4.4456e-03\n",
      "step 1979, train_loss 2.9080e-03, test_loss 4.5926e-03\n",
      "step 1980, train_loss 3.0664e-03, test_loss 4.7050e-03\n",
      "step 1981, train_loss 3.1628e-03, test_loss 4.5789e-03\n",
      "step 1982, train_loss 3.0481e-03, test_loss 4.4433e-03\n",
      "step 1983, train_loss 2.9085e-03, test_loss 4.4661e-03\n",
      "step 1984, train_loss 2.9264e-03, test_loss 4.5467e-03\n",
      "step 1985, train_loss 3.0197e-03, test_loss 4.5580e-03\n",
      "step 1986, train_loss 3.0191e-03, test_loss 4.4640e-03\n",
      "step 1987, train_loss 2.9316e-03, test_loss 4.4223e-03\n",
      "step 1988, train_loss 2.8896e-03, test_loss 4.4708e-03\n",
      "step 1989, train_loss 2.9301e-03, test_loss 4.4935e-03\n",
      "step 1990, train_loss 2.9631e-03, test_loss 4.4709e-03\n",
      "step 1991, train_loss 2.9320e-03, test_loss 4.4235e-03\n",
      "step 1992, train_loss 2.8880e-03, test_loss 4.4216e-03\n",
      "step 1993, train_loss 2.8898e-03, test_loss 4.4553e-03\n",
      "step 1994, train_loss 2.9161e-03, test_loss 4.4465e-03\n",
      "step 1995, train_loss 2.9155e-03, test_loss 4.4256e-03\n",
      "step 1996, train_loss 2.8887e-03, test_loss 4.4130e-03\n",
      "step 1997, train_loss 2.8758e-03, test_loss 4.4207e-03\n",
      "step 1998, train_loss 2.8878e-03, test_loss 4.4365e-03\n",
      "step 1999, train_loss 2.8967e-03, test_loss 4.4191e-03\n",
      "step 2000, train_loss 2.8858e-03, test_loss 4.4076e-03\n",
      "best test loss at step 1937, train_loss 2.8638e-03, test_loss 4.3994e-03\n"
     ]
    }
   ],
   "source": [
    "print_results(stats, 1)\n",
    "print_best(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hnn](./hnn.png)\n",
    "![baseline](./baseline.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
