{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ZongyuWu/hamiltonian-nn/experiment-spring\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "THIS_DIR = os.path.abspath(os.path.join('.'))\n",
    "sys.path.insert(0, THIS_DIR)\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import torch, argparse\n",
    "import numpy as np\n",
    "from nn_models import MLP\n",
    "from hnn import HNN\n",
    "from data import get_dataset, get_trajectory\n",
    "from utils import L2_loss, rk4\n",
    "\n",
    "import scipy.integrate\n",
    "\n",
    "solve_ivp = scipy.integrate.solve_ivp\n",
    "print(THIS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print squared loss at specific steps for comparison with HNN\n",
    "print_every = 200\n",
    "def print_results(results, print_every=200):\n",
    "    for step in range(0, len(results[\"train_loss\"]), print_every):\n",
    "        print(\n",
    "            \"step {}, train_loss {:.4e}, test_loss {:.4e}\".format(\n",
    "                step,\n",
    "                results[\"train_loss\"][step],\n",
    "                results[\"test_loss\"][step],\n",
    "            )\n",
    "        )\n",
    "    print('Final train loss {:.4e} +/- {:.4e}\\nFinal test loss {:.4e} +/- {:.4e}'\n",
    "        .format(results[\"train_loss\"][-1], results[\"train_std\"][-1],\n",
    "                results[\"test_loss\"][-1], results[\"test_std\"][-1]))\n",
    "\n",
    "def print_best(results):\n",
    "    curr_min = 0\n",
    "\n",
    "    for step in range(0, len(results[\"train_loss\"])):\n",
    "        if results[\"test_loss\"][step] < results[\"test_loss\"][curr_min]:\n",
    "            curr_min = step\n",
    "    print(\n",
    "        \"best test loss at step {}, train_loss {:.4e}, test_loss {:.4e}\".format(\n",
    "            curr_min,\n",
    "            results[\"train_loss\"][curr_min],\n",
    "            results[\"test_loss\"][curr_min],\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_model(model, t_span, y0, **kwargs):\n",
    "    \n",
    "    def fun(t, np_x):\n",
    "        x = torch.tensor( np_x, requires_grad=True, dtype=torch.float32).view(1,2)\n",
    "        dx = model.time_derivative(x).data.numpy().reshape(-1)\n",
    "        return dx\n",
    "\n",
    "    return solve_ivp(fun=fun, t_span=t_span, y0=y0, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument(\n",
    "        \"--input_dim\", default=2, type=int, help=\"dimensionality of input tensor\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_dim\", default=200, type=int, help=\"hidden dimension of mlp\"\n",
    "    )\n",
    "    parser.add_argument(\"--learn_rate\", default=1e-3, type=float, help=\"learning rate\")\n",
    "    parser.add_argument(\n",
    "        \"--nonlinearity\", default=\"tanh\", type=str, help=\"neural net nonlinearity\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--total_iterations\", default=10, type=int, help=\"number of active learning iterations\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epoch_per_iter\", default=200, type=int, help=\"number of epochs per iteration\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sample_per_iter\", default=25, type=int, help=\"number of samples generated per iteration\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--print_every\",\n",
    "        default=200,\n",
    "        type=int,\n",
    "        help=\"number of gradient steps between prints\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--name\", default=\"spring\", type=str, help=\"only one option right now\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--baseline\",\n",
    "        dest=\"baseline\",\n",
    "        action=\"store_true\",\n",
    "        help=\"run baseline or experiment?\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_rk4\",\n",
    "        dest=\"use_rk4\",\n",
    "        action=\"store_true\",\n",
    "        help=\"integrate derivative with RK4\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--verbose\", dest=\"verbose\", action=\"store_true\", help=\"verbose?\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--kan\", dest=\"kan\", action=\"store_true\", help=\"use kan instead of mlp?\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--field_type\",\n",
    "        default=\"solenoidal\",\n",
    "        type=str,\n",
    "        help=\"type of vector field to learn\",\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int, help=\"random seed\")\n",
    "    parser.add_argument(\n",
    "        \"--save_dir\", default=THIS_DIR, type=str, help=\"where to save the trained model\"\n",
    "    )\n",
    "    parser.set_defaults(feature=True)\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # Set random seed\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    if args.verbose:\n",
    "        print(\"Training baseline model:\" if args.baseline else \"Training HNN model:\")\n",
    "    output_dim = args.input_dim if args.baseline else 2\n",
    "    nn_model = MLP(args.input_dim, args.hidden_dim, output_dim, args.nonlinearity)\n",
    "    model = HNN(\n",
    "        args.input_dim,\n",
    "        differentiable_model=nn_model,\n",
    "        field_type=args.field_type,\n",
    "        baseline=args.baseline,\n",
    "    )\n",
    "    optim = torch.optim.Adam(model.parameters(), args.learn_rate, weight_decay=1e-4)\n",
    "\n",
    "    # Generate initial dataset\n",
    "    points_per_sample = 30 # Change if made any change to t_span or timescale. Adjust this to change the number of points per sample. \n",
    "    data = get_dataset(seed=args.seed, samples=args.sample_per_iter * 2) # 2x for train/test split\n",
    "    x = torch.tensor(data[\"x\"], requires_grad=True, dtype=torch.float32)\n",
    "    test_x = torch.tensor(data[\"test_x\"], requires_grad=True, dtype=torch.float32)\n",
    "    dxdt = torch.Tensor(data[\"dx\"])\n",
    "    test_dxdt = torch.Tensor(data[\"test_dx\"])\n",
    "    \n",
    "    # Active learning loop\n",
    "    stats = {\"train_loss\": [], \"test_loss\": []}\n",
    "    for iter in range(args.total_iterations):\n",
    "        # Train model for epoch_per_iter epochs\n",
    "        for epoch in range(args.epoch_per_iter + 1):\n",
    "            # Train one step\n",
    "            dxdt_hat = (\n",
    "                model.rk4_time_derivative(x) if args.use_rk4 else model.time_derivative(x)\n",
    "            )\n",
    "            loss = L2_loss(dxdt, dxdt_hat)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # run test data\n",
    "            test_dxdt_hat = (\n",
    "                model.rk4_time_derivative(test_x)\n",
    "                if args.use_rk4\n",
    "                else model.time_derivative(test_x)\n",
    "            )\n",
    "            test_loss = L2_loss(test_dxdt, test_dxdt_hat)\n",
    "\n",
    "            # logging\n",
    "            stats[\"train_loss\"].append(loss.item())\n",
    "            stats[\"test_loss\"].append(test_loss.item())\n",
    "            if args.verbose and epoch % args.print_every == 0:\n",
    "                print(\n",
    "                    \"iter {} step {}, train_loss {:.4e}, test_loss {:.4e}\".format(\n",
    "                        iter, epoch, loss.item(), test_loss.item()\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "        # Generate new train data, randomly sample inputs\n",
    "        data = {}\n",
    "        xs, dxs = [], []\n",
    "        for s in range(args.sample_per_iter):\n",
    "            q, p, dq, dp, t = get_trajectory(t_span=[0, 3], timescale=10, radius=None, y0=None, noise_std=0.1)\n",
    "            xs.append(np.stack([q, p]).T)\n",
    "            dxs.append(np.stack([dq, dp]).T)\n",
    "        data[\"x\"] = np.concatenate(xs)\n",
    "        data[\"dx\"] = np.concatenate(dxs).squeeze()\n",
    "        \n",
    "        # Merge new data with previous train set\n",
    "        x = torch.cat([x, torch.tensor(data[\"x\"], requires_grad=True, dtype=torch.float32)])\n",
    "        dxdt = torch.cat([dxdt, torch.Tensor(data[\"dx\"])])\n",
    "        \n",
    "        # From the initial point of each sample, integrate their path and get the hamiltonian\n",
    "        t_span=[0,10]\n",
    "        t_eval = np.linspace(t_span[0], t_span[1], 1000)\n",
    "        kwargs = {'t_eval': t_eval, 'rtol': 1e-12}\n",
    "        hamiltonians = []\n",
    "        for i in range(0, len(x), points_per_sample):\n",
    "            path = integrate_model(model, t_span, x[i].detach().numpy(), **kwargs)\n",
    "            hamiltonian = model(torch.Tensor(path.y.T))[1].detach().numpy().squeeze()\n",
    "            hamiltonians.append((i, hamiltonian))\n",
    "        \n",
    "        # Sort hamiltonians by std \n",
    "        hamiltonians.sort(key=lambda h: h[1].std().item(), reverse=True)\n",
    "\n",
    "        # Select train set by top args.sample_per_iter hamiltonians\n",
    "        x = torch.stack([x[i] for i, _ in hamiltonians[:args.sample_per_iter * points_per_sample]])\n",
    "        dxdt = torch.stack([dxdt[i] for i, _ in hamiltonians[:args.sample_per_iter * points_per_sample]])\n",
    "\n",
    "    # Get final train and test loss\n",
    "    train_dxdt_hat = model.time_derivative(x)\n",
    "    train_dist = (dxdt - train_dxdt_hat) ** 2\n",
    "    test_dxdt_hat = model.time_derivative(test_x)\n",
    "    test_dist = (test_dxdt - test_dxdt_hat) ** 2\n",
    "    print(\n",
    "        \"Final train loss {:.4e} +/- {:.4e}\\nFinal test loss {:.4e} +/- {:.4e}\".format(\n",
    "            train_dist.mean().item(),\n",
    "            train_dist.std().item() / np.sqrt(train_dist.shape[0]),\n",
    "            test_dist.mean().item(),\n",
    "            test_dist.std().item() / np.sqrt(test_dist.shape[0]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return model, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HNN model:\n",
      "iter 0 step 0, train_loss 8.0827e-01, test_loss 7.7982e-01\n",
      "iter 0 step 200, train_loss 3.6939e-02, test_loss 3.6360e-02\n",
      "iter 1 step 0, train_loss 3.7899e-02, test_loss 5.8920e-02\n",
      "iter 1 step 200, train_loss 2.9611e-02, test_loss 4.4670e-02\n",
      "iter 2 step 0, train_loss 4.7311e-02, test_loss 6.0883e-02\n",
      "iter 2 step 200, train_loss 2.0326e-02, test_loss 4.7045e-02\n",
      "iter 3 step 0, train_loss 5.3151e-02, test_loss 8.1623e-02\n",
      "iter 3 step 200, train_loss 1.8468e-02, test_loss 5.7951e-02\n",
      "iter 4 step 0, train_loss 7.4321e-02, test_loss 5.6075e-02\n",
      "iter 4 step 200, train_loss 2.4988e-02, test_loss 8.8482e-02\n",
      "iter 5 step 0, train_loss 5.1097e-02, test_loss 8.6034e-02\n",
      "iter 5 step 200, train_loss 1.6459e-02, test_loss 5.6507e-02\n",
      "iter 6 step 0, train_loss 5.0103e-02, test_loss 5.4365e-02\n",
      "iter 6 step 200, train_loss 1.9843e-02, test_loss 5.7485e-02\n",
      "iter 7 step 0, train_loss 5.8281e-02, test_loss 5.6575e-02\n",
      "iter 7 step 200, train_loss 1.5808e-02, test_loss 6.0593e-02\n",
      "iter 8 step 0, train_loss 6.2340e-02, test_loss 9.6649e-02\n",
      "iter 8 step 200, train_loss 9.9977e-03, test_loss 8.9369e-02\n",
      "iter 9 step 0, train_loss 6.5430e-02, test_loss 1.0733e-01\n",
      "iter 9 step 200, train_loss 9.6351e-03, test_loss 7.1555e-02\n",
      "Final train loss 6.0287e-02 +/- 2.0622e-02\n",
      "Final test loss 7.1555e-02 +/- 4.2751e-03\n"
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "args.verbose = True\n",
    "model, stats = train(args)\n",
    "\n",
    "# save\n",
    "os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None\n",
    "label = \"-active-hnn\" \n",
    "path = \"{}/{}{}.tar\".format(args.save_dir, args.name, label)\n",
    "torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline model:\n",
      "iter 0 step 0, train_loss 8.6160e-01, test_loss 9.0597e-01\n",
      "iter 0 step 200, train_loss 3.8308e-02, test_loss 3.8129e-02\n",
      "iter 1 step 0, train_loss 4.0415e-02, test_loss 5.6570e-02\n",
      "iter 1 step 200, train_loss 3.1882e-02, test_loss 4.0645e-02\n",
      "iter 2 step 0, train_loss 4.0925e-02, test_loss 4.4863e-02\n",
      "iter 2 step 200, train_loss 2.7554e-02, test_loss 3.9633e-02\n",
      "iter 3 step 0, train_loss 4.4358e-02, test_loss 1.1742e-01\n",
      "iter 3 step 200, train_loss 2.9178e-02, test_loss 4.5318e-02\n",
      "iter 4 step 0, train_loss 6.1258e-02, test_loss 4.5581e-02\n",
      "iter 4 step 200, train_loss 3.8507e-02, test_loss 4.5118e-02\n",
      "iter 5 step 0, train_loss 3.3670e-02, test_loss 4.5419e-02\n",
      "iter 5 step 200, train_loss 2.8148e-02, test_loss 3.9434e-02\n",
      "iter 6 step 0, train_loss 3.3291e-02, test_loss 4.0455e-02\n",
      "iter 6 step 200, train_loss 2.7397e-02, test_loss 4.0383e-02\n",
      "iter 7 step 0, train_loss 4.3547e-02, test_loss 4.4896e-02\n",
      "iter 7 step 200, train_loss 3.3889e-02, test_loss 3.8631e-02\n",
      "iter 8 step 0, train_loss 2.7852e-02, test_loss 5.9618e-02\n",
      "iter 8 step 200, train_loss 2.2033e-02, test_loss 4.2692e-02\n",
      "iter 9 step 0, train_loss 3.3419e-02, test_loss 4.7160e-02\n",
      "iter 9 step 200, train_loss 2.1101e-02, test_loss 4.2946e-02\n",
      "Final train loss 4.6387e-02 +/- 1.3575e-02\n",
      "Final test loss 4.2946e-02 +/- 2.1171e-03\n"
     ]
    }
   ],
   "source": [
    "args.baseline = True\n",
    "model, stats = train(args)\n",
    "\n",
    "# save\n",
    "os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None\n",
    "label = \"-active-baseline\" \n",
    "path = \"{}/{}{}.tar\".format(args.save_dir, args.name, label)\n",
    "torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KARHNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
