{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('.')))\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import torch, argparse\n",
    "import numpy as np\n",
    "from hnn import HNN\n",
    "from data import get_dataset, get_trajectory\n",
    "\n",
    "import scipy.integrate\n",
    "\n",
    "solve_ivp = scipy.integrate.solve_ivp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print squared loss at specific steps for comparison with HNN\n",
    "print_every = 200\n",
    "def print_results(results, print_every=200):\n",
    "    for step in range(0, len(results[\"train_loss\"]), print_every):\n",
    "        print(\n",
    "            \"step {}, train_loss {:.4e}, test_loss {:.4e}\".format(\n",
    "                step,\n",
    "                results[\"train_loss\"][step],\n",
    "                results[\"test_loss\"][step],\n",
    "            )\n",
    "        )\n",
    "    print('Final train loss {:.4e} +/- {:.4e}\\nFinal test loss {:.4e} +/- {:.4e}'\n",
    "        .format(results[\"train_loss\"][-1], results[\"train_std\"][-1],\n",
    "                results[\"test_loss\"][-1], results[\"test_std\"][-1]))\n",
    "\n",
    "def print_best(results):\n",
    "    curr_min = 0\n",
    "\n",
    "    for step in range(0, len(results[\"train_loss\"])):\n",
    "        if results[\"test_loss\"][step] < results[\"test_loss\"][curr_min]:\n",
    "            curr_min = step\n",
    "    print(\n",
    "        \"best test loss at step {}, train_loss {:.4e}, test_loss {:.4e}\".format(\n",
    "            curr_min,\n",
    "            results[\"train_loss\"][curr_min],\n",
    "            results[\"test_loss\"][curr_min],\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_model(model, t_span, y0, **kwargs):\n",
    "    \n",
    "    def fun(t, np_x):\n",
    "        x = torch.tensor( np_x, requires_grad=True, dtype=torch.float32).view(1,2)\n",
    "        dx = model.time_derivative(x).data.numpy().reshape(-1)\n",
    "        return dx\n",
    "\n",
    "    return solve_ivp(fun=fun, t_span=t_span, y0=y0, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument(\n",
    "        \"--input_dim\", default=2, type=int, help=\"dimensionality of input tensor\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_dim\", default=200, type=int, help=\"hidden dimension of mlp\"\n",
    "    )\n",
    "    parser.add_argument(\"--learn_rate\", default=1e-3, type=float, help=\"learning rate\")\n",
    "    parser.add_argument(\n",
    "        \"--nonlinearity\", default=\"tanh\", type=str, help=\"neural net nonlinearity\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--total_iterations\", default=10, type=int, help=\"number of active learning iterations\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epoch_per_iter\", default=200, type=int, help=\"number of epochs per iteration\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sample_per_iter\", default=25, type=int, help=\"number of samples generated per iteration\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--print_every\",\n",
    "        default=200,\n",
    "        type=int,\n",
    "        help=\"number of gradient steps between prints\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--name\", default=\"spring\", type=str, help=\"only one option right now\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--baseline\",\n",
    "        dest=\"baseline\",\n",
    "        action=\"store_true\",\n",
    "        help=\"run baseline or experiment?\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_rk4\",\n",
    "        dest=\"use_rk4\",\n",
    "        action=\"store_true\",\n",
    "        help=\"integrate derivative with RK4\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--verbose\", dest=\"verbose\", action=\"store_true\", help=\"verbose?\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--kan\", dest=\"kan\", action=\"store_true\", help=\"use kan instead of mlp?\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--field_type\",\n",
    "        default=\"solenoidal\",\n",
    "        type=str,\n",
    "        help=\"type of vector field to learn\",\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int, help=\"random seed\")\n",
    "    parser.add_argument(\n",
    "        \"--save_dir\", default=THIS_DIR, type=str, help=\"where to save the trained model\"\n",
    "    )\n",
    "    parser.set_defaults(feature=True)\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # Set random seed\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    if args.verbose:\n",
    "        print(\"Training baseline model:\" if args.baseline else \"Training HNN model:\")\n",
    "\n",
    "    output_dim = args.input_dim if args.baseline else 2\n",
    "    nn_model = MLP(args.input_dim, args.hidden_dim, output_dim, args.nonlinearity)\n",
    "    model = HNN(\n",
    "        args.input_dim,\n",
    "        differentiable_model=nn_model,\n",
    "        field_type=args.field_type,\n",
    "        baseline=args.baseline,\n",
    "    )\n",
    "    optim = torch.optim.Adam(model.parameters(), args.learn_rate, weight_decay=1e-4)\n",
    "\n",
    "    # Generate initial dataset\n",
    "    data = get_dataset(seed=args.seed, samples=args.sample_per_iter * 2) # 2x for train/test split\n",
    "    x = torch.tensor(data[\"x\"], requires_grad=True, dtype=torch.float32)\n",
    "    # print('\\ninit x shape:', x.shape)\n",
    "    # print('\\ninit x:', x)\n",
    "    test_x = torch.tensor(data[\"test_x\"], requires_grad=True, dtype=torch.float32)\n",
    "    dxdt = torch.Tensor(data[\"dx\"])\n",
    "    test_dxdt = torch.Tensor(data[\"test_dx\"])\n",
    "    \n",
    "    # Active learning loop\n",
    "    stats = {\"train_loss\": [], \"test_loss\": []}\n",
    "    for iter in range(args.total_iterations):\n",
    "        # Train model for epoch_per_iter epochs\n",
    "        for epoch in range(args.epoch_per_iter + 1):\n",
    "            # Train one step\n",
    "            dxdt_hat = (\n",
    "                model.rk4_time_derivative(x) if args.use_rk4 else model.time_derivative(x)\n",
    "            )\n",
    "            loss = L2_loss(dxdt, dxdt_hat)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # run test data\n",
    "            test_dxdt_hat = (\n",
    "                model.rk4_time_derivative(test_x)\n",
    "                if args.use_rk4\n",
    "                else model.time_derivative(test_x)\n",
    "            )\n",
    "            test_loss = L2_loss(test_dxdt, test_dxdt_hat)\n",
    "\n",
    "            # logging\n",
    "            stats[\"train_loss\"].append(loss.item())\n",
    "            stats[\"test_loss\"].append(test_loss.item())\n",
    "            if args.verbose and epoch % args.print_every == 0:\n",
    "                print(\n",
    "                    \"iter {} step {}, train_loss {:.4e}, test_loss {:.4e}\".format(\n",
    "                        iter, epoch, loss.item(), test_loss.item()\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "        # Generate new train data, randomly sample inputs\n",
    "        data = {}\n",
    "        xs, dxs = [], []\n",
    "        for s in range(args.sample_per_iter):\n",
    "            q, p, dq, dp, t = get_trajectory(t_span=[0, 3], timescale=10, radius=None, y0=None, noise_std=0.1)\n",
    "            xs.append(np.stack([q, p]).T)\n",
    "            dxs.append(np.stack([dq, dp]).T)\n",
    "        data[\"x\"] = np.concatenate(xs)\n",
    "        data[\"dx\"] = np.concatenate(dxs).squeeze()\n",
    "        \n",
    "        # Merge with previous train set\n",
    "        # print('\\nx shape:', x.shape)\n",
    "        # print('\\nx:', x)\n",
    "        # print('\\ndata[\"x\"] shape:', data[\"x\"].shape)\n",
    "        # print('\\ndata[\"x\"]:', data[\"x\"])\n",
    "        x = torch.cat([x, torch.tensor(data[\"x\"], requires_grad=True, dtype=torch.float32)])\n",
    "        dxdt = torch.cat([dxdt, torch.Tensor(data[\"dx\"])])\n",
    "        \n",
    "        # Integrate from each initial point to get paths\n",
    "        t_span=[0,10]\n",
    "        t_eval = np.linspace(t_span[0], t_span[1], 1000)\n",
    "        kwargs = {'t_eval': t_eval, 'rtol': 1e-12}\n",
    "        hamiltonians = []\n",
    "        print('\\nx:', x)\n",
    "        print('\\nx[0]:', x[0])\n",
    "        for i in range(len(x)):\n",
    "            path = integrate_model(model, t_span, x[i][0], **kwargs)\n",
    "            hamiltonian = model(torch.Tensor(path.y.T))[1].detach().numpy().squeeze()\n",
    "            hamiltonians.append((i, hamiltonian))\n",
    "        \n",
    "        # Sort hamiltonians by std \n",
    "        hamiltonians.sort(key=lambda h: h[1].std().item(), reverse=True)\n",
    "\n",
    "        # Select train set by top args.sample_per_iter hamiltonians\n",
    "        x = torch.stack([x[i] for i, _ in hamiltonians[:args.sample_per_iter]])\n",
    "        dxdt = torch.stack([dxdt[i] for i, _ in hamiltonians[:args.sample_per_iter]])\n",
    "\n",
    "\n",
    "    train_dxdt_hat = model.time_derivative(x)\n",
    "    train_dist = (dxdt - train_dxdt_hat) ** 2\n",
    "    test_dxdt_hat = model.time_derivative(test_x)\n",
    "    test_dist = (test_dxdt - test_dxdt_hat) ** 2\n",
    "    print(\n",
    "        \"Final train loss {:.4e} +/- {:.4e}\\nFinal test loss {:.4e} +/- {:.4e}\".format(\n",
    "            train_dist.mean().item(),\n",
    "            train_dist.std().item() / np.sqrt(train_dist.shape[0]),\n",
    "            test_dist.mean().item(),\n",
    "            test_dist.std().item() / np.sqrt(test_dist.shape[0]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return model, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x: tensor([[-0.0847,  0.8053],\n",
      "        [ 0.4012,  0.5271],\n",
      "        [ 0.2978,  0.5341],\n",
      "        ...,\n",
      "        [-0.4611, -0.2224],\n",
      "        [-0.3949, -0.0863],\n",
      "        [-0.4341, -0.0959]], grad_fn=<CatBackward0>)\n",
      "\n",
      "x[0]: tensor([-0.0847,  0.8053], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m get_args()\n\u001b[0;32m----> 2\u001b[0m model, stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# save\u001b[39;00m\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(args\u001b[38;5;241m.\u001b[39msave_dir) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(args\u001b[38;5;241m.\u001b[39msave_dir) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[59], line 87\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mx[0]:\u001b[39m\u001b[38;5;124m'\u001b[39m, x[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x)):\n\u001b[0;32m---> 87\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mintegrate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_span\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     hamiltonian \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mTensor(path\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mT))[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     89\u001b[0m     hamiltonians\u001b[38;5;241m.\u001b[39mappend((i, hamiltonian))\n",
      "Cell \u001b[0;32mIn[57], line 8\u001b[0m, in \u001b[0;36mintegrate_model\u001b[0;34m(model, t_span, y0, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m     dx \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtime_derivative(x)\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dx\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msolve_ivp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_span\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_span\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/KARHNN/lib/python3.13/site-packages/scipy/integrate/_ivp/ivp.py:621\u001b[0m, in \u001b[0;36msolve_ivp\u001b[0;34m(fun, t_span, y0, method, t_eval, dense_output, events, vectorized, args, **options)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m METHODS:\n\u001b[1;32m    619\u001b[0m     method \u001b[38;5;241m=\u001b[39m METHODS[method]\n\u001b[0;32m--> 621\u001b[0m solver \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t_eval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m     ts \u001b[38;5;241m=\u001b[39m [t0]\n",
      "File \u001b[0;32m~/miniforge3/envs/KARHNN/lib/python3.13/site-packages/scipy/integrate/_ivp/rk.py:89\u001b[0m, in \u001b[0;36mRungeKutta.__init__\u001b[0;34m(self, fun, t0, y0, t_bound, max_step, rtol, atol, vectorized, first_step, **extraneous)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, fun, t0, y0, t_bound, max_step\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39minf,\n\u001b[1;32m     86\u001b[0m              rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, vectorized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     87\u001b[0m              first_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextraneous):\n\u001b[1;32m     88\u001b[0m     warn_extraneous(extraneous)\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_bound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                     \u001b[49m\u001b[43msupport_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_old \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_step \u001b[38;5;241m=\u001b[39m validate_max_step(max_step)\n",
      "File \u001b[0;32m~/miniforge3/envs/KARHNN/lib/python3.13/site-packages/scipy/integrate/_ivp/base.py:135\u001b[0m, in \u001b[0;36mOdeSolver.__init__\u001b[0;34m(self, fun, t0, y0, t_bound, vectorized, support_complex)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_old \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m=\u001b[39m t0\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fun, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_arguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msupport_complex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_bound \u001b[38;5;241m=\u001b[39m t_bound\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorized \u001b[38;5;241m=\u001b[39m vectorized\n",
      "File \u001b[0;32m~/miniforge3/envs/KARHNN/lib/python3.13/site-packages/scipy/integrate/_ivp/base.py:6\u001b[0m, in \u001b[0;36mcheck_arguments\u001b[0;34m(fun, y0, support_complex)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_arguments\u001b[39m(fun, y0, support_complex):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper function for checking arguments common to all solvers.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     y0 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(y0\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mcomplexfloating):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m support_complex:\n",
      "File \u001b[0;32m~/miniforge3/envs/KARHNN/lib/python3.13/site-packages/torch/_tensor.py:1149\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "model, stats = train(args)\n",
    "\n",
    "# save\n",
    "os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None\n",
    "label = \"-active\" \n",
    "path = \"{}/{}{}.tar\".format(args.save_dir, args.name, label)\n",
    "torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KARHNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
